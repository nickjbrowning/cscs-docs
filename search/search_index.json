{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["trimmer"]},"docs":[{"location":"","title":"CSCS Documentation","text":"<ul> <li> <p>Visit the CSCS status site for the status of Alps systems, and the latest announcements.</p> <p> status.cscs.ch</p> </li> </ul> <p>The Alps Research infrastructure hosts multiple platforms and clusters targeting different communities</p> <ul> <li> <p> Platforms</p> <p>Projects at CSCS are granted access to clusters, which are managed by platforms. Start by finding the platform for the cluster that you want to use.</p> <p> Platforms overview</p> <p>Go straight to the documentation for the platform that hosts your project:</p> <p> HPC Platform (Daint, Eiger)</p> <p> Machine Learning Platform (Clariden)</p> <p> Climate and Weather Platform (Santis)</p> </li> <li> <p> Alps</p> <p>Learn more about the Alps research infrastructure</p> <p> Alps Overview</p> <p>Get detailed information about the main components of the infrastructre</p> <p> Alps Clusters</p> <p> Alps Hardware</p> <p> Alps Storage</p> </li> </ul> <ul> <li> <p> Accounts and Projects</p> <p>The first step is to get an account and a project</p> <p> Accounts and Projects</p> </li> <li> <p> Logging In</p> <p>Once you have an account, you can set up multi factor authentication</p> <p> Setting up MFA</p> <p>Then access CSCS services</p> <p> Accessing CSCS Web Services</p> <p> Using SSH</p> </li> </ul> <p></p>"},{"location":"#get-in-touch","title":"Get in Touch","text":"<p>If you cannot find the information that you need in the documentation, help is available.</p> <ul> <li> <p> Get Help</p> <p>Contact the CSCS Service Desk for help.</p> <p> Service Desk</p> </li> <li> <p> Chat</p> <p>Discuss Alps with other users and CSCS staff on Slack.</p> <p> CSCS User Slack</p> </li> </ul> <ul> <li> <p> Contribute</p> <p>The source for the documentation is hosted on GitHub.</p> <p> Contribute to the docs </p> </li> </ul>"},{"location":"#tools-and-services","title":"Tools and Services","text":"<p>Todo</p> <p>Provide some links to the \"how\" documentation here.</p> <ul> <li> <p> Services</p> <p>CSCS provides services and software on Alps.</p> <p> Services Overview</p> </li> <li> <p> Build and Install Software</p> <p>Guides on how to build and install software from source using uenv and containers</p> <p> Building and Installing Software</p> </li> </ul>"},{"location":"access/","title":"Connecting to Alps","text":"<p>This documentation guides users through the process of accessing CSCS systems and services.</p> <p>Before accessing CSCS, you need to have an account at CSCS, and be part of a project that has been allocated resources. More information on how to get an account is available in accounts and projects.</p> <ul> <li> <p> Multi Factor Authentication</p> <p>Before signing in to CSCS' web portals or using SSH, all users have to set up multi factor authentication (MFA)</p> <p> MFA</p> </li> <li> <p> Web Services</p> <p>Before signing in to CSCS' web portals or using SSH, all users have to set up multi factor authentication (MFA)</p> <p> Accessing CSCS web services</p> </li> <li> <p> SSH Access</p> <p>Logging into Clusters on Alps</p> <p> SSH</p> </li> <li> <p> VSCode</p> <p>How to connect VSCode IDE on your laptop with Alps</p> <p> SSH</p> </li> </ul>"},{"location":"access/mfa/","title":"Multi Factor Authentication (MFA)","text":""},{"location":"access/mfa/#multi-factor-authentication","title":"Multi Factor Authentication","text":"<p>To access CSCS services and systems users are required to authenticate using multi-factor authentication (MFA). MFA is implemented as a two-factor authentication, where one factor is the login and password pair (\"the thing you know\") and the other factor is the device which generates one-time passwords (OTPs, \"the thing you have\"). In this way security is significantly improved compared to single-factor (password only) authentication.</p> <p>The MFA workflow uses a time-based one-time password (OTP) to verify identity. An OTP is a six-digit number which changes every 30 seconds. OTPs are generated using a tool installed on a device other than the one used to access CSCS services and infrastructure. We recommend to use a smartphone with an application such as Google Authenticator to obtain the OTPs.</p> <p></p>"},{"location":"access/mfa/#getting-started","title":"Getting Started","text":"<p>When you first log in to any of the CSCS web applications such as UMP, Jupyter, etc., you will be asked to register your device.</p> <p>Firstly, you will be asked to provide a code that you received by email. After this validation step, you will need to scan a QR code with your mobile phone using an application such as Google Authenticator. Lastly, you will need to enter the OTP from the authenticator application to complete the registration of your device. From then on, two-factor authentication will be required to access CSCS services and systems. A more detailed explanation of the registration process is provided in the next section.</p> <p>Warning</p> <p>It is not possible to log in to CSCS systems using SSH without registering a device and creating certified SSH keys. See below for details on generating certified SSH keys.</p>"},{"location":"access/mfa/#authenticator-application","title":"Authenticator Application","text":"<p>CSCS supports authenticators that follow an open standard called TOTP. The recommended way to access such an authenticator is to install an application on your mobile phone. Google Authenticator and FreeOTP have been tested successfully; however, if you are using a different mobile application for OTPs, feel free to continue using it - given it supports the TOTP standard.</p> <p>You can download Google Authenticator for your phone:</p> <ul> <li> Android: on the Google Play Store.</li> <li> iOS: on the Apple Store.</li> </ul> <p></p>"},{"location":"access/mfa/#configure-the-authenticator","title":"Configure the Authenticator","text":"<p>Before starting, ensure that the following pre-requisites are satisfied</p> <ol> <li>You have an invitation email from CSCS for MFA enrollment<ul> <li>a notification email will be sent at least one week before we sent the invitation email.</li> </ul> </li> <li>You have installed an OTP Authenticator app on your mobile device (see above).</li> </ol> <p>Note</p> <p>If you try access any of our web applications without setting up MFA, you will be redirected to enroll for MFA.</p> <p>Warning</p> <p>If you try to SSH to CSCS systems without setting up MFA, you will be prompted with permission denied error, for example: <pre><code>$ ssh ela.cscs.ch\nbobsmith@ela.cscs.ch: Permission denied (publickey).\nConnection closed by UNKNOWN port 65535\n</code></pre></p> <p>Steps:</p> <ol> <li>Access any of the CSCS Web applications such as <code>account.cscs.ch</code>, Jupyter, etc., on a new browser session which will redirects you to the CSCS login page.</li> <li>Log in with your username and password.</li> <li>You will be asked to key in a code which CSCS Authentication system sent to you by email.    After successful validation of the code you will be redirected to the next page which present a QR code.</li> <li>Scan the QR code with the authenticator app that was installed on your mobile device.    After scanning the QR code the authenticator app will start generating a new 6 digit OTP every 60 seconds.</li> <li>To complete the OTP registration process, please enter the 6 digit OTP from the authenticator app at the bottom of the the same QR code page. Optionally, you can input your device name where you imported the OTP seed by scanning the QR code</li> <li>On successful registration you will be logged into the CSCS web application that you accessed in step-1</li> </ol> <p>Todo</p> <p>do we need the images from KB?</p>"},{"location":"access/mfa/#resetting-the-authenticator","title":"Resetting the Authenticator","text":"<p>In case users lose access to their mobile device/Authenticator OTP, users can reset their OTP by following the below self-service process.</p> <ol> <li>Access any CSCS web application like: account.cscs.ch which redirects you to the CSCS Login page. </li> <li>From the login screen, click the \"Reset OTP\" link below the \"LOG IN\" button</li> <li>Enter your username and password.</li> <li>On successful validation of user credentials, users will receive an email with a reset credentials link like the one below, click on the link in the email</li> <li>The steps are the same as for the first time you configured the authenticator.</li> </ol> <p>Warning</p> <p>When replacing your smartphone remember to sync the authenticator app before resetting the old smartphone. Otherwise, you will have to follow this process.</p>"},{"location":"access/ssh/","title":"SSH","text":""},{"location":"access/ssh/#using-ssh","title":"Using SSH","text":"<p>Before accessing CSCS clusters using SSH, first ensure that you have created a user account that is part of a project that has access to the cluster, and have multi factor authentication configured.</p> <p></p>"},{"location":"access/ssh/#generating-keys-with-sshservice","title":"Generating Keys with SSHService","text":"<p>It is not possible to authenticate with a username/password and user-created SSH keys. Instead, it is necessary to use a certified SSH key created using the CSCS SSHService.</p> <p>Note</p> <p>Keys are valid for 24 hours, after which a new key must be generated.</p> <p>Warning</p> <p>The number of certified SSH keys is limited to five per day. Once you have reached this number you will not be able to generate new keys until at least one of these key expires or keys are revoked.</p> <p>There are two methods for generating SSH keys using the SSHService, the SSHService web app or by using a command-line script.</p>"},{"location":"access/ssh/#getting-keys-via-the-command-line","title":"Getting keys via the command line","text":"<p>On Linux and MacOS, the SSH keys can be generated and automatically installed using a command-line script. This script is provided in pure Bash and in Python. Python 3 is required together with packages listed in <code>requirements.txt</code> provided with the scripts.</p> <p>Note</p> <p>We recommend to using a virtual environment for Python.</p> <p>If this is the first time, download the ssh service from CSCS GitHub:</p> <pre><code>git clone https://github.com/eth-cscs/sshservice-cli\ncd sshservice-cli\n</code></pre> <p>The next step is to use either the bash or python scripts:</p> bashpython <p>Run the bash script in the <code>sshservice-cli</code> path:</p> <pre><code>./cscs-keygen.sh\n</code></pre> <p>The first time you use the script, you can set up a python virtual environment with the dependencies installed:</p> <pre><code>python3 -m venv mfa\nsource mfa/bin/activate\npip install -r requirements.txt\n</code></pre> <p>Thereafter, activate the venv before using the script:</p> <pre><code>source mfa/bin/activate\npython cscs-keygen.py\n</code></pre> <p>For both approaches, follow the on screen instructions that require you to enter your username, password and the six-digit OTP from the authenticator app on your phone. The script generates the key pair (<code>cscs-key</code> and <code>cscs-key-cert.pub</code>) in your <code>~/.ssh</code> path:</p> <pre><code>&gt; ls ~/.ssh/cscs-key*\n/home/bobsmith/.ssh/cscs-key  /home/bobsmith/.ssh/cscs-key-cert.pub\n</code></pre>"},{"location":"access/ssh/#getting-keys-via-the-web-app","title":"Getting keys via the web app","text":"<p>Access the SSHService web application by accessing the URL, sshservice.cscs.ch.</p> <ol> <li>Sign in with username, password and OTP</li> <li>Select \"Signed key\" on the left tab and click on \"Get a signed key\"</li> <li>On the next page a key pair is generated and ready to be downloaded. Download or copy/paste both keys.</li> </ol> <p>Once generated, the keys need to be copied from where your browser downloaded them to your <code>~/.ssh</code> path, for example: <pre><code>mv /download/location/cscs-key-cert.pub ~/.ssh/cscs-key-cert.pub\nmv /download/location/cscs-key ~/.ssh/cscs-key\nchmod 0600 ~/.ssh/cscs-key\n</code></pre></p>"},{"location":"access/ssh/#adding-a-password-to-the-key","title":"Adding a password to the key","text":"<p>Once the key has been generated using either the CLI or web interface above, it is strongly recommended that you add a password to the generated key using the ssh-keygen tool.</p> <pre><code>ssh-keygen -f ~/.ssh/cscs-key -p\n</code></pre>"},{"location":"access/ssh/#logging-in","title":"Logging In","text":"<p>To ensure secure access, CSCS requires users to connect through the designated jump host Ela (<code>ela.cscs.ch</code>) before accessing any cluster.</p> <p>Before trying to log into your target cluster, you can first check that the SSH key generated above can be used to access Ela: <pre><code>ssh -i ~/.ssh/cscs-key ela.cscs.ch\n</code></pre></p> <p>To log into a target system at CSCS, you need to perform some additional setup to handle forwarding of SSH keys generated using the SSHService. There are two alternatives detailed below.</p> <p></p>"},{"location":"access/ssh/#adding-ela-as-a-jump-host-in-ssh-configuration","title":"Adding Ela as a jump host in SSH Configuration","text":"<p>This approach configures Ela as a jump host and creates aliases for the systems that you want to access in <code>~/.ssh/config</code> on your laptop or PC. The benefit of this approach is that once the <code>~/.ssh/config</code> file has been configured, no additional steps are required between creating a new key using MFA, and logging in.</p> <p>Below is an example <code>~/.ssh/config</code> file that facilitates directly logging into the Daint, Santis and Clariden clusters using <code>ela.cscs.ch</code> as a Jump host:</p> <pre><code>Host ela\n    HostName ela.cscs.ch\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n\nHost daint\n    HostName daint.alps.cscs.ch\n    User cscsusername\n    ProxyJump ela\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n\nHost santis\n    HostName santis.alps.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n\nHost clariden\n    HostName clariden.alps.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n</code></pre> <p> Replace <code>cscsusername</code> with your CSCS username in the file above.</p> <p>After saving this file, one can directly log into <code>daint.alps.cscs.ch</code> from your local system using the alias <code>daint</code>:</p> <pre><code>ssh daint\n</code></pre> <p></p>"},{"location":"access/ssh/#using-ssh-agent","title":"Using SSH Agent","text":"<p>Alternatively, the SSH authentication agent can be configured to manage the keys.</p> <p>Each time a new key is generated using the SSHService, add the key to the SSH agent: <pre><code>ssh-add -t 1d ~/.ssh/cscs-key\n</code></pre></p> Could not open a connection to your authentication agent <p>If you see this error message, the ssh agent is not running. You can start it with the following command: <pre><code>eval $(ssh-agent)\n</code></pre></p> <p>Once the key has been configured, log into Ela using the <code>-A</code> flag, and then jump to the target system: <pre><code># log in to ela.cscs.ch\nssh -A cscsusername@ela.cscs.ch\n\n# then jump to a cluster\nssh daint.cscs.ch\n</code></pre></p> <p></p>"},{"location":"access/ssh/#frequently-encountered-issues","title":"Frequently encountered issues","text":"too many authentication failures <p>You may have too many keys in your ssh agent. Remove the unused keys from the agent or flush them all with the following command: <pre><code>ssh-add -D\n</code></pre></p> Permission denied <p>This might indicate that they key has expired.</p> Could not open a connection to your authentication agent <p>If you see this error when adding keys to the ssh-agent, please make sure the agent is up, and if not bring up the agent using the following command: <pre><code>eval $(ssh-agent)\n</code></pre></p>"},{"location":"access/vscode/","title":"VSCode","text":""},{"location":"access/vscode/#connecting-with-vscode","title":"Connecting with VSCode","text":"<p>Visual Studio Code provides flexible support for remote development. VSCode's remote tunnel feature starts a server on a remote system, and connects the editor to this server. There are two ways to set up the connection:</p> <ul> <li>using the code CLI: the most flexible method if using containers or uenv.</li> <li>using the VSCode interface: VSCode will connect onto the system, download and start the server</li> </ul> <p>The main challenge with using VSCode is that the most convenient method for starting a remote session is to start a remote tunnel from the VS Code GUI. This approach starts a session in the standard login environment on that node, however this won't work if you want to be developing in a container, in a uenv, or on a compute node.</p>"},{"location":"access/vscode/#flexible-method-remote-server","title":"Flexible method: remote server","text":"<p>The most flexible method for connecting VSCode is to log in to the Alps system, set up your environment (start a container or uenv, start a session on a compute node), and start the remote server in that environment pre-configured.</p> <p>Note</p> <p>This approach requires that you have a GitHub account, and that the GitHub account is configured with your VS Code editor.</p> <p>The first step is to download the VS Code CLI tool <code>code</code>, which CSCS provides for easy download. There are two executables, one for using on systems with x86 or ARM CPUs respectively.</p> <code>aarch64</code> nodes (daint, clariden, santis)<code>x86_64</code> nodes (eiger, bristen) <pre><code>wget https://jfrog.svc.cscs.ch/artifactory/uenv-sources/vscode/vscode_cli_alpine_arm64_cli.tar.gz\ntar -xf vscode_cli_alpine_arm64_cli.tar.gz\n</code></pre> <pre><code>wget https://jfrog.svc.cscs.ch/artifactory/uenv-sources/vscode/vscode_cli_alpine_x64_cli.tar.gz\ntar -xf vscode_cli_alpine_x64_cli.tar.gz\n</code></pre> <p>Note</p> <p>See the guide on how to manage architecture-specific binaries if you plan to use VScode on both x86 and ARM clusters.</p> <p>Alternatively, download the CLI tool from the VS Code site -- take care to select either x86 or Arm64 version that matches the target system.</p> <p>After downloading, copy the <code>code</code> executable to a location in your PATH, so that it is available for future sessions.</p> guidance on where to put architecture-specific executables <p>The home directory can be shared by multiple clusters that might have different micro-architectures, so it is important to separate executables for x86 and aarch64 (ARM) targets.</p> <p>In <code>~/.bashrc</code>, add the following line (you will need to log in again for this to take effect): <pre><code>export PATH=$HOME/.local/$(uname -m)/bin:$PATH\n</code></pre> The <code>uname -m</code> command will print <code>aarch64</code> or <code>x86_64</code>, according to the microarchitecture of the node it is run on.</p> <p>Then create the path, and copy the <code>code</code> executable to the architecture-specific path: <pre><code>mkdir -p $HOME/.local/$(uname -m)/bin\ncp ./code $HOME/.local/$(uname -m)/bin\n</code></pre></p> <p>To set up a remote server on the target system, run the <code>code</code> executable that you downloaded the <code>tunnel</code> argument. You will be asked to choose whether to log in to Microsoft or GitHub (we have tested with GitHub):</p> <pre><code>&gt; code tunnel --name=$CLUSTER_NAME-tunnel\n...\n? How would you like to log in to Visual Studio Code? \u203a\n  Microsoft Account\n\u276f GitHub Account\n</code></pre> <p>Tip</p> <p>Give the tunnel a unique name using the <code>--name</code> flag, which will later be listed on the VSCode UI.</p> <p>You will be requested to go to github.com/login/device and enter an 8-digit code. Once you have finished registering the service with GitHub, in VSCode on your PC/laptop open the \"remote explorer\" pane on the left hand side of the main window, and the connection will be visible under REMOTES (TUNNELS/SSH) -&gt; Tunnels.</p> <p>first time setting up a remote service</p> <p>If this is the first time you have followed this procedure, you may have to sign in to GitHub in VSCode. Click on the Remote Explorer button on the left hand side, and then find the following option:</p> <pre><code>REMOTES(TUNNELS/SSH)\n Tunnels\n    Sign in to tunnels registered with GitHub\n</code></pre> <p>If you have not signed in to GitHub with VS Code editor, you will be redirected to the browser to sign in.</p> <p>After signing in and authorizing VSCode, the open tunnel should be visible under REMOTES (TUNNELS/SSH) -&gt; Tunnels.</p>"},{"location":"access/vscode/#using-with-uenv","title":"Using with uenv","text":"<p>To use a uenv with VSCode, the uenv must be started before calling <code>code tunnel</code>. Log into the target system and start the uenv, then start the remote server, for example: <pre><code># log into daint (this could be any other Alps cluster)\nssh daint\n# start a uenv session on the login node\nuenv start --view=default prgenv-gnu/24.11:v1\n# then start the tunnel\ncode tunnel --name=$CLUSTER_NAME-tunnel\n</code></pre></p> <p>Alternatively, you can execute <code>code tunnel</code> directly in the environment: <pre><code>ssh daint\nuenv run --view=default prgenv-gnu/24.11:v1 -- code tunnel --name=$CLUSTER_NAME-tunnel\n</code></pre></p> <p>Once the tunnel is configured, you can access it from VSCode.</p> <p>Warning</p> <p>If you plan to do any intensive work: repeated compilation of large projects or running python code in Jupyter, please see the guide to running on a compute node below. Running intensive workloads on login nodes, which are shared resources between all users, is against CSCS fair usage of Shared Resources policy.</p>"},{"location":"access/vscode/#using-with-containers","title":"Using with containers","text":"<p>Todo</p> <p>write a guide</p>"},{"location":"access/vscode/#running-on-a-compute-node","title":"Running on a compute node","text":"<p>If you plan to do computation using your VSCode, then you should first allocate resources on a compute node and set up your environment there.</p> <p>directly create the tunnel using srun</p> <p>You can directly execute the <code>code tunnel</code> command using srun: <pre><code>ssh daint\nsrun --uenv=prgenv-gnu/24.11:v1 --view=default -t120 -n1 --pty code tunnel --name=$CLUSTER_NAME-tunnel\n</code></pre></p> <ul> <li><code>--uenv</code> and <code>--view</code> set up the uenv</li> <li><code>-t120</code> requests a 2 hour (120 minute) reservation</li> <li><code>-n1</code> requests a single rank - only one rank/process is required for VSCode</li> <li><code>--pty</code> allows forwarding of terminal I/O, required to sign in to Github</li> </ul> <p>Once the job allocation is granted, you will be prompted to log into GitHub, the same as starting a session on the login node. If you don't want to use a uenv, the command is even simpler: <pre><code>ssh daint\nsrun -t120 -n1 --pty code tunnel --name=$CLUSTER_NAME-tunnel\n</code></pre></p> <p>log into a node before starting</p> <p>It is also possible to log into a compute node before executing the <code>code tunnel</code> command, if that suits your workflow: <pre><code># log into daint\nssh daint\n\n# start an interactive shell session\nsrun -t120 -n1 --pty bash\n\n# set up the environment before starting the tunnel\nuenv start prgenv-gnu/24.11:v1 --view=default\ncode tunnel --name=$CLUSTER_NAME-tunnel\n</code></pre></p> <ul> <li><code>-t120</code> requests a 2 hour (120 minute) reservation</li> <li><code>-n1</code> requests a single rank - only one rank/process is required for VSCode</li> <li><code>--pty</code> allows forwarding of terminal I/O, for bash to work interactively</li> </ul>"},{"location":"access/vscode/#connecting-via-vscode-ui","title":"Connecting via VSCode UI","text":"<p>Warning</p> <p>This approach is not recommended, because while it may be easier to connect via the VS Code UI, it is much more difficult to configure the connection so that you can use uenv, containers or compute nodes.</p> <p>Todo</p> <p>Write the guide</p>"},{"location":"access/web/","title":"Web Services","text":""},{"location":"access/web/#accessing-cscs-web-portals","title":"Accessing CSCS Web Portals","text":"<p>Most services at CSCS are connected to the CSCS Single Sign-On gate. This gives users the comfort of not having to sign in multiple times in each individual service connected to this gate and increases security. Furthermore, the Single Sign-On gate allow users to recover their forgotten passwords and authenticate using a third-party account. The login page looks like</p> <p></p>"},{"location":"access/web/#using-mfa-to-access-web-based-services","title":"Using MFA to access web-based services","text":"<p>After having completed the setup of MFA, you will be asked to enter your login/password and the OTP to access all web-based services.</p> <p>Enter username and password.</p> <p></p> <p>Then you will be prompted to enter the 6-digit code obtained from your device.</p> <p></p>"},{"location":"accounts/","title":"Index","text":""},{"location":"accounts/#getting-and-managing-accounts","title":"Getting and Managing Accounts","text":"<p>Users at CSCS have one account that can be used to access all services and systems at CSCS. To get an account you must be invited by a member of CSCS project adminstration or by a the principle investigator (PI) of a current project at CSCS.</p> <p>Getting a project at CSCS for PIs</p> <p>In order to get an account at CSCS, or to request access for the members of your team, you first need to get a project at CSCS. CSCS issues calls for proposals that are announced via the CSCS website and e-mails. More information about upcoming calls is available on the CSCS web site.</p> <p>New PIs who have sucessfully applied for a preparatory project will receive an invitation from CSCS to get an account at CSCS. PIs can then invite members of their groups to join their project.</p> <p>Info</p> <p>It is possible for users to be part of multiple projects by being invited separately by the PI of each project.</p> <p>Note</p> <p>Accounts are bound to projects, and accounts will be closed with the project unless the account is also part of another open project.</p>"},{"location":"accounts/#tools-for-managing-accounts-and-projects","title":"Tools for managing accounts and projects","text":"<p>The tool used to manage projects and accounts depends on the platform on which the project was granted:</p> <ul> <li>The HPC Platform and Climate and Weather Platform use the account and resources management tool at account.cscs.ch</li> <li>The Machine Learning Platform uses the project and resources management tool at portal.cscs.ch.</li> </ul> <p>Note</p> <p>The portal.cscs.ch site will be used to manage all projects in the future.</p>"},{"location":"accounts/#signing-up-for-a-new-account","title":"Signing up for a new account","text":"<p>New users who do not already have an account at CSCS, including PIs, need to provide the following information before CSCS can open their account:</p> <ul> <li>a scanned copy of your passport or recognised id card.<ul> <li>this will be deleted by CSCS immediately after the account has been created.</li> </ul> </li> <li>an institutional email address (gmail, hotmail, etc. will not be accepted)</li> <li>correct information (title, name, etc.)</li> </ul> <p>New accounts are usually opened within 48 hours.</p>"},{"location":"accounts/#using-different-accounts","title":"Using different accounts","text":"<p>In order to use a different account, log out of the Single Sign-On gate by going to the Account and Resources Tool and selecting \"Log out of CSCS\" on the upper-right profile icon with the tool used to manage your project, account.cscs.ch or portal.cscs.ch.</p>"},{"location":"accounts/#signing-in-with-a-third-part-account","title":"Signing in with a third-part account","text":"<p>All users at CSCS need to go through the standard registration process and get a CSCS account. In addition, they can also link their CSCS account to an external account, e.g. the one from their home institution. In this case, they can sign into the CSCS services using his/her home institution credentials instead of the CSCS username/password. This process happens only during the Single Sign-On procedure described above, and from that time on and for all purposes, and until the user logs out, the user identifier that presents itself to all CSCS services is the CSCS username, not the external one. The number of external institutions that are allowed to link their accounts is limited and displayed in the login page.</p> <p>Linking an external account can be done in the Profile section (upper-right corner) of your account page at the tool used to manage your project, account.cscs.ch or portal.cscs.ch.</p>"},{"location":"accounts/#regulations-and-policies","title":"Regulations and Policies","text":"<p>Please note that as soon as you receive and accept an invitation to get an account at CSCS, you agree to the CSCS/ETHZ regulations.</p>"},{"location":"accounts/ump/","title":"Account and Resources Management Tool","text":""},{"location":"accounts/ump/#account-and-resources-management-tool","title":"Account and Resources Management Tool","text":"<p>The Swiss National Supercomputing Centre (CSCS) offers a web-based tool for users to manage their accounts and projects at account.cscs.ch.</p> <p>With this tool, users can:</p> <ul> <li>Access their profile, manage institutional details, or reset their password.</li> <li>List the projects they belong to, including closed ones.</li> <li>Check details on each project, quotas, and current utilization.</li> <li>Get an overview of where their files are stored at CSCS (including home directories, scratch, etc.).</li> </ul> <p>For group leaders (or PIs), the tool allows:</p> <ul> <li>Managing user membership and access control.</li> <li>Inviting users to their projects via email. Existing users can accept immediately, while new users will receive instructions to create an account and join the project.</li> <li>Removing users from their projects.</li> <li>Selecting which users can access a system (and submit jobs) and which ones can only access project data.</li> <li>Defining one or more deputies to perform such tasks. Note: The responsibility of what happens within the project still belongs to the group leader or PI.</li> </ul> <p>A short guideline on how to perform these tasks is provided below.</p>"},{"location":"accounts/ump/#usage","title":"Usage","text":"<p>The tool is designed to be intuitive and comprises the following main areas:</p> <ul> <li>A) Account selector: For users with multiple accounts (e.g., service accounts).</li> <li>B) Profile management: To view and edit the account's institutional details and change the password.</li> <li>C) Project membership: To show the selected project in detail.</li> <li>D) Storage: Where users can see where they have stored their files (home, scratch, and project areas).</li> <li>E) Main view</li> </ul> <p></p>"},{"location":"accounts/ump/#membership-management-for-group-leaders-and-deputies-only","title":"Membership Management (for Group Leaders and Deputies Only)","text":"<p>To invite users to a selected project, group leaders or their deputies need to:</p> <ol> <li>Select the project on the left menu.</li> <li>Click the \"Members\" tab.</li> <li>Scroll down to the \"Users\" (or \"Deputies\" to manage deputies) section.</li> <li>Use the \"+\" (plus) button on the right of the section and enter the given and family names and email address of the invitee.    The invitee will receive instructions on how to join the project. The group leader will get a confirmation on whether the invitee has accepted or rejected the invitation.    If the invitee does not have an account, they will also receive instructions on how to create one, which needs to be verified by CSCS administration staff.</li> </ol> <p>To remove users from a selected project, group leaders or their deputies need to:</p> <ol> <li>Repeat steps 1 to 3 above.</li> <li>Use the icon with the three horizontal lines (see screenshot below) that is on the right of the user and select \"Remove user.\"</li> </ol>"},{"location":"accounts/waldur/","title":"Project and Resources Management Tool","text":""},{"location":"accounts/waldur/#the-project-and-resources-management-tool","title":"The Project and Resources Management Tool","text":"<p>CSCS Account Managers, PIs and deputy PIs can invite users to the respective projects following the below steps on CSCS's new project management portal.</p> <p>Info</p> <p>The new user project management portal is currently only used by the Machine Learning Platform All other platforms use the old user management portal</p>"},{"location":"accounts/waldur/#log-in-to-the-portal","title":"log in to the portal","text":"<p>Navigate to the site project management portal portal.cscs.ch.</p>"},{"location":"accounts/waldur/#select-the-organisation","title":"Select the Organisation","text":"<p>After login to the portal, choose the corresponding organization in which the project was created.</p> <p>Todo</p> <p>screenshot</p> <p>In this example, The project was hosted by the CSCS organization, and say the project name is <code>csstaff_n</code>, From the organization dashboard navigate to Projects and click on <code>csstaff_n</code> Project</p> <p>Todo</p> <p>screenshot</p>"},{"location":"accounts/waldur/#invite-users","title":"Invite users","text":"<p>From the project dashboard, navigate to Team -&gt; Invitations</p> <p>Todo</p> <p>screenshot</p> <p>Info</p> <p>Using both the web interface and bulk invitation, the following roles can be assigned in the tool:</p> <ul> <li>Project administrator: PI</li> <li>Project manager:  deputy PI</li> <li>Project member: team member</li> </ul> invite individual usersbulk invite <p>To invite a user, click on the \"Invite Users\" button on the right hand side of the tab.</p> <p>Todo</p> <p>screenshot</p> <p>Todo</p> <p>screenshot</p> <p>It is also possible to bulk invite users by preparing a CSV file and uploading it in this step.</p> <pre><code>Email,Role,Project\nCragAlvarado@example.com,Project member,prj02\nAndrease@example.com,Project member,prj02\nJoannWaters@example.com,Project administrator,prj02\nDonnaSchwartz@example.com,Project manager,prj02\n</code></pre> <p>Note</p> <p>An email will be sent to the invited user:</p> <ul> <li>users who already have CSCS accounts should click on the link in the email they received, and authenticate against CSCS KeyCloak with username, password, and OTP to accept the invitation.</li> <li>new users should follow the procedure to create a CSCS account.</li> </ul>"},{"location":"alps/","title":"Index","text":""},{"location":"alps/#alps-infrastructure","title":"Alps Infrastructure","text":"<p>Alps is a general-purpose compute and data Research Infrastructure (RI) open to the broad community of researchers in Switzerland and the rest of the world. Alps provides a high impact, challenging and innovative RI that will allows Switzerland to advance science and impact society.</p> <p>Alps enables the creation of versatile clusters (vClusters) that can be tailored to the specific needs of users while maintaining confidentiality. For example, a vCluster will be dedicated to MeteoSwiss\u2019 numerical weather forecasts, another one to the User Lab and another one to Machine Learning and Artificial Intelligence.</p> <p>A key feature of Alps is multi-tenancy, where tenants are organizations, typically a research institution, that deploys, operates, or manages its platform on the Alps infrastructure. Tenants have privileged access to resource nodes, enabling them to deploy their own services and resource configurations. Additionally, network segregation ensures secure and isolated communication, with the option to connect to the tenant's private network.</p> <ul> <li> <p> Platforms</p> <p> Alps Platforms</p> </li> <li> <p> Clusters</p> <p>The resources on Alps are partitioned and configured into versatile software defined clusters (vClusters).</p> <p> Alps vClusters</p> </li> <li> <p> Hardware</p> <p>Learn about the node types and networking infrastructure in Alps.</p> <p> Alps Hardware</p> </li> <li> <p> Storage</p> <p>Learn about the file systems attached to Alps.</p> <p> Alps Storage</p> </li> </ul>"},{"location":"alps/clusters/","title":"Clusters","text":""},{"location":"alps/clusters/#alps-clusters","title":"Alps Clusters","text":"<p>A vCluster (versatile software-defined cluster) is a logical partition of the supercomputing resources where platform services are deployed. It serves as a dedicated environment supporting a specific platform. The composition of resources and services for each vCluster is defined in a configuration file used by an automated pipeline for deployment. Once deployed by CSCS, the vCluster becomes immutable.</p>"},{"location":"alps/clusters/#clusters-on-alps","title":"Clusters on Alps","text":"<p>Clusters on Alps are provided as part of different platforms.</p> <ul> <li> <p> Machine Learning Platform</p> <p>Clariden is the main Grace-Hopper cluster</p> <p> Clariden</p> <p>Bristen is a small system with a100 nodes, used for todo</p> <p> Bristen</p> </li> </ul> <ul> <li> <p> HPC Platform</p> <p>Daint is the main Grace-Hopper cluster for GPU workloads</p> <p> Daint</p> <p>Eiger is a large AMD-CPU cluster for CPU workloads</p> <p> Eiger</p> </li> </ul> <ul> <li> <p> Climate and Weather Platform</p> <p>Santis is a Grace-Hopper cluster for climate and weather simulation</p> <p> Santis</p> </li> </ul>"},{"location":"alps/hardware/","title":"Hardware","text":""},{"location":"alps/hardware/#alps-hardware","title":"Alps Hardware","text":"<p>Alps is a HPE Cray EX3000 system, a liquid cooled blade-based, high-density system.</p> <p>Todo</p> <p>this is a skeleton - all of the details need to be filled in</p>"},{"location":"alps/hardware/#alps-cabinets","title":"Alps Cabinets","text":"<p>The basic building block of the system is a liquid-cooled cabinet. A single cabinet can accommodate up to 64 compute blade slots within 8 compute chassis. The cabinet is not configured with any cooling fans. All cooling needs for the cabinet are provided by direct liquid cooling and the CDU. This approach to cooling provides greater efficiency for the rack-level cooling, decreases power costs associated with cooling (no blowers) and utilizes a single water source per CDU One cabinet supports the following:</p> <ul> <li>8 compute chassis</li> <li>4 power shelves with a maximum of 6 rectifiers per shelf- 24 total 12.5 or 15kW rectifiers per cabinet</li> <li>4 PDUs (1 per power shelf)</li> <li>3 power input whips (3-phase)</li> <li>Maximum of 64 quad-blade compute blades</li> <li>Maximum of 64 Slingshot switch blades</li> </ul> <p></p>"},{"location":"alps/hardware/#alps-high-speed-network","title":"Alps High Speed Network","text":"<p>Todo</p> <p>information about the network.</p> <ul> <li>Details about SlingShot 11.<ul> <li>how many NICS per node</li> <li>raw feeds and speeds</li> </ul> </li> <li>Some OSU benchmark results.</li> <li>GPU-aware communication</li> <li>slingshot is not infiniband - there is no NVSwitch</li> </ul>"},{"location":"alps/hardware/#alps-nodes","title":"Alps Nodes","text":"<p>Alps was installed in phases, starting with the installation of 1024 AMD Rome dual socket CPU nodes in 2020, through to the main installation of 2,688 Grace-Hopper nodes in 2024.</p> <p>There are currently four node types in Alps, with another becoming available in 2025:</p> type blades nodes CPU sockets GPU devices NVIDIA GH200 1344 2688 10,752 10,752 AMD Rome 256 1024 2,048 -- NVIDIA A100 72 144 144 576 AMD MI250x 12 24 24 96 AMD MI300A 64 128 512 512 <p></p>"},{"location":"alps/hardware/#nvidia-gh200-gpu-nodes","title":"NVIDIA GH200 GPU Nodes","text":"<p>Perry Peak</p> <p></p>"},{"location":"alps/hardware/#amd-rome-cpu-nodes","title":"AMD Rome CPU Nodes","text":"<p>EX425</p> <p></p>"},{"location":"alps/hardware/#nvidia-a100-gpu-nodes","title":"NVIDIA A100 GPU Nodes","text":"<p>Grizzly Peak</p> <p></p>"},{"location":"alps/hardware/#amd-mi250x-gpu-nodes","title":"AMD MI250x GPU Nodes","text":"<p>Bard Peak</p> <p></p>"},{"location":"alps/hardware/#amd-mi300a-gpu-nodes","title":"AMD MI300A GPU Nodes","text":"<p>Parry Peak</p> <p>coming soon</p> <p>H1 2025</p>"},{"location":"alps/platforms/","title":"Platforms","text":""},{"location":"alps/platforms/#platforms-on-alps","title":"Platforms on Alps","text":"<p>A platform represents a set of scientific services along with compute and data resources hosted on the Alps research infrastructure, provided to a specific scientific community. Each platform addresses particular research needs and domains, such as climate and weather modeling, machine learning, or high-performance computing applications. A platform can consist of one or multiple clusters, and its services can be managed either by CSCS or by the scientific community itself, including access control, usage policies, and support.</p> <ul> <li> <p> Machine Learning Platform</p> <p>The Machine Learning Platform (MLP) hosts ML and AI researchers.</p> <p> MLP</p> </li> <li> <p> HPC Platform</p> <p>Todo</p> <p> HPCP</p> </li> <li> <p> Climate and Weather Platform</p> <p>The Climate and Weather Platform (CWP) provides resources to the climate modeling community.</p> <p> CWP</p> </li> </ul>"},{"location":"alps/storage/","title":"Storage","text":""},{"location":"alps/storage/#alps-storage","title":"Alps Storage","text":"<p>Alps has different storage attached, each with characteristics suited to different workloads and use cases. HPC storage is manged in a separate cluster of nodes that host servers that manage the storage and the physical storage drives. These separate clusters are on the same Slingshot 11 network as the Alps.</p> Capstor IOPStor Vast Model HPE ClusterStor E1000D HPE ClusterStor E1000F Vast Type Lustre Lustre NFS Capacity 129 PB raw GridRAID 7.2 PB raw RAID 10 1 PB Number of Drives 8,480 16 TB HDD 240 * 30 TB NVME SSD N/A Read Speed 1.19 TB/s 782 GB/s 38 GB/s Write Speed 1.09 TB/s 393 GB/s 11 GB/s IOPs 1.5M 8.6M read, 24M write 200k read, 768k write file create/s 374k 214k 97k <p></p>"},{"location":"alps/storage/#capstor","title":"capstor","text":"<p>Capstor is the largest file system, for storing large amounts of input and output data. It is used to provide SCRATCH and STORE for different clusters - the precise details are platform-specific.</p> <p></p>"},{"location":"alps/storage/#iopstor","title":"iopstor","text":"<p>Todo</p> <p>small text explaining what iopstor is designed to be used for.</p> <p></p>"},{"location":"alps/storage/#vast","title":"vast","text":"<p>The Vast storage is smaller capacity system that is designed for use as home folders.</p> <p>Todo</p> <p>small text explaining what iopstor is designed to be used for.</p> <p>The mounts, and how they are used for SCRATCH, STORE, PROJECT, HOME would be in the storage docs</p>"},{"location":"build-install/","title":"Building and Installing Software","text":"<p>CSCS provides commonly used software and tools on Alps, however many use cases will require first installing software on a system before you can start working.</p> <p>Modern HPC applications and software stacks are often very complicated, and there is no one-size-fits-all method for building and installing them.</p>"},{"location":"build-install/#programming-environments","title":"Programming environments","text":"<ul> <li> uenv \u2013 uenv provide isolated software environments for applications and developers.</li> <li> CPE \u2013 the Cray Programming Environment is provided as-is on Alps.</li> </ul>"},{"location":"build-install/#python","title":"Python","text":"<p>There are multiple ways to install Python software.</p> <ul> <li> pip \u2013 create a virtual environment using python in a uenv</li> </ul>"},{"location":"build-install/#containers","title":"Containers","text":"<p>CSCS provides tools for building software in containers</p>"},{"location":"build-install/containers/","title":"Containers","text":""},{"location":"build-install/containers/#building-container-images-on-alps","title":"Building container images on Alps","text":"<p>Building OCI container images on Alps vClusters is supported through Podman, an open-source container engine that adheres to OCI standards and supports rootless containers by leveraging Linux user namespaces. Its command-line interface (CLI) closely mirrors Docker\u2019s, providing a consistent and familiar experience for users of established container tools.</p>"},{"location":"build-install/containers/#preliminary-step-configuring-podmans-storage","title":"Preliminary step: configuring Podman's storage","text":"<p>The first step in order to use Podman on Alps is to create a valid Container Storage configuration file at <code>$HOME/.config/containers/storage.conf</code>, according to the following minimal template:</p> <pre><code>[storage]\ndriver = \"overlay\"\nrunroot = \"/dev/shm/$USER/runroot\"\ngraphroot = \"/dev/shm/$USER/root\"\n</code></pre> <p>Warning</p> <p>In the above configuration, <code>/dev/shm</code> is used to store the container images. <code>/dev/shm</code> is the mount point of a tmpfs filesystem and is compatible with the user namespaces used by Podman. The limitation of this approach  is that container images created during a job allocation are deleted when the job ends. Therefore, the image needs to either be pushed to a container registry or imported by the Container Engine before the job allocation finishes.</p>"},{"location":"build-install/containers/#building-images-with-podman","title":"Building images with Podman","text":"<p>The easiest way to build a container image is to rely on a Containerfile (a more generic name for a container image recipe, but essentially equivalent to Dockerfile):</p> <pre><code># Allocate a compute node and open an interactive terminal on it\nsrun --pty --partition=&lt;partition&gt; bash\n\n# Change to the directory containing the Containerfile/Dockerfile and build the image\npodman build -t &lt;image:tag&gt; .\n</code></pre> <p>In general, <code>podman build</code> follows the Docker options convention.</p>"},{"location":"build-install/containers/#importing-images-in-the-container-engine","title":"Importing images in the Container Engine","text":"<p>An image built using Podman can be easily imported as a squashfs archive in order to be used with our Container Engine solution. It is important to keep in mind that the import has to take place in the same job allocation where the image creation took place, otherwise the image is lost due to the temporary nature of <code>/dev/shm</code>.</p> <p>To import the image:</p> <pre><code>enroot import -x mount -o &lt;image_name.sqsh&gt; podman://&lt;image:tag&gt;\n</code></pre> <p>The resulting <code>&lt;image_name.sqsh&gt;</code> can used directly as an explicitly pulled container image, as documented in Container Engine. An example Environment Definition File (EDF) using the imported image looks as follows:</p> <pre><code>image = \"/&lt;path to image directory&gt;/&lt;image_name.sqsh&gt;\"\nmounts = [\"/capstor/scratch/cscs/&lt;username&gt;:/capstor/scratch/cscs/&lt;username&gt;\"]\nworkdir = \"/capstor/scratch/cscs/&lt;username&gt;\"\n</code></pre>"},{"location":"build-install/containers/#pushing-images-to-a-container-registry","title":"Pushing Images to a Container Registry","text":"<p>In order to push an image to a container registry, you first need to follow three steps:</p> <ol> <li>Use your credential to login to the container registry with podman login.</li> <li>Tag the image according to the name of your container registry and the corresponding repository, using podman tag. This step can be skipped if you already provided the appropriate tag when building the image.</li> <li>Push the image using podman push.</li> </ol> <pre><code># Login to a container registry using username/password interactively\npodman login &lt;registry_url&gt;\n\n# Tag the image accordingly\npodman tag &lt;image:tag&gt; &lt;registry url&gt;/&lt;image:tag&gt;\n\n# Push the image (for docker type registries use the docker:// prefix)\npodman push docker://&lt;registry url&gt;/&lt;image:tag&gt;\n</code></pre> <p>For example, to push an image to the DockerHub container registry, the following steps have to be performed:</p> <pre><code># Login to DockerHub (Podman will ask for your credentials)\npodman login docker.io\n\n# Tag the image based on your username\npodman tag &lt;image:tag&gt; docker.io/&lt;username&gt;/myimage:latest\n\n# Push the image to the repository of your choice\npodman push docker://docker.io/&lt;username&gt;/myimage:latest\n</code></pre>"},{"location":"build-install/cpe/","title":"Cray Programming Environment (CPE)","text":"<p>Warning</p> <p>you don't want to use this</p> <p>Mlp</p> <p>The CPE is not provided on the machine learning platform.</p> <p>Cwp</p> <p>The CPE is not provided on the climage and weather platform</p>"},{"location":"build-install/pip/","title":"Installing Python software with pip","text":"<p>todo</p>"},{"location":"build-install/uenv/","title":"uenv","text":"<p>Uenv are user environments that provide scientific applications, libraries and tools on Alps.  This article explains how to use them to build software.</p> <p>For more documentation on how to find, download and use uenv in your workflow, see the uenv documentation.</p> <p></p>"},{"location":"build-install/uenv/#building-software-using-spack","title":"Building software using Spack","text":"<p>Each uenv is tightly coupled with Spack and can be used as an upstream Spack instance, because the software in uenv is built with Spack using the Stackinator tool.</p> <p>CSCS provides <code>uenv-spack</code> - a tool that can be used to quickly install software using the software and configuration provided inside a uenv, similarly to how <code>module load</code> loads software packages.</p>"},{"location":"build-install/uenv/#installing-uenv-spack","title":"Installing <code>uenv-spack</code>","text":"<pre><code>git clone https://github.com/eth-cscs/uenv-spack.git # (1)!\n\n(cd uenv-spack &amp;&amp; ./bootstrap) # (2)!\n\nexport PATH=$PWD/uenv-spack:$PATH\n</code></pre> <ol> <li> <p>Download the <code>uenv-spack</code> tool from GitHub.</p> </li> <li> <p>Initialize the <code>uenv-spack</code> tool.</p> </li> </ol>"},{"location":"build-install/uenv/#select-the-uenv","title":"Select the uenv","text":"<p>The next step is to choose which uenv to use. The uenv will provide the compilers, Cray MPICH, and other libraries and tools.</p> <pre><code>graph TD\n  A[/is there a uenv for the application?\\] --&gt;|yes| B[use that image, e.g. **gromacs**]\n  A --&gt; |no| C[/do I need OpenACC or CUDA Fortran?\\]\n  C --&gt; |no| D[use **prgenv-gnu**]\n  C --&gt; |yes| E[/are you _really_ sure?\\]\n  E --&gt; |yes| F[use **prgenv-nvfortran**]\n  E --&gt; |no| D</code></pre> <p>Use <code>prgenv-gnu</code> when in doubt</p> <p>If you don't know where to start, use the latest release of the <code>prgenv-gnu</code> on the system that you are targeting. It provides the latest versions of <code>gcc</code>, <code>cray-mpich</code>, <code>python</code> and commonly used libraries like <code>fftw</code> and <code>boost</code>.</p> <p>On systems that have NVIDIA GPUs (<code>gh200</code> and <code>a100</code> uarch), it also provides the latest version of <code>cuda</code> and <code>nccl</code>, and it is configured for GPU-aware MPI communication.</p> <p>To use an uenv as an upstream Spack instance, the uenv has to be started with the <code>spack</code> view:</p> <pre><code>uenv start prgenv-gnu/24.11:v1 --view=spack\n</code></pre> <p>What does the <code>spack</code> view do?</p> <p>The <code>spack</code> view sets environment variables that provide information about the version of Spack that was used to build the uenv, and where the uenv Spack configuration is stored.</p> variable example description <code>UENV_SPACK_CONFIG_PATH</code> <code>user-environment/config</code> the path of the upstream spack configuration files. <code>UENV_SPACK_REF</code> <code>releases/v0.23</code> the branch or tag used - this might be empty if a specific commit of Spack was used. <code>UENV_SPACK_URL</code> <code>https://github.com/spack/spack.git</code> The git repository for Spack - nearly always the main spack/spack repository. <code>UENV_SPACK_COMMIT</code> <code>c6d4037758140fe...0cd1547f388ae51</code> The commit of Spack that was used <p>Warning</p> <p>The environment variables set by the <code>spack</code> view are scoped by <code>UENV_</code>. Therefore, they don't change Spack-related environment variables. You can use them to consistently set Spack-related environment variables.</p> Upstream Spack version <p>It is strongly recomended that your version of Spack and the version of Spack in the uenv match when building software on top of an uenv.</p> <p>Advanced Spack users</p> <p>Advanced Spack users can use the environment variables set by the <code>spack</code> view to manually configure the uenv as a Spack upstream instance.</p> <p>Tip</p> <p>If using multiple uenvs, we recommend using a different Spack instance per uenv.</p> Setting Spack configuration path <pre><code>export SPACK_SYSTEM_CONFIG_PATH=$UENV_SPACK_CONFIG_PATH\n</code></pre>"},{"location":"build-install/uenv/#describing-what-to-build","title":"Describing what to build","text":"<p>The next step is to describe what software to build. This is done using a Spack environment file and a Spack package repository.</p> <p>The <code>uenv-spack</code> tool can be used to create a build directory with a template Spack environment file (<code>spack.yaml</code>) and a Spack package repository (<code>repo/</code> directory).</p> <p>Create a build directory with a Spack environment file and a Spack package repository</p> <pre><code>uenv-spack &lt;build-path&gt; --uarch=gh200\ncd &lt;build-path&gt;\n./build\n</code></pre> <p><code>&lt;build-path&gt;</code> is a path (typically in <code>$SCRATCH</code>, e.g. <code>$SCRATCH/builds/gromacs-24.11</code>).</p> <p><code>uenv-spack</code> creates a directory tree with the following contents:</p> <pre><code>&lt;build-path&gt;\n\u251c\u2500 build # (1)!\n\u251c\u2500 spack # (2)!\n\u251c\u2500 config # (3)!\n\u2502   \u251c\u2500 meta.json # (4)!\n\u2502   \u251c\u2500 user\n\u2502   \u2502  \u251c\u2500 config.yaml\n\u2502   \u2502  \u251c\u2500 modules.yaml\n\u2502   \u2502  \u2514\u2500 repos.yaml\n\u2502   \u2514\u2500 system\n\u2502      \u251c\u2500 compilers.yaml\n\u2502      \u251c\u2500 packages.yaml\n\u2502      \u251c\u2500 repos.yaml\n\u2502      \u2514\u2500 upstreams.yaml\n\u2514\u2500 env # (5)!\n    \u251c\u2500 spack.yaml # (6)!\n    \u2514\u2500 repo # (7)!\n       \u251c\u2500 repo.yaml\n       \u2514\u2500 packages\n</code></pre> <ol> <li>Script to build the software stack.</li> <li><code>git</code> clone of the required version of Spack.</li> <li>Spack onfiguration files for the software stack.</li> <li>Information about the uenv that was used to run <code>uenv-spack</code>.</li> <li>Description of the software to build.</li> <li>Template Spack environment file.</li> <li>Empty Spack package repository.</li> </ol> <p>The <code>env</code> path contains a template <code>spack.yaml</code> file, and an empty Spack package repository:</p> <pre><code>env\n\u251c\u2500 spack.yaml\n\u2514\u2500 repo\n   \u251c\u2500 repo.yaml\n   \u2514\u2500 packages\n</code></pre> <p>where the <code>spack.yaml</code> file contains an empty list of specs:</p> <pre><code>    specs: []\n</code></pre> <p>Edit this file to add the specs that you wish to build, for example:</p> <pre><code>    specs: [tree, screen, emacs +treesitter]\n</code></pre> <p>The step of adding a list of specs to the <code>spack.yaml</code> template can be skipped by providing them using the <code>--specs</code> argument to <code>uenv-spack</code>.</p> <p>Create a build path and populate the <code>spack.yaml</code> file with some Spack specs</p> <pre><code>uenv-spack $SCRATCH/install/tools --uarch=gh200 \\\n           --specs=\"tree, screen, emacs +treesitter\"\ncd $SCRATCH/install/tools\n./build\n</code></pre> <p>If you already have a directory with a complete <code>spack.yaml</code> file and custom repo, you can provide it as an argument to <code>uenv-spack</code>:</p> <p>Create a build path and use a pre-configured <code>spack.yaml</code> and <code>repo</code></p> <pre><code>uenv-spack $SCRATCH/install/arbor --uarch=gh200 \\\n           --recipe=&lt;path-to-recipe&gt;\ncd $SCRATCH/install/tools\n./build\n</code></pre> Create a build path and use your own <code>spack.yaml</code> <p>NOT YET IMPLEMENTED</p> <pre><code>uenv-spack $SCRATCH/install/arbor --uarch=gh200 \\\n           --recipe=&lt;path-to-spack.yaml&gt;\ncd $SCRATCH/install/tools\n./build\n</code></pre>"},{"location":"build-install/uenv/#build-the-software","title":"Build the software","text":"<p>Once specs have been added to <code>spack.yaml</code>, you can build the image using the <code>build</code> script that was generated in <code>&lt;build-dir&gt;</code>:</p> <pre><code>./build\n</code></pre> <p>This process will take a while, because the version of Spack that was downloaded needs to</p> <ul> <li>bootstrap,</li> <li>concretise the environment,</li> <li>and build all of the packages.</li> </ul> <p>The duration of the build depends on the specs: some specs may require a long time to build, or require installing many dependencies.</p> <p>The build step generates multiple outputs, described below.</p>"},{"location":"build-install/uenv/#installed-packages","title":"Installed packages","text":"<p>The packages built by Spack are installed in <code>&lt;build-dir&gt;/store</code>.</p>"},{"location":"build-install/uenv/#spack-view","title":"Spack view","text":"<p>A Spack view is generated in <code>&lt;build-dir&gt;/view</code>.</p>"},{"location":"build-install/uenv/#modules","title":"Modules","text":"<p>Module files are generated in the <code>module</code> sub-directory of the <code>&lt;build-path&gt;</code></p> <p>To use them, add them to the module environment</p> <pre><code>module use &lt;build-dir&gt;/modules # (1)!\nmodule avail # (2)!\n</code></pre> <ol> <li>Make modules available.</li> <li>Check that the modules are available.</li> </ol> <p>Note</p> <p>The generation of modules can be customised by editing the <code>&lt;build-dir&gt;/config/user/modules.yaml</code> file before running <code>build</code>. See the Spack modules documentation.</p>"},{"location":"build-install/uenv/#use-the-software","title":"Use the software","text":"<p>Warning</p> <p>This step is not fully covered by the tool/workflow yet.</p> <p>Warning</p> <p>The uenv that was used to configure and build must always be loaded when using the software stack.</p> <p>To use the installed software, you have the following options:</p> <ul> <li>Loading modules</li> <li>Activate the Spack view</li> <li><code>source &lt;build-dir&gt;/spack/share/spack/setup-env.sh</code> and then use Spack</li> </ul>"},{"location":"clusters/bristen/","title":"bristen","text":""},{"location":"clusters/bristen/#bristen","title":"Bristen","text":"<p>Todo</p> <p>use the clariden as template.</p>"},{"location":"clusters/clariden/","title":"clariden","text":""},{"location":"clusters/clariden/#clariden","title":"Clariden","text":"<p>Clariden is an Alps cluster that provides GPU accelerators and file systems designed to meet the needs of machine learning workloads in the MLP.</p>"},{"location":"clusters/clariden/#cluster-specification","title":"Cluster Specification","text":""},{"location":"clusters/clariden/#compute-nodes","title":"Compute Nodes","text":"<p>Clariden consists of around 1200 Grace-Hopper nodes. The number of nodes can change when nodes are added or removed from other clusters on Alps.</p> node type number of nodes total CPU sockets total GPUs gh200 1,200 4,800 4,800 <p>Most nodes are in the <code>normal</code> slurm partition, while a few nodes are in the <code>debug</code> partition.</p>"},{"location":"clusters/clariden/#storage-and-file-systems","title":"Storage and file systems","text":"<p>Clariden uses the MLp filesystems and storage policies.</p>"},{"location":"clusters/clariden/#getting-started","title":"Getting started","text":""},{"location":"clusters/clariden/#logging-into-clariden","title":"Logging into Clariden","text":"<p>To connect to Clariden via SSH, first refer to the ssh guide.</p> <p><code>~/.ssh/config</code></p> <p>Add the following to your SSH configuration to enable you to directly connect to clariden using <code>ssh clariden</code>. <pre><code>Host clariden\n    HostName clariden.alps.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n</code></pre></p>"},{"location":"clusters/clariden/#software","title":"Software","text":"<p>Users are encouraged to use containers on Clariden.</p> <ul> <li>Jobs using containers can be easily set up and submitted using the container engine.</li> <li>To build images, see the guide to building container images on Alps.</li> </ul> <p>Alternatively, uenv are also available on Clariden. Currently the only uenv that is deployed on Clariden is prgenv-gnu.</p> using uenv provided for other clusters <p>You can run uenv that were built for other Alps clusters using the <code>@</code> notation. For example, to use uenv images for daint: <pre><code># list all images available for daint\nuenv image find @daint\n\n# download an image for daint\nuenv image pull namd/3.0:v3@daint\n\n# start the uenv\nuenv start namd/3.0:v3@daint\n</code></pre></p>"},{"location":"clusters/clariden/#running-jobs-on-clariden","title":"Running Jobs on Clariden","text":""},{"location":"clusters/clariden/#slurm","title":"SLURM","text":"<p>Clariden uses SLURM as the workload manager, which is used to launch and monitor distributed workloads, such as training runs.</p> <p>There are two slurm partitions on the system:</p> <ul> <li>the <code>normal</code> partition is for all production workloads.</li> <li>the <code>debug</code> partition can be used to access a small allocation for up to 30 minutes for debugging and testing purposes.</li> <li>the <code>xfer</code> partition is for internal data transfer at CSCS.</li> </ul> name nodes max nodes per job time limit <code>normal</code> 1266 - 24 hours <code>debug</code> 32 2 30 minutes <code>xfer</code> 2 1 24 hours <ul> <li>nodes in the <code>normal</code> and <code>debug</code> partitions are not shared</li> <li>nodes in the <code>xfer</code> partition can be shared</li> </ul> <p>See the SLURM documentation for instructions on how to run jobs on the Grace-Hopper nodes.</p> how to check the number of nodes on the system <p>You can check the size of the system by running the following command in the terminal: <pre><code>$ sinfo --format \"| %20R | %10D | %10s | %10l | %10A |\"\n| PARTITION            | NODES      | JOB_SIZE   | TIMELIMIT  | NODES(A/I) |\n| debug                | 32         | 1-2        | 30:00      | 3/29       |\n| normal               | 1266       | 1-infinite | 1-00:00:00 | 812/371    |\n| xfer                 | 2          | 1          | 1-00:00:00 | 1/1        |\n</code></pre> The last column shows the number of nodes that have been allocated in currently running jobs (<code>A</code>) and the number of jobs that are idle (<code>I</code>).</p>"},{"location":"clusters/clariden/#firecrest","title":"FirecREST","text":"<p>Clariden can also be accessed using FircREST at the <code>https://api.cscs.ch/ml/firecrest/v1</code> API endpoint.</p>"},{"location":"clusters/clariden/#maintenance-and-status","title":"Maintenance and status","text":""},{"location":"clusters/clariden/#scheduled-maintenance","title":"Scheduled Maintenance","text":"<p>Wednesday morning 8-12 CET is reserved for periodic updates, with services potentially unavailable during this timeframe. If the queues must be drained (redeployment of node images, rebooting of compute nodes, etc) then a Slurm reservation will be in place that will prevent jobs from running into the maintenance window. </p> <p>Exceptional and non-disruptive updates may happen outside this time frame and will be announced to the users mailing list, and on the CSCS status page.</p>"},{"location":"clusters/clariden/#change-log","title":"Change log","text":"<p>2025-03-05 container engine updated</p> <p>now supports better containers that go faster. Users do not to change their workflow to take advantage of these updates.</p> 2024-10-07 old event <p>this is an old update. Use <code>???</code> to automatically fold the update.</p>"},{"location":"clusters/clariden/#known-issues","title":"Known issues","text":""},{"location":"clusters/daint/","title":"daint","text":""},{"location":"clusters/daint/#daint","title":"Daint","text":""},{"location":"clusters/eiger/","title":"eiger","text":""},{"location":"clusters/eiger/#eiger","title":"Eiger","text":""},{"location":"clusters/santis/","title":"santis","text":""},{"location":"clusters/santis/#santis","title":"Santis","text":"<p>Santis is an Alps cluster that provides GPU accelerators and file systems designed to meet the needs of climate and weather models for the CWP.</p>"},{"location":"clusters/santis/#cluster-specification","title":"Cluster specification","text":""},{"location":"clusters/santis/#compute-nodes","title":"Compute nodes","text":"<p>Santis consists of around 600 Grace-Hopper nodes.</p> <p>Note</p> <p>In late March 2025 Santis was temporarily expanded to 1233 nodes for Gordon Bell and HPL runs.</p> <p>The number of nodes can change when nodes are added or removed from other clusters on Alps.</p> <p>There are four login nodes, labelled <code>santis-ln00[1-4]</code>. You will be assigned to one of the four login nodes when you ssh onto the system, from where you can edit files, compile applications and start simulation jobs.</p> node type number of nodes total CPU sockets total GPUs gh200 600 2,400 2,400"},{"location":"clusters/santis/#storage-and-file-systems","title":"Storage and file systems","text":"<p>Santis uses the CWP filesystems and storage policies.</p>"},{"location":"clusters/santis/#getting-started","title":"Getting started","text":""},{"location":"clusters/santis/#logging-into-santis","title":"Logging into Santis","text":"<p>To connect to Santis via SSH, first refer to the ssh guide.</p> <p><code>~/.ssh/config</code></p> <p>Add the following to your SSH configuration to enable you to directly connect to santis using <code>ssh santis</code>. <pre><code>Host santis\n    HostName santis.alps.cscs.ch\n    ProxyJump ela\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n</code></pre></p>"},{"location":"clusters/santis/#software","title":"Software","text":"<p>CSCS and the user community provide software environments tailored to  uenv are also available on Santis.</p> <p>Currently, the following uenv are provided for the climate and weather community</p> <ul> <li><code>icon/25.1</code></li> <li><code>climana/25.1</code></li> </ul> <p>In adition to the climate and weather uenv, all of the </p> using uenv provided for other clusters <p>You can run uenv that were built for other Alps clusters using the <code>@</code> notation. For example, to use uenv images for daint: <pre><code># list all images available for daint\nuenv image find @daint\n\n# download an image for daint\nuenv image pull namd/3.0:v3@daint\n\n# start the uenv\nuenv start namd/3.0:v3@daint\n</code></pre></p> <p>It is also possible to use HPC containers on Santis:</p> <ul> <li>Jobs using containers can be easily set up and submitted using the container engine.</li> <li>To build images, see the guide to building container images on Alps.</li> </ul>"},{"location":"clusters/santis/#running-jobs-on-santis","title":"Running jobs on Santis","text":""},{"location":"clusters/santis/#slurm","title":"SLURM","text":"<p>Santis uses SLURM as the workload manager, which is used to launch and monitor distributed workloads, such as training runs.</p> <p>There are two slurm partitions on the system:</p> <ul> <li>the <code>normal</code> partition is for all production workloads.</li> <li>the <code>debug</code> partition can be used to access a small allocation for up to 30 minutes for debugging and testing purposes.</li> <li>the <code>xfer</code> partition is for internal data transfer at CSCS.</li> </ul> name nodes max nodes per job time limit <code>normal</code> 1266 - 24 hours <code>debug</code> 32 2 30 minutes <code>xfer</code> 2 1 24 hours <ul> <li>nodes in the <code>normal</code> and <code>debug</code> partitions are not shared</li> <li>nodes in the <code>xfer</code> partition can be shared</li> </ul> <p>See the SLURM documentation for instructions on how to run jobs on the Grace-Hopper nodes.</p> how to check the number of nodes on the system <p>You can check the size of the system by running the following command in the terminal: <pre><code>$ sinfo --format \"| %20R | %10D | %10s | %10l | %10A |\"\n| PARTITION            | NODES      | JOB_SIZE   | TIMELIMIT  | NODES(A/I) |\n| debug                | 32         | 1-2        | 30:00      | 3/29       |\n| normal               | 1266       | 1-infinite | 1-00:00:00 | 812/371    |\n| xfer                 | 2          | 1          | 1-00:00:00 | 1/1        |\n</code></pre> The last column shows the number of nodes that have been allocted in currently running jobs (<code>A</code>) and the number of jobs that are idle (<code>I</code>).</p>"},{"location":"clusters/santis/#firecrest","title":"FirecREST","text":"<p>Santis can also be accessed using FircREST at the <code>https://api.cscs.ch/ml/firecrest/v1</code> API endpoint.</p>"},{"location":"clusters/santis/#maintenance-and-status","title":"Maintenance and status","text":""},{"location":"clusters/santis/#scheduled-maintenance","title":"Scheduled maintenance","text":"<p>Wednesday morning 8-12 CET is reserved for periodic updates, with services potentially unavailable during this timeframe. If the queues must be drained (redeployment of node images, rebooting of compute nodes, etc) then a Slurm reservation will be in place that will prevent jobs from running into the maintenance window. </p> <p>Exceptional and non-disruptive updates may happen outside this time frame and will be announced to the users mailing list, and on the CSCS status page.</p>"},{"location":"clusters/santis/#change-log","title":"Change log","text":"<p>2025-03-05 container engine updated</p> <p>now supports better containers that go faster. Users do not to change their workflow to take advantage of these updates.</p> 2024-10-07 old event <p>this is an old update. Use <code>???</code> to automatically fold the update.</p>"},{"location":"clusters/santis/#known-issues","title":"Known issues","text":""},{"location":"contributing/","title":"Contributing","text":"<p>This documentation is developed using the Material for MkDocs framework, and the source code for the docs is publicly available on GitHub. This means that everybody, CSCS staff and the CSCS user community can contribute to the documentation.</p>"},{"location":"contributing/#getting-started","title":"Getting started","text":"<p>We use the GitHub fork and pull request model for development:</p> <ul> <li>First create a fork of the main GitHub repository.</li> <li>Make all proposed changes in branches on your fork - don't make branches on the main repository (we reserve the right to block creating branches on the main repository).</li> </ul> <p>Clone your fork repository on your PC/laptop: <pre><code># clone your fork of the repository\ngit clone git@github.com:${githubusername}/cscs-docs.git\ncd cscs-docs\ngit switch -c 'fix/ssh-alias'\n# ... make your edits ...\n# add and commit your changes\ngit add &lt;files&gt;\ngit commit -m 'update the ssh docs with aliases for all user lab vclusters'\ngit push origin 'fix/ssh-alias'\n</code></pre> Then navigate to GitHub, and create a pull request.</p> <p>The <code>serve</code> script in the root path of the repository can be used to view the docs locally:<code><pre><code>./serve\n...\nINFO    -  [08:33:34] Serving on http://127.0.0.1:8000/\n</code></pre> This generates the documentation locally, which can be viewed using a local link, which is</code>http://127.0.0.1:8000/` by default. The documentation will be rebuilt and the webpage reloaded when changed files are saved.</p> <p>Note</p> <p>To run the serve script, you need to first install uv.</p> <p>To build the docs in a <code>site</code> sub-directory: <pre><code>./serve build\n</code></pre></p>"},{"location":"contributing/#review-process","title":"Review process","text":"<p>Documentation is owned by everybody - so don't be afraid to jump in and make changes or fixes where you see that there is something missing or outdated.</p> <p>If you plan to make large changes or contributions, please discuss them with the documentation owners beforehand, to make sure that the documentation will fit into the larger documentation structure.</p> <p>If the documentation that you write or update might affect multiple stakeholders, ping them for a review. If you don't get a timely reply, ask the documentation owners for a review or for permission to merge.</p> <p>Note</p> <p>To minimise the overhead of the contributing to the documentation and speed up \"time-to-published-docs\" we do not have a formal review process. We will start simple, and add more formality as needed.</p>"},{"location":"contributing/#guidelines","title":"Guidelines","text":""},{"location":"contributing/#links","title":"Links","text":""},{"location":"contributing/#external-links","title":"External links","text":"<p>Links to external sites use the <code>[]()</code> syntax:</p> external link syntaxresult <pre><code>[The Spack repository](https://github.com/spack/spack)\n</code></pre> <p>The Spack repository</p>"},{"location":"contributing/#internal-links","title":"Internal links","text":"<p>Note</p> <p>The CI/CD pipeline will fail if it detects broken links in your draft documentation. It is not completely foolproof - to ensure that your changes do not create broken links you should merge the most recent version of the <code>main</code> branch of the docs into your branch branch.</p> <p>Adding and maintaining links to internal pages and sections that don't break or conflict requires care. It is possible to refer to links in other files using relative links, for example <code>[the fast server](../servers.md#fast-server)</code>, however if the target file is moved, or the section title \"fast-server\" is changed, the link will break.</p> <p>Instead, we advocate adding unique references to sections.</p> adding a referencelinking to a reference <p>Add a reference above the item, in this case we want to link to the section with the title <code>## The Fast Server</code>:</p> <pre><code>[](){#ref-servers-fast}\n## Fast Server\n</code></pre> <p>Use the <code>[](){#}</code> syntax to define the reference/anchor.</p> <p>Note</p> <p>Always place the anchor above the item you are linking to.</p> <p>In any other file in the project, use the <code>[][]</code> syntax to refer to the link (note that this link type uses square braces, instead of the usual parenthesis):</p> <pre><code>[the fast server][ref-servers-fast]\n</code></pre> <p>The benefits of this approach are that the link won't break if</p> <ul> <li>either the file containing the link or the file that refers to the link move,</li> <li>or if the title of the target sections changes.</li> </ul>"},{"location":"contributing/#images","title":"Images","text":"<p>Images are stored in the <code>docs/images</code> directory.</p> <ul> <li>there are sub-directories in the <code>docs/images</code> path - create a new sub-directory for your images if appropriate</li> <li>choose image and path names that make sense - <code>screenshot.png</code> is not a great file name. Neither is <code>PX-202502025-imgx.png</code>.</li> </ul> <p>Tip</p> <p>When providing a screenshot, do you need to show the whole screen, or just part of one window?</p> <p>Cropping the image will decrease file size, and might also draw the readers attention to the most relevant information.</p> <p>Tip</p> <p>Do you need a screenshot, or can a text description also work?</p>"},{"location":"contributing/#text-formatting","title":"Text formatting","text":"<p>Turn off automatic line breaks in your text editor, and stick to one sentence per line in paragraphs of text.</p> <p>See the good and bad examples below for an example of of what happens when a change to a sentence forces a line rebalance:</p> goodbad <p>Before: <pre><code>There are many different versions of MPI that can be used for communication.\nThe final choice of which to use is up to you.\n</code></pre></p> <p>After: <pre><code>There are many different versions of the popular MPI communication library that can be used for communication.\nThe final choice of which to use is up to you.\n</code></pre></p> <p>The diff in this case affects only one line.</p> <p>Before: <pre><code>There are many different versions of MPI that\ncan be used for communication. The final choice\nof which to use is up to you.\n</code></pre></p> <p>After: <pre><code>There are many different versions of the popular\nMPI communication library that can be used for\ncommunication. The final choice of which to use\nis up to you.\n</code></pre></p> <p>The diff in this case affects the original 3 lines, and creates a new one.</p> <p>This method defines a canonical representation of text, i.e. there is one and only one way to write a paragraph of text, which plays much better with git.</p> <ul> <li>changes to the text are less likely to create merge conflicts</li> <li>changing one line of text will not modify the surrounding lines (see example above)</li> <li>git diffs and git history are easier to read.</li> </ul>"},{"location":"contributing/#frequently-asked-questions","title":"Frequently asked questions","text":"<p>The documentation does not have a FAQ section, because questions are best answered by the documentation, not in a separate section. Integrating information into the main documentation requires some care to identify where the information needs to go, and edit the documentation around it. Adding the information to a FAQ is easier, but the result is information about a topic distributed between the docs and FAQ questions, which ultimately makes the documentation harder to search.</p> <p>FAQ content, such as lists of most frequently encountered error messages, is still very useful in many contexts. If you want to add such content, create a section at the bottom of a topic page, for example this section on the SSH documentation page.</p>"},{"location":"contributing/#small-contributions","title":"Small contributions","text":"<p>Small changes that only modify the contents of a single file, for example to fix some typos or add some clarifying detail to an example, it is possible to quickly create a pull request directly in the browser.</p> <p>At the top of each page there is an \"edit\" icon , which will open the markdown source for the page in the GitHub text editor.</p> <p>Once your changes are ready, click on the \"Commit changes...\" button in the top right hand corner of the editor, and add at least a description commit message.</p> <p>Note</p> <p>You will need to keep the default option Create a new branch for this commit and start a pull request.</p> <ul> <li>if the change is small and you are CSCS staff, you can merge the PR immediately</li> <li>all other changes can be</li> </ul>"},{"location":"contributing/#style-guide","title":"Style guide","text":"<p>This section contains general guidelines for how to format and present documentation in this repository. They should be followed for most cases, but as a guideline it can be broken, with good reason.</p>"},{"location":"contributing/#headings-are-written-in-sentence-case","title":"Headings are written in sentence case","text":"<p>Use title case for headings, meaning all words are capitalized except for minor words.</p>"},{"location":"contributing/#avoid-nesting-headings-too-deep","title":"Avoid nesting headings too deep","text":"<p>Nesting headings up to three levels is generally ok.</p>"},{"location":"contributing/#lists","title":"Lists","text":"<p>Write lists as proper sentences. Separate the items simply with commas if each item is simple, or make each item a full sentence if the items are longer and contain multiple sentences.</p> <ol> <li>The first item can look like this,</li> <li>the second like this, and</li> <li>the third item like this.</li> </ol>"},{"location":"contributing/#using-admonitions","title":"Using admonitions","text":"<p>Aim to include examples, notes, warnings using admonitions whenever appropriate. They stand out better from the main text, and can be collapsed by default if needed.</p> <p>Example one</p> <p>This is an example. The title of the example uses sentence case.</p> Collapsed note <p>This note is collapsed, because it uses <code>???</code>.</p> <p>If an admonition is collapsed by default, it should have a title.</p>"},{"location":"contributing/#code-blocks","title":"Code blocks","text":"<p>Use code blocks when you want to display monospace text in a programming language, terminal output, configuration files etc. The documentation uses pygments for highlighting. See list of available lexers for the languages that you can use for code blocks.</p> <p>Use <code>console</code> for interactive sessions with prompt-output pairs:</p> MarkdownRendered <pre><code>```console title=\"Hello, world!\"\n$ echo \"Hello, world!\"\nHello, world!\n```\n</code></pre> Hello, world!<pre><code>$ echo \"Hello, world!\"\nHello, world!\n</code></pre> <p>Warning</p> <p><code>terminal</code> is not a valid lexer, but MkDocs or pygments will not warn about using it as a language. The text will be rendered without highlighting.</p> <p>Warning</p> <p>Use <code>$</code> as the prompt character, optionally preceded by text. <code>&gt;</code> as the prompt character will not be highlighted correctly.</p> <p>Note the use of <code>title=...</code>, which will give the code block a heading.</p> <p>Tip</p> <p>Include a title whenever possible to describe what the code block does or is.</p> <p>If you want to display commands without output that can easily be copied, use <code>bash</code> as the language:</p> MarkdownRendered <pre><code>```bash title=\"Hello, world!\"\necho \"Hello, world!\"\n```\n</code></pre> Hello, world!<pre><code>echo \"Hello, world!\"\n</code></pre>"},{"location":"guides/","title":"Index","text":""},{"location":"guides/#guides","title":"Guides","text":"<p>Documentation that provides best practices, practical tips, known problems and useful background information.</p> <p>The guides are grouped around top-level topics.</p>"},{"location":"guides/gb2025/","title":"Gordon Bell 2025","text":""},{"location":"guides/gb2025/#gordon-bell-and-hpl-runs-2025","title":"Gordon Bell and HPL runs 2025","text":"<p>For Gordon Bell and HPL runs in March-April 2025, CSCS has expanded Santis to 1333 nodes (12 cabinets).</p> <p>For the runs, CSCS has applied some updates and changes that aim to improve performance and scaling scale, particularly for NCCL. If you are already familiar with running on Daint, you might have to make some small changes to your current job scripts and parameters, which will be documented here.</p>"},{"location":"guides/gb2025/#santis","title":"Santis","text":""},{"location":"guides/gb2025/#connecting","title":"Connecting","text":"<p>Connecting to Santis via SSH is the same as for Daint and Clariden, see the ssh guide for more information.</p> <p>Add the following to your SSH configuration to enable you to directly connect to Santis using <code>ssh santis</code>. <pre><code>Host santis\n    HostName santis.alps.cscs.ch\n    ProxyJump ela\n# change cscsusername to your CSCS username\n    User cscsusername\n    IdentityFile ~/.ssh/cscs-key\n    IdentitiesOnly yes\n</code></pre></p>"},{"location":"guides/gb2025/#reservations","title":"Reservations","text":"<p>The <code>normal</code> partition is used with no reservation, which means that that jobs can be submittied without <code>--partition</code> and <code>--reservation</code> flags.</p> <p>Timeline:</p> <ol> <li>Friday 4th April:<ul> <li>HPE finish HPL runs at 10:30am</li> <li>CSCS performs testing on the reconfigured system for ~1 hour on the <code>GB_TESTING_2</code> reservation</li> <li>The reservation is removed and all GB teams have access to test and tune applications.</li> </ul> </li> <li>Monday 7th April:<ul> <li>at 4pm the runs will start for the first team</li> </ul> </li> </ol> <p>Note</p> <p>There will be no special reservation during the open testing and tuning between Friday and Monday.</p>"},{"location":"guides/gb2025/#storage","title":"Storage","text":"<p>Your data sets from Daint are available on Santis</p> <ul> <li>the same Home is shared between Daint, Clariden and Santis</li> <li>the same Scratch is mounted on both Santis and Daint</li> <li>Store/Project are also mounted.</li> </ul>"},{"location":"guides/gb2025/#low-noise-mode","title":"Low Noise Mode","text":"<p>Note</p> <p>Low noise mode has been relaxed, so the previous requirement that you set <code>OMP_PLACES</code> and <code>OMP_PROC_BIND</code> no longer applies. One core per module is still reserved for system processes.</p> <p>Santis uses low noise mode, which reserves one core per Grace-Hopper module (i.e. per 72 cores) for system processes. This mode is intended to reduce performance variability caused by system processes interfering with application threads and processes. This means that SLURM job scripts must be updated to account for the reserved cores.</p>"},{"location":"guides/gb2025/#slurm","title":"SLURM","text":"<p>Unable to allocate resources: Requested node configuration is not available</p> <p>If you try to use all 72 cores on each socket, SLURM will give a hard error, because only 71 are available:</p> <pre><code># try to run 4 ranks per node, with 72 cores each\n$ srun -n4 -N1 -c72 ./build/affinity.mpi\nsrun: error: Unable to allocate resources: Requested node configuration is not available\n</code></pre> <p>Explicitly set the number of cores per task using the <code>--cpus-per-task/-c</code> flag, e.g.: <pre><code>#SBATCH --cpus-per-task=64\n</code></pre> or <pre><code>srun -N1 -n4 -c71 ...\n</code></pre></p>"},{"location":"guides/gb2025/#nccl","title":"NCCL","text":"<p>Todo</p> <p>write a guide on which versions to use, environment variables to set, etc.</p>"},{"location":"guides/internet-access/","title":"Internet Access on Alps","text":""},{"location":"guides/internet-access/#internet-access-on-alps","title":"Internet Access on Alps","text":"<p>The Alps network is mostly configured with private IP addresses (<code>172.28.0.0/16</code>). Login nodes have public IP addresses which means that they can directly access the internet, while a proxy server provides internet access for compute nodes.</p> Compute node proxy configuration <p>Compute nodes are configured with the following environment variables to use the proxy server:</p> <pre><code>export https_proxy='http://proxy.cscs.ch:8080'\nexport http_proxy='http://proxy.cscs.ch:8080'\nexport no_proxy='.local, .cscs.ch, localhost, 148.187.0.0/16, 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16'\nexport HTTPS_PROXY='http://proxy.cscs.ch:8080'\nexport HTTP_PROXY='http://proxy.cscs.ch:8080'\nexport NO_PROXY='.local, .cscs.ch, localhost, 148.187.0.0/16, 10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16'\n</code></pre> <p>Public IPs are a shared resource</p> <p>Be aware that public IPs, whether on login nodes or through the proxy, are essentially a shared resource. Many services will rate limit or block usage based on the IP address if abused. An example is pulling container images from Docker Hub. Authenticating with Docker Hub makes their rate limit apply per user instead.</p>"},{"location":"guides/internet-access/#using-ssh-through-the-proxy-server","title":"Using SSH through the proxy server","text":"<p>While use of the proxy server is transparent for most use cases, others need additional configuration for compute nodes. An example is cloning git repositories from GitHub over SSH. Cloning over https works without additional configuration. To make SSH use the proxy server, add the following to your <code>~/.ssh/config</code> file:</p> ~/.ssh/config<pre><code>Match Host *,!148.187.0.0/16,!192.168.0.0/16,!172.16.0.0/12,!10.0.0.0/8exec \"hostname -I | grep -vqF 148.187.\"\n    ProxyCommand nc -X connect -x proxy.cscs.ch:8080 %h %p\n</code></pre> <p>This configuration takes into account that login and compute nodes require a different setup.</p> Error message when cloning without the proxy set up for SSH <p>When cloning a git repository without the correct SSH configuration, cloning will time out as follows: <pre><code>[daint][&lt;user&gt;@daint-ln001 ~]$ git clone git@github.com:open-mpi/ompi.git\nCloning into 'ompi'...\nssh: connect to host github.com port 22: Connection timed out\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists.\n</code></pre></p>"},{"location":"guides/internet-access/#accessing-the-public-ip-of-a-node","title":"Accessing the public IP of a node","text":"<p>When on a login node configured with a public IP address, you can retrieve the public IP address for example as follows:</p> <pre><code>[daint][&lt;user&gt;@daint-ln001 ~]$ curl api.ipify.org\n148.187.6.19\n</code></pre>"},{"location":"guides/storage/","title":"Storage","text":""},{"location":"guides/storage/#storage","title":"Storage","text":""},{"location":"guides/storage/#many-small-files-vs-hpc-file-systems","title":"Many small files vs. HPC File Systems","text":"<p>Workloads that read or create many small files are not well-suited to parallel file systems, which are designed for parallel and distributed I/O.</p> <p>Workloads that do not play nicely with Lustre include:</p> <ul> <li>Configuration and compiling applications.</li> <li>Using Python virtual environments</li> </ul> <p>At first it can seem strange that a \"high-performance\" file system is significantly slower than a laptop drive for a \"simple\" task like compilation or loading Python modules, however Lustre is designed for high-bandwidth parallel file access from many nodes at the same time, with the attendant trade offs this implies.</p> <p>Meta data lookups on Lustre are expensive compared to your laptop, where the local file system is able to aggressively cache meta data.</p>"},{"location":"guides/storage/#python-virtual-environments-with-uenv","title":"Python virtual environments with uenv","text":"<p>Python virtual environments can be very slow on Lustre, for example a simple <code>import numpy</code> command run on Lustre might take seconds, compared to milliseconds on your laptop.</p> <p>The main reasons for this include:</p> <ul> <li>Python virtual environments contain many small files, on which Python performs <code>stat()</code>, <code>open()</code> and <code>read()</code> commands when loading a module.</li> <li>Python pre-compiles <code>.pyc</code> files for each <code>.py</code> file in a project.</li> <li>All of these operations create a lot of meta-data lookups.</li> </ul> <p>As a result, using virtual environments can be slow, and these problems are only exacerbated when the virtual environment is loaded simultaneously by many ranks in an MPI job.</p> <p>One solution is to use the tool <code>mksquashfs</code> to compresses the contents of a directory - files, inodes and sub-directories - into a single file. This file can be mounted as a read-only Squashfs file system, which is much faster because a single file is accessed instead of the many small files that were in the original environment.</p>"},{"location":"guides/storage/#step-1-create-the-virtual-environment","title":"Step 1: create the virtual environment","text":"<p>The first step is to create the virtual environment using the usual workflow.</p> uvvenv <p>The recommended way to create a new virtual environment is to use the uv tool, which supports relocatable virtual environments and asynchronous package downloads. The main benefit of a relocatable virtual environment is that it does not need to be created in the final path from where it will be used. This allows the use of shared memory to speed up the creation and initialization of the virtual environment and, since the virtual environment can be used from any location, the resulting squashfs image can be safely shared across projects.</p> <pre><code># start the uenv\n# in this case the \"default\" view of prgenv-gnu provides python, cray-mpich,\n# and other useful tools\nuenv start prgenv-gnu/24.11:v1 --view=default\n\n# create and activate a new relocatable venv using uv\n# in this case we explicitly select python 3.12\nuv venv -p 3.12 --relocatable --link-mode=copy /dev/shm/sqfs-demo/.venv\ncd /dev/shm/sqfs-demo\nsource .venv/bin/activate\n\n# install software in the virtual environment using uv\n# in this case we install install pytorch\nuv pip install --link-mode=copy torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu126\n\n# optionally, to reduce the import times, precompile all\n# python modules to bytecode before creating the squashfs image\npython -m compileall -j 8 -o 0 -o 1 -o 2 .venv/lib/python3.12/site-packages\n</code></pre> <p>A new virtual environment can also be created using the standard <code>venv</code> module. However, virtual environments created by <code>venv</code> are not relocatable, and thus they need to be created and initialized in the path from where they will be used. This implies that the installation process can not be optimized for file system performance and will still be slow on Lustre filesystems.</p> <pre><code># start the uenv\n# in this case the \"default\" view of prgenv-gnu provides python, cray-mpich,\n# and other useful tools\nuenv start prgenv-gnu/24.11:v1 --view=default\n\n# for the example create a working path on SCRATCH\nmkdir $SCRATCH/sqfs-demo\ncd $SCRATCH/sqfs-demo\n\n# create and activate the empty venv\npython -m venv ./.venv\nsource ./.venv/bin/activate\n\n# install software in the virtual environment\n# in this case we install install pytorch\npip install torch torchvision torchaudio \\\n    --index-url https://download.pytorch.org/whl/cu126\n</code></pre> how many files did that create? <p>An inode is created for every file, directory and symlink on a file system. In order to optimise performance, we want to reduce the number of inodes (i.e. the number of files and directories).</p> <p>The following command can be used to count the number of inodes: <pre><code>find $SCRATCH/sqfs-demo/.venv -exec stat --format=\"%i\" {} + | sort -u | wc -l\n</code></pre> <code>find</code> is used to list every path and file, and <code>stat</code> is called on each of these to get the inode, and then <code>sort</code> and <code>wc</code> are used to count the number of unique inodes.</p> <p>In our \"simple\" pytorch example, I counted 22806 inodes!</p>"},{"location":"guides/storage/#step-2-make-a-squashfs-image-of-the-virtual-environment","title":"Step 2: make a squashfs image of the virtual environment","text":"<p>The next step is to create a single squashfs file that contains the whole virtual environment folder (i.e. <code>/dev/shm/sqfs-demo/.venv</code> or <code>$SCRATCH/sqfs-demo/.venv</code>).</p> <p>This is performed using the <code>mksquashfs</code> command, that is installed on all Alps clusters.</p> uvvenv <pre><code>mksquashfs /dev/shm/sqfs-demo/.venv py_venv.squashfs \\\n    -no-recovery -noappend -Xcompression-level 3\n</code></pre> <pre><code>mksquashfs $SCRATCH/sqfs-demo/.venv py_venv.squashfs \\\n    -no-recovery -noappend -Xcompression-level 3\n</code></pre> <p>Hint</p> <p>The <code>-Xcompression-level</code> flag sets the compression level to a value between 1 and 9, with 9 being the most compressed. We find that level 3 provides a good trade off between the size of the compressed image and performance: both uenv and the container engine use level 3.</p> I am seeing errors of the form <code>Unrecognised xattr prefix...</code> <p>You can safely ignore the (possibly many) warning messages of the form: <pre><code>Unrecognised xattr prefix lustre.lov\nUnrecognised xattr prefix system.posix_acl_access\nUnrecognised xattr prefix lustre.lov\nUnrecognised xattr prefix system.posix_acl_default\n</code></pre></p> <p>Tip</p> <p>The default installed version of <code>mksquashfs</code> on Alps does not support the best <code>zstd</code> compression method. Every uenv contains a better version of <code>mksquashfs</code>, which is used by the uenv to compress itself when it is built.</p> <p>The exact location inside the uenv depends on the target architecture, and version, and will be of the form: <pre><code>/user-environment/linux-sles15-${arch}/gcc-7.5.0/squashfs-${version}-${hash}/bin/mksquashfs\n</code></pre> Use this version for the best results, though it is also perfectly fine to use the system version.</p>"},{"location":"guides/storage/#step-3-use-the-squashfs","title":"Step 3: use the squashfs","text":"<p>To use the optimised virtual environment, mount the squashfs image at the location of the original virtual environment when starting the uenv.</p> uvvenv <pre><code>cd $SCRATCH/sqfs-demo\nuenv start --view=default \\\n    prgenv-gnu/24.11:v1,$PWD/py_venv.squashfs:$SCRATCH/sqfs-demo/.venv\nsource .venv/bin/activate\n</code></pre> <p>Remember that virtual environments created by <code>uv</code> are relocatable only if the <code>--relocatable</code> option flag is passed to the <code>uv venv</code> command as mentioned in step 1. In that case, the generated environment is relocatable and thus it is possible to mount it in multiple locations without problems.</p> <pre><code>cd $SCRATCH/sqfs-demo\nuenv start --view=default \\\n    prgenv-gnu/24.11:v1,$PWD/py_venv.squashfs:$SCRATCH/sqfs-demo/.venv\nsource .venv/bin/activate\n</code></pre> <p>Note that the original virtual environment is still installed in <code>$SCRATCH/sqfs-demo/.venv</code>, however the squashfs image has been mounted on top of it, so the single squashfs file is being accessed instead of the many files in the original version.</p> <p>A benefit of this approach is that the squashfs file can be copied to a location that is not subject to the Scratch cleaning policy.</p> <p>Warning</p> <p>Virtual environments created by <code>venv</code> are not relocatable as they contain symlinks to absolute locations inside the virtual environment. This means that the squashfs file must be mounted in the exact same location where the virtual environment was created.</p>"},{"location":"guides/storage/#step-4-optional-regenerate-the-virtual-environment","title":"Step 4: (optional) regenerate the virtual environment","text":"<p>The squashfs file is immutable - it is not possible to modify the contents of <code>.venv</code> while it is mounted. This means that it is not possible to <code>pip install</code> more packages in the virtual environment.</p> <p>If you need to modify the virtual environment, run the original uenv without the squashfs file mounted, make changes to the virtual environment, and run step 2 again to generate a new image.</p> <p>Hint</p> <p>If you save the updated copy in a different file, you can now \"roll back\" to the old version of the environment by mounting the old image.</p>"},{"location":"guides/terminal/","title":"Using the terminal","text":""},{"location":"guides/terminal/#terminal-usage-on-alps","title":"Terminal usage on Alps","text":"<p>This documentation is a collection of guides, hints, and tips for setting up your terminal environment on Alps.</p> <p></p>"},{"location":"guides/terminal/#shells","title":"Shells","text":"<p>Every user has a shell that will be used when they log in, with bash as the default shell for new users at CSCS.</p> <p>At CSCS the vast majority of users stick with the default <code>bash</code>: at the time of writing, of over 1000 users on Daint, over 99% were using bash.</p> <p>Which shell am I using?</p> <p>Run the following command after logging in:</p> <pre><code>$ getent passwd | grep $USER\nbcumming:*:22008:1000:Benjamin Cumming, CSCS:/users/bcumming:/usr/local/bin/bash\n</code></pre> <p>The last entry in the output points to the shell of the user, in this case <code>/usr/local/bin/bash</code>.</p> <p>Tip</p> <p>If you would like to change your shell, for example to zsh, you have to open a service desk ticket to request the change. You can't make the change yourself.</p> <p>Warning</p> <p>Because <code>bash</code> is used by all CSCS staff and the overwhelming majority of users, it is the best tested, and safest default.</p> <p>We strongly recommend against using cshell - tools like uenv are not tested against it.</p> <p></p>"},{"location":"guides/terminal/#managing-x86-and-arm","title":"Managing x86 and ARM","text":"<p>Alps has nodes with different CPU architectures, for example Santis has ARM (Grace <code>aarch64</code>) processors, and Eiger uses x86 (AMD Rome <code>x86_64</code>) processors. Binary applications are generally not portable, for example if you compile or install a tool compiled for <code>x86_64</code> on Eiger, you will get an error when you run it on an <code>aarch64</code> node.</p> cannot execute binary file: Exec format error <p>You will see this error message if you try to execute an executable built for a different architecture.</p> <p>In this case, the <code>rg</code> executable built for <code>aarch64</code> (Grace-Hopper nodes) is run on an <code>x86_64</code> node on Eiger: <pre><code>$ ~/.local/aarch64/bin/rg\n-bash: ./rg: cannot execute binary file: Exec format error\n</code></pre></p> <p>A common pattern for installing local software, for example some useful command line utilities like ripgrep, is to install them in <code>$HOME/.local/bin</code>. This approach won't work if the same home directory is mounted on two different clusters with different architectures: the version of ripgrep in our example would crash with <code>Exec format error</code> on one of the clusters.</p> <p>Care needs to be taken to store executables, configuration and data for different architecures in separate locations, and automatically configure the login environment to use the correct location when you log into different systems.</p> <p>The following example: * sets architecture-specific <code>bin</code> path for installing programs * sets architecture-specific paths for installing application data and configuration * selects the correct path by running <code>uname -m</code> when you log in to a cluster</p> .bashrc<pre><code># Set the \"base\" directory in which all architecture specific will be installed.\n# The $(uname -m) command will generate either x86_64 or aarch64 to match the\n# node type, when run during login.\nxdgbase=$HOME/.local/$(uname -m)\n\n# The XDG variables define where applications look for configurations\nexport XDG_DATA_HOME=$xdgbase/share\nexport XDG_CONFIG_HOME=$xdgbase/config\nexport XDG_STATE_HOME=$xdgbase/state\n\n# set PATH to look for in architecture specific path:\n# - on x86: $HOME/.local/x86_64/bin\n# - on ARM: $HOME/.local/aarch64/bin\nexport PATH=$xdgbase/bin:$PATH\n</code></pre> <p>XDG what?</p> <p>The XDG base directory specification is used by most applications to determine where to look for configurations, and where to store data and temporary files.</p>"},{"location":"platforms/cwp/","title":"Index","text":""},{"location":"platforms/cwp/#climate-and-weather-platform","title":"Climate and weather platform","text":"<p>The Climate and Weather Platform (CWP) provides compute, storage and support to the climate and weather modeling community in Switzerland.</p>"},{"location":"platforms/cwp/#getting-started","title":"Getting Started","text":""},{"location":"platforms/cwp/#getting-access","title":"Getting access","text":"<p>Project administrators (PIs and deputy PIs) of projects on the CWP can to invite users to join their project, before they can use the project's resources on Alps.</p> <p>Todo</p> <p>This points to the Waldur solution - whether the UMP or Waldur docs are linked depends on which is being used when these docs go live.</p> <p>This is performed using the project management tool</p> <p>Once invited to a project, you will receive an email, which you can need to create an account and configure multi-factor authentification (MFA).</p>"},{"location":"platforms/cwp/#systems","title":"Systems","text":"<p>Santis is the system deployed on the Alps infrastructure for the climate and weather platform. Its name derives from the highest mountain S\u00e4ntis in the Alpstein massif of North-Eastern Switzerland.</p> <ul> <li> <p> Santis</p> <p>Santis is a large Grace-Hopper cluster.</p> </li> </ul> <p></p>"},{"location":"platforms/cwp/#file-systems-and-storage","title":"File systems and storage","text":"<p>There are three main file systems mounted on the CWP system Santis.</p> type mount filesystem Home /users/$USER VAST Scratch <code>/capstor/scratch/cscs/$USER</code> Capstor Project <code>/capstor/store/cscs/userlab/&lt;project&gt;</code> Capstor"},{"location":"platforms/cwp/#home","title":"Home","text":"<p>Every user has a home path (<code>$HOME</code>) mounted at <code>/users/$USER</code> on the VAST filesystem. The home directory has 50 GB of capacity, and is intended for configuration, small software packages and scripts.</p>"},{"location":"platforms/cwp/#scratch","title":"Scratch","text":"<p>The Scratch filesystem provides temporary storage for high-performance I/O for executing jobs. Use scratch to store datasets that will be accessed by jobs, and for job output. Scratch is per user - each user gets separate scratch path and quota.</p> <p>Info</p> <p>A quota of 150 TB and 1 million inodes (files and folders) is applied to your scratch path.</p> <p>These are implemented as soft quotas: upon reaching either limit there is a grace period of 1 week before write access to <code>$SCRATCH</code> is blocked.</p> <p>You can check your quota at any time from Ela or one of the login nodes, using the <code>quota</code> command.</p> <p>Info</p> <p>The environment variable <code>SCRATCH=/capstor/scratch/cscs/$USER</code> is set automatically when you log into the system, and can be used as a shortcut to access scratch.</p> <p>scratch cleanup policy</p> <p>Files that have not been accessed in 30 days are automatically deleted.</p> <p>Scratch is not intended for permanant storage: transfer files back to the capstor project storage after job runs.</p>"},{"location":"platforms/cwp/#project","title":"Project","text":"<p>Project storage is backed up, with no cleaning policy: it provides intermediate storage space for datasets, shared code or configuration scripts that need to be accessed from different vClusters. Project is per project - each project gets a project folder with project-specific quota.</p> <ul> <li>hard limits on capacity and inodes prevent users from writing to project if the quota is reached - you can check quota and available space by running the <code>quota</code> command on a login node or ela.</li> <li>it is not recommended to write directly to the project path from jobs.</li> </ul>"},{"location":"platforms/hpcp/","title":"Index","text":""},{"location":"platforms/hpcp/#hpc-platform","title":"HPC Platform","text":"<p>Todo</p> <p>follow the template of the MLp</p>"},{"location":"platforms/mlp/","title":"Index","text":""},{"location":"platforms/mlp/#machine-learning-platform","title":"Machine learning platform","text":"<p>The Machine Learning Platform (MLP) provides compute, storage and expertise to the machine learning and AI community in Switzerland, with the main user being the Swiss AI Initiative.</p>"},{"location":"platforms/mlp/#getting-started","title":"Getting started","text":""},{"location":"platforms/mlp/#getting-access","title":"Getting access","text":"<p>Project administrators (PIs and deputy PIs) of projects on the MLP can to invite users to join their project, before they can use the project's resources on Alps. This is performed using the project management tool</p> <p>Once invited to a project, you will receive an email, which you can need to create an account and configure multi-factor authentication (MFA).</p>"},{"location":"platforms/mlp/#systems","title":"Systems","text":"<p>The main cluster provided by the MLP is Clariden, a large Grace-Hopper GPU system on Alps.</p> <ul> <li> <p> Clariden</p> <p>Clariden is the main Grace-Hopper cluster.</p> </li> </ul> <ul> <li> <p> Bristen</p> <p>Bristen is a smaller system with A100 GPU nodes for todo</p> </li> </ul> <p></p>"},{"location":"platforms/mlp/#file-systems-and-storage","title":"File Systems and Storage","text":"<p>There are three main file systems mounted on the MLP clusters Clariden and Bristen.</p> type mount filesystem Home /users/$USER VAST Scratch <code>/iopstor/scratch/cscs/$USER</code> Iopstor Project <code>/capstor/store/cscs/swissai/&lt;project&gt;</code> Capstor"},{"location":"platforms/mlp/#home","title":"Home","text":"<p>Every user has a home path (<code>$HOME</code>) mounted at <code>/users/$USER</code> on the VAST filesystem. The home directory has 50 GB of capacity, and is intended for configuration, small software packages and scripts.</p>"},{"location":"platforms/mlp/#scratch","title":"Scratch","text":"<p>Scratch filesystems provide temporary storage for high-performance I/O for executing jobs. Use scratch to store datasets that will be accessed by jobs, and for job output. Scratch is per user - each user gets separate scratch path and quota.</p> <ul> <li>The environment variable <code>SCRATCH=/iopstor/scratch/cscs/$USER</code> is set automatically when you log into the system, and can be used as a shortcut to access scratch.</li> </ul> <p>scratch cleanup policy</p> <p>Files that have not been accessed in 30 days are automatically deleted.</p> <p>Scratch is not intended for permanant storage: transfer files back to the capstor project storage after job runs.</p> <p>Note</p> <p>There is an additional scratch path mounted on Capstor at <code>/capstor/scratch/cscs/$USER</code>, however this is not reccomended for ML workloads for performance reasons.</p>"},{"location":"platforms/mlp/#project","title":"Project","text":"<p>Project storage is backed up, with no cleaning policy: it provides intermediate storage space for datasets, shared code or configuration scripts that need to be accessed from different vClusters. Project is per project - each project gets a project folder with project-specific quota.</p> <ul> <li>if you need additional storage, ask your PI to contact the CSCS service managers Fawzi or Nicholas.</li> <li>hard limits on capacity and inodes prevent users from writing to project if the quota is reached - you can check quota and available space by running the <code>quota</code> command on a login node or ela </li> <li>it is not recommended to write directly to the project path from jobs.</li> </ul>"},{"location":"platforms/mlp/#guides-and-tutorials","title":"Guides and tutorials","text":"<p>Todo</p> <p>links to tutorials and guides for ML workflows</p>"},{"location":"policies/","title":"CSCS User Policies","text":"<p>The CSCS code of conduct outlines the responsibilities and proper practices for the CSCS user community.</p> <p>The User Regulations define the basic guidelines for the usage of CSCS computing resources. The right to access CSCS resources may be revoked to whoever breaches any of the user regulations.</p>"},{"location":"policies/#computing-budget","title":"Computing Budget","text":"<p>Compute time on Alps systems is accounted in node hours; computing time on CSCS systems that allow node sharing will be accounted in core hours.</p> <p>Please note that resources at CSCS are assigned over three-months windows</p> <ul> <li>Quotas are reset on April 1st, July 1st, October 1st and January 1st</li> <li>Please make sure to use thoroughly your quarterly compute budget within the corresponding time frame</li> <li>Resources unused in the three-month periods are not transferred to the next allocation period but are forever lost</li> </ul>"},{"location":"policies/#data-retention-policies","title":"Data Retention Policies","text":"<p>Data belonging to active projects in the filesystems /users, /project, /store are under backup. There is no backup for data under the scratch filesystem, therefore no data recovery is possible in case of accidental loss or for data deleted due to the cleaning policy implemented on this filesystem.</p> <p>Please note that the long term storage service is granted as long as your project is active, and the data will be removed without further notice 3 months after the expiration of the project: please check the applicable filesystem policies for the grace period granted after the expiration of the project.</p> <p>Furthermore, as soon as your project expires, the backup of the data belonging to the project will be disabled immediately: therefore no data backup will be available after the final data removal.</p> <p></p>"},{"location":"policies/#fair-usage-of-shared-resources","title":"Fair Usage of Shared Resources","text":"<p>The Slurm scheduling system is a shared resource that can handle a limited number of batch jobs and interactive commands simultaneously. Therefore users should not submit hundreds of Slurm jobs and commands at the same time, as doing so would infringe our fair usage policy.</p> <p>Let us also remind you that running compute or memory intensive applications on the login nodes is forbidden. Please submit batch jobs with the Slurm scheduler, in order to allocate and run your processes on compute nodes: compute or memory intensive processes affecting the performance of login nodes will be terminated without warning.</p>"},{"location":"policies/code-of-conduct/","title":"CSCS Code of Conduct","text":"<p>The CSCS code of conduct aims to outline the responsibilities and the proper practices for CSCS user community.</p>"},{"location":"policies/code-of-conduct/#access-to-source-code","title":"Access to Source Code","text":"<p>If you are using your own code for production projects at CSCS, you agree to make this code available to CSCS application analysts for performance analysis and optimization purposes (if necessary). If you are using third-party commercial or community open-source codes, CSCS will contact the developer as needed.</p>"},{"location":"policies/code-of-conduct/#scientific-advisory-committee-sac","title":"Scientific Advisory Committee (SAC)","text":"<p>It is expressly stated that panel committee members must not be contacted under any circumstances on issues regarding proposals. Violation of this rule disqualifies the proposal from scientific review and leads to immediate rejection of proposals.</p>"},{"location":"policies/code-of-conduct/#acknowledgements","title":"Acknowledgements","text":"<p>The User Lab Users must quote and acknowledge the use of CSCS resources in all publications related to their production and development projects as follows:</p> <p>This work was supported by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID ### on Alps</p> <p>The User Lab Users must quote and acknowledge the use of Swiss Share of the LUMI resources in all publications related to their production and development projects as follows:</p> <p>This work was supported by a grant from the Swiss National Supercomputing Centre (CSCS) on the Swiss share of the LUMI system under project ID ###</p> <p>Users with allocations under the  Swiss AI Initiative must quote and acknowledge the use of CSCS resources in all publications related to their projects on Alps as follows:</p> <p>This work was supported as part of the \u201cSwiss AI initiative\u201d by a grant from the Swiss National Supercomputing Centre (CSCS) under project ID ### on Alps</p> <p>User Lab Users should acknowledge the PASC program in their publications as follows:</p> <p>This work was supported by the  Platform for Advanced Scientific Computing (PASC) project \"name of the project\".\"</p> <p>Users with a share of CSCS infrastructure (Contractual Partners) should acknowledge use of CSCS resources in their publications as follows:</p> <p>We acknowledge access to Piz Daint or Alps at the Swiss National Supercomputing Centre, Switzerland under the [institution]'s share with the project ID ####</p>"},{"location":"policies/regulations/","title":"User Regulations","text":"<p>The User Regulations define the basic guidelines for the usage of CSCS computing resources. The right to access CSCS resources may be revoked to whoever breaches any of the user regulations.</p> <p>These are the Terms &amp; Conditions, which users need to follow in order to access CSCS computing resources.</p> <ul> <li>Access to CSCS facilities is granted on an individual basis. An account is usable by the applicant only and only for the explicit purposes stated in the project application. CSCS does not allow sharing of accounts.</li> <li>The applicant is not permitted to give any other person (project member or otherwise), organization or representative of any organization access to CSCS facilities explicitly or implicitly, through negligence or carelessness. Revealing of passwords or identification protocols through verbal, written or electronic means is strictly prohibited. Any such activity is considered a breach of CSCS security, the contract between the applicant and CSCS at the moment the Account Application Form is submitted and approved, and the established contracts between CSCS and its computer vendors. Should such activity occur, the applicant will be immediately barred from all present and future use of CSCS facilities and is fully liable for all consequences arising from the infraction.</li> <li>Any indication of usage or requests for runs which give rise to serious suspicion will be further investigated and escalated to the appropriate authorities if necessary.</li> <li>Access to and use of data of other accounts on CSCS systems without prior consent from the principal investigator to which project the user account pertains is strictly prohibited. The terms and conditions for use of data from other accounts must be directly agreed to by the data owner.</li> <li>The applicant confirms that all information provided on his/her Account Application Form is true and accurate, and that she/he has not knowingly misrepresented him/herself.</li> <li>The principal investigator should promptly and proactively notify CSCS as soon as the applicants (i.e. the future account owner) should be suspended.</li> </ul> <p>All CSCS account holders are fully bound to obey by the ETH Zurich Acceptable Use Policy for Telematics Resources (\u201cBOT\u201d).</p>"},{"location":"policies/slack/","title":"Slack Code of Conduct","text":"<p>The CSCS Users Slack space is designed to foster a positive and inclusive environment for all members. To ensure a respectful and engaging experience, we kindly ask you to adhere to the following code of conduct:</p> <ol> <li>Respectful Communication<ul> <li>Treat all members with respect and kindness. Avoid offensive, derogatory, or discriminatory language and behaviour.</li> <li>Engage in constructive discussions and debates. Maintain a respectful tone.</li> </ul> </li> <li>Inclusive Atmosphere<ul> <li>Embrace diversity in all forms, including but not limited to, race, gender, sexual orientation, disability, and cultural background.</li> <li>Avoid making assumptions about others based on their background or identity.</li> </ul> </li> <li>Professionalism and Relevance<ul> <li>Keep discussions relevant to the scope of the channel and the scope of CSCS.</li> <li>Refrain from promoting unrelated content, products, or services.</li> </ul> </li> <li>Intellectual Property and Copyright<ul> <li>Respect intellectual property rights. Only share content that you have the right to share.</li> <li>Provide appropriate attribution when sharing information or resources.</li> </ul> </li> <li>Avoid Spam and Self-Promotion<ul> <li>Avoid excessive self-promotion or advertisements: please use the dedicated channel for job postings</li> <li>Share content that adds value to the community, such as relevant articles, resources, and insights.</li> </ul> </li> <li>Privacy and Data Protection<ul> <li>Respect the privacy of others. Do not share personal information without explicit consent.</li> <li>Do not share confidential or sensitive information in the channel.</li> </ul> </li> <li>Helpful and Supportive Environment<ul> <li>Offer help and support to fellow members when possible. Collaboration is key to our community.</li> <li>Ask questions and seek assistance respectfully. We\u2019re here to learn and grow together.</li> </ul> </li> <li>Reporting Violations<ul> <li>If you encounter behaviour that violates this code of conduct, please report it to Admins promptly (for information on how to browse a list of Admins, refer to the Slack help center) .</li> <li>Do not engage in public call-outs or confrontations. Let the Admins handle the situation.</li> </ul> </li> <li>Moderator Authority<ul> <li>Respect the decisions of Admins. They are responsible for maintaining the integrity of the space and ensuring a positive environment. Continuous Improvement This code of conduct is subject to updates and improvements. Your feedback is valuable in creating a better community.</li> </ul> </li> </ol> <p>By participating in the CSCS Users Slack, you agree to abide by this code of conduct. Remember, your contributions can have a positive impact on the community. Let\u2019s work together to create an environment that fosters learning, collaboration, and innovation. Thank you for being a part of the CSCS Users Slack community! \u001b</p>"},{"location":"policies/support/","title":"UserLab Support Policy","text":""},{"location":"policies/support/#1-user-support-policy","title":"1. User Support Policy","text":"<p>CSCS operates an advanced research infrastructure dedicated to High-Performance Computing (HPC) and other scientific applications. Our infrastructure encompasses a wide array of resources including compute, network, supporting software and tools, and several software applications used by a broad user base. Our user support policy outlines the level of assistance users can expect, the types of support offered, and the guidelines for requesting and receiving assistance.</p>"},{"location":"policies/support/#2-best-effort-support","title":"2. Best Effort Support","text":"<p>CSCS is committed to offering best effort support to our users. Our goal is to provide responsive and effective assistance, ensuring the hardware and software infrastructure operates at a high level to satisfy the majority of the scientific community\u2019s needs. However, while we will make a reasonable attempt to assist users with their inquiries, we cannot always guarantee a resolution.</p> <p>Our best effort support includes the following elements:</p> <ul> <li>Timely Responses: Users can expect an initial acknowledgment or response to their inquiry in a timely manner during regular working hours.</li> <li>Direct Assistance: Our support staff is available to provide guidance on technical issues, configuration challenges, troubleshooting, and to offer general advice to address an issue. </li> <li>Escalation Process: In cases where initial support efforts are insufficient to resolve an issue CSCS may, at its discretion, escalate an issue to additional staff or third-party vendors, contingent upon the availability of resources or capacity.</li> <li>Quality Documentation: CSCS provides comprehensive, accurate, and up-to-date documentation. This documentation is designed to help users understand and effectively utilize our infrastructure and services.</li> </ul> <p>CSCS reserves the right to decline support for requests that fall outside the scope of the activities described in the user\u2019s initial project allocation proposal. Support will be focused on ensuring that the resources are used in alignment with the approved objectives and goals. Requests that significantly deviate from the original proposal may not be accommodated.</p>"},{"location":"policies/support/#3-user-applications","title":"3. User Applications","text":"<p>User applications are those brought to CSCS systems by the users, whether they are developed by the users themselves or another third-party. Packages or applications not provided by CSCS are considered user applications. Users may need to compile or adapt these applications to our system. CSCS will provide guidance on deploying applications on our systems, including configuration and optimizations of the CSCS environment. While we can assist with infrastructure-related issues, we can not configure, optimize, debug, or fix the applications themselves. Users are responsible for resolving application-specific issues themselves or contacting the respective developers.</p>"},{"location":"policies/support/#4-officially-supported-applications","title":"4. Officially Supported Applications","text":"<p>CSCS offers a range of officially supported applications and their respective versions and configurations, which are packaged and released by CSCS or its supply partners. These packages benefit from our resources, expertise, and comprehensive documentation. They include mission-critical software chosen for their significant impact on the center\u2019s goals, strategic projects, and wide user base. Users can expect timely assistance, troubleshooting, optimization, and integration with CSCS infrastructure for these packages. This support also extends to common tools and libraries provided by CSCS for the development and deployment of scientific applications.</p> <p>While CSCS provides enhanced support for third-party software included in our officially supported applications, our ability to resolve issues is contingent on the extent of our expertise and control. Bugs or other problems that fall outside of our immediate control will be escalated to the relevant third-party vendors, but further resolution will depend on their response and capabilities, limiting our ability to fully address such issues.</p>"},{"location":"policies/support/#5-prioritisation-criteria","title":"5. Prioritisation Criteria","text":"<p>Support cases will be prioritised based on factors such as the impact on CSCS's overall mission and services, potential for knowledge transfer, degree of expertise required, and time and effort required to provide support. Issues directly concerning products and services offered by CSCS will be given higher priority.</p>"},{"location":"policies/support/#6-collaborative-support","title":"6. Collaborative Support","text":"<p>The effectiveness and efficiency of our support are greatly enhanced when users work collaboratively with us. By providing thorough information users enable us to deliver more effective and timely assistance. To facilitate effective support, users are expected to:</p> <ul> <li>Consult Documentation: Users are encouraged to review the provided documentation and indicate what they have consulted before seeking support.</li> <li>Provide Detailed Information: Users should offer, to the best of their ability, sufficient documentation and information about their software and the issues they are experiencing.     This includes detailing previous attempts to resolve the issue and any relevant error messages or logs. Clear and precise communication of the problem and steps already taken helps us diagnose and address issues more efficiently.</li> </ul>"},{"location":"policies/support/#7-closure-of-support-tickets","title":"7. Closure of Support Tickets","text":"<p>Support tickets related to user applications will be closed if, after providing all feasible guidance and troubleshooting within our support scope and capacity, it is determined that the issue lies beyond the control of CSCS, such as in the user\u2019s application code or third-party dependencies. In such cases, the ticket will be closed after the user has been informed of the situation and provided with any relevant recommendations or resources for further investigation. Users are welcome to reopen the ticket if new, actionable information becomes available.</p>"},{"location":"policies/support/#8-communication-channels","title":"8. Communication Channels","text":"<p>Users can request support through the CSCS Service Desk. Updates and communication with support staff will be provided through e-mail or via the Service Desk. Users are also encouraged to communicate with each other via our community channels. CSCS reserves the right to make other forms of communication also available.</p>"},{"location":"policies/support/#9-continuous-improvement","title":"9. Continuous Improvement","text":"<p>We are committed to continuously improving our support services. Feedback from users is welcomed and will be used to refine our support policies and procedures to better meet the needs of our community.</p> <p>By adhering to this user support policy, we aim to ensure a consistent and satisfactory support experience for all users at CSCS.</p>"},{"location":"running/slurm/","title":"slurm","text":""},{"location":"running/slurm/#slurm","title":"SLURM","text":"<p>CSCS uses the SLURM as its workload manager to efficiently schedule and manage jobs on Alps vClusters. SLURM is an open-source, highly scalable job scheduler that allocates computing resources, queues user jobs, and optimizes workload distribution across the cluster. It supports advanced scheduling policies, job dependencies, resource reservations, and accounting, making it well-suited for high-performance computing environments.</p>"},{"location":"running/slurm/#accounting","title":"Accounting","text":"<p>Todo</p> <p>document <code>--account</code>, <code>--constraint</code> and other generic flags.</p> <p></p>"},{"location":"running/slurm/#partitions","title":"Partitions","text":"<p>At CSCS, SLURM is configured to accommodate the diverse range of node types available in our HPC clusters. These nodes vary in architecture, including CPU-only nodes and nodes equipped with different types of GPUs. Because of this heterogeneity, SLURM must be tailored to ensure efficient resource allocation, job scheduling, and workload management specific to each node type.</p> <p>Each type of node has different resource constraints and capabilities, which SLURM takes into account when scheduling jobs. For example, CPU-only nodes may have configurations optimized for multi-threaded CPU workloads, while GPU nodes require additional parameters to allocate GPU resources efficiently. SLURM ensures that user jobs request and receive the appropriate resources while preventing conflicts or inefficient utilization.</p> <p></p>"},{"location":"running/slurm/#debug-partition","title":"Debug partition","text":"<p>The SLURM <code>debug</code> partition is useful for quick turnaround workflows. The partition has a short maximum time (timelimit can be seen with <code>sinfo -p debug</code>), and a low number of maximum nodes (the <code>MaxNodes</code> can be seen with <code>scontrol show partition=debug</code>).</p> <p></p>"},{"location":"running/slurm/#normal-partition","title":"Normal partition","text":"<p>This is the default partition, and will be used when you do not explicitly set a partition. This is the correct choice for standard jobs. The maximum time is usually set to 24 hours (<code>sinfo -p normal</code> for timelimit), and the maximum nodes can be as much as nodes are available.</p> <p>The following sections will provide detailed guidance on how to use SLURM to request and manage CPU cores, memory, and GPUs in jobs. These instructions will help users optimize their workload execution and ensure efficient use of CSCS computing resources.</p> <p></p>"},{"location":"running/slurm/#nvidia-gh200-gpu-nodes","title":"NVIDIA GH200 GPU Nodes","text":"<p>The GH200 nodes on Alps have four GPUs per node, and SLURM job submissions must be configured appropriately to best make use of the resources. Applications that can saturate the GPUs with a single process per GPU should generally prefer this mode. Configuring SLURM jobs to use a single GPU per rank is also the most straightforward setup. Some applications perform badly with a single rank per GPU, and require use of NVIDIA's Multi-Process Service (MPS) to oversubscribe GPUs with multiple ranks per GPU.</p> <p>The best SLURM configuration is application- and workload-specific, so it is worth testing which works best in your particular case. See Scientific Applications for information about recommended application-specific SLURM configurations.</p> <p>Warning</p> <p>The GH200 nodes have their GPUs configured in \"default\" compute mode. The \"default\" mode is used to avoid issues with certain containers. Unlike \"exclusive process\" mode, \"default\" mode allows multiple processes to submit work to a single GPU simultaneously. This also means that different ranks on the same node can inadvertently use the same GPU leading to suboptimal performance or unused GPUs, rather than job failures.</p> <p>Some applications benefit from using multiple ranks per GPU. However, MPS should be used in these cases.</p> <p>If you are unsure about which GPU is being used for a particular rank, print the <code>CUDA_VISIBLE_DEVICES</code> variable, along with e.g. <code>SLURM_LOCALID</code>, <code>SLURM_PROCID</code>, and <code>SLURM_NODEID</code> variables, in your job script. If the variable is unset or empty all GPUs are visible to the rank and the rank will in most cases only use the first GPU. </p> <p></p>"},{"location":"running/slurm/#one-rank-per-gpu","title":"One rank per GPU","text":"<p>Configuring SLURM to use one GH200 GPU per rank is easiest done using the <code>--ntasks-per-node=4</code> and <code>--gpus-per-task=1</code> SLURM flags. For advanced users, using <code>--gpus-per-task</code> is equivalent to setting <code>CUDA_VISIBLE_DEVICES</code> to <code>SLURM_LOCALID</code>, assuming the job is using four ranks per node. The examples below launch jobs on two nodes with four ranks per node using <code>sbatch</code> and <code>srun</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=gh200-single-rank-per-gpu\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --gpus-per-task=1\n\nsrun &lt;application&gt;\n</code></pre> <p>Omitting the <code>--gpus-per-task</code> results in <code>CUDA_VISIBLE_DEVICES</code> being unset, which will lead to most applications using the first GPU on all ranks.</p> <p></p>"},{"location":"running/slurm/#multiple-ranks-per-gpu","title":"Multiple ranks per GPU","text":"<p>Using multiple ranks per GPU can improve performance e.g. of applications that don't generate enough work for a GPU using a single rank, or ones that scale badly to all 72 cores of the Grace CPU. In these cases SLURM jobs must be configured to assign multiple ranks to a single GPU. This is best done using NVIDIA's Multi-Process Service (MPS). To use MPS, launch your application using the following wrapper script, which will start MPS on one rank per node and assign GPUs to ranks according to the CPU mask of a rank, ensuring the closest GPU is used:</p> <pre><code>#!/bin/bash\n# Example mps-wrapper.sh usage:\n# &gt; srun [srun args] mps-wrapper.sh [cmd] [cmd args]\n\n# Only this path is supported by MPS\nexport CUDA_MPS_PIPE_DIRECTORY=/tmp/nvidia-mps\nexport CUDA_MPS_LOG_DIRECTORY=/tmp/nvidia-log-$(id -un)\n\n# Launch MPS from a single rank per node\nif [[ $SLURM_LOCALID -eq 0 ]]; then\n    CUDA_VISIBLE_DEVICES=0,1,2,3 nvidia-cuda-mps-control -d\nfi\n\n# Set CUDA device\nnuma_nodes=$(hwloc-calc --physical --intersect NUMAnode $(hwloc-bind --get --taskset))\nexport CUDA_VISIBLE_DEVICES=$numa_nodes\n\n# Wait for MPS to start\nsleep 1\n\n# Run the command\nnumactl --membind=$numa_nodes \"$@\"\nresult=$?\n\n# Quit MPS control daemon before exiting\nif [[ $SLURM_LOCALID -eq 0 ]]; then\n    echo quit | nvidia-cuda-mps-control\nfi\n\nexit $result\n</code></pre> <p>Save the above script as <code>mps-wrapper.sh</code> and make it executable with <code>chmod +x mps-wrapper.sh</code>. If the <code>mps-wrapper.sh</code> script is in the current working directory, you can then launch jobs using MPS for example as follows:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=gh200-multiple-ranks-per-gpu\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=32\n#SBATCH --cpus-per-task=8\n\nsrun ./mps-wrapper.sh &lt;application&gt;\n</code></pre> <p>Note that in the example job above:</p> <ul> <li><code>--gpus-per-node</code> is not set at all; the <code>mps-wrapper.sh</code> script ensures that the right GPU is visible for each rank using <code>CUDA_VISIBLE_DEVICES</code></li> <li><code>--ntasks-per-node</code> is set to 32; this results in 8 ranks per GPU</li> <li><code>--cpus-per-task</code> is set to 8; this ensures that threads are not allowed to migrate across the whole GH200 node</li> </ul> <p>The configuration that is optimal for your application may be different.</p> <p></p>"},{"location":"running/slurm/#amd-cpu","title":"AMD CPU","text":"<p>Todo</p> <p>document how slurm is configured on AMD CPU nodes (e.g. eiger)</p>"},{"location":"services/","title":"Services","text":"<ul> <li> <p> CI/CD</p> <p>Configure CI/CD on Alps for your GitHub, GitLab and Bitbucket projects.</p> <p> CI/CD</p> </li> <li> <p> FirecREST</p> <p>FirecREST is a RESTful API for programmatically accessing High-Performance Computing resources.</p> <p> FirecREST</p> </li> </ul>"},{"location":"services/cicd/","title":"CI/CD","text":""},{"location":"services/cicd/#continuous-integration-continuous-deployment-cicd","title":"Continuous Integration / Continuous Deployment (CI/CD)","text":""},{"location":"services/cicd/#introduction-containerized-cicd","title":"Introduction containerized CI/CD","text":"<p>Containerized CI/CD allows you to build containers and run them at scale on CSCS systems. The basic idea is that you provide a Dockerfile with build instructions and run the newly created container. Most of the boilerplate work is being taken care by the CI implementation such that you can concentrate on providing build instructions and testing. The important information is provided to you from the CI side for the configuration of your repository.</p> <p>We support any git provider that supports webhooks. This includes GitHub, GitLab and Bitbucket. A typical pipeline consists of at least one build job and one test job. The build job makes sure that a new container with your most recent code changes is built. The test step uses the new container as part of an MPI job; e.g., it can run your tests on multiple nodes with GPU support.</p> <p>Building your software inside a container requires a Dockerfile and a name for the container in the registry where the container will be stored. Testing your software then requires the commands that must be executed to run the tests. No explicit container spawning is required (and also not possible). Your test jobs need to specify the number of nodes and tasks required for the test and the test commands.</p> <p>Here is an example of a full helloworld project.</p> <p>It is also helpful to consult the\u00a0GitLab CI yaml reference documentation and the predefined pipeline variables reference.</p> <p></p>"},{"location":"services/cicd/#tutorial-hello-world","title":"Tutorial Hello World","text":"<p>In this example we are using the\u00a0containerized hello world repository. This is a sample Hello World CMake project. The application only echos <code>Hello from $HOSTNAME</code>, but this should demonstrate the idea of how to run a program on multiple nodes. The pipeline instructions are inside the file <code>ci/cscs.yml</code>. Let's walk through the pipeline bit by bit. <pre><code>include:\n  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'\n</code></pre></p> <p>This block includes a yaml file which contains definitions with default values to build and run containers. Have a look inside this file to see available building blocks. <pre><code>stages:\n  - build\n  - test\n</code></pre> Here we define two different stages, named <code>build</code> and <code>test</code>. The names can be chosen freely. <pre><code>variables:\n  PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/helloworld:$CI_COMMIT_SHORT_SHA\n</code></pre></p> <p>This block defines variables that will apply to all jobs. See CI variables.</p> <pre><code>build_job:\n  stage: build\n  extends: .container-builder-cscs-zen2\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile.build\n</code></pre> <p>This adds a job named\u00a0<code>build_job</code> to the stage <code>build</code>. This runner expects a Dockerfile as input, which is specified in the variable\u00a0<code>DOCKERFILE</code>. The resulting container name is specified with the variable <code>PERSIST_IMAGE_NAME</code>, which has been defined already above, therefore it does not need to be explicitly mentioned in the <code>variables</code> block, again. There is further documentation of this runner at gitlab-runner-k8s-container-builder.</p> <p>Todo</p> <p>link to runner specs</p> <pre><code>test_job:\n  stage: test\n  extends: .container-runner-eiger-zen2\n  image: $PERSIST_IMAGE_NAME\n  script:\n    - /opt/helloworld/bin/hello\n  variables:\n    SLURM_JOB_NUM_NODES: 2\n    SLURM_NTASKS: 2\n</code></pre> <p>This block defines a test job. The job will be executed by the\u00a0container-runner-eiger-zen2.</p> <p>Todo</p> <p>link to runner</p> <p>This runner will pull the image on the cluster Eiger and run the commands as specified in the\u00a0<code>script</code> tag. In this example we are requesting 2 nodes with 1 task on each node, i.e. 2 tasks total. All Slurm environment variables are supported. The commands will be running inside the container specified by the <code>image</code> tag.</p> <p></p>"},{"location":"services/cicd/#ci-at-cscs","title":"CI at CSCS","text":""},{"location":"services/cicd/#enable-ci-for-your-project","title":"Enable CI for your project","text":"<p>While the procedure to enable CSCS CI for your repository consists of only a few steps outlined below, many of them require features in GitHub, GitLab or Bitbucket. The links in the text contain additional steps which may be needed. Some of those documents are non-trivial, especially if you do not have considerable background in the repository features. Plan sufficient time for the setup and contact a GitHub/GitLab/Bitbucket professional, if needed.</p> <ol> <li> <p>Register your project with CSCS: The first step to use containerized CI/CD is to register your Git repository with CSCS. Please open an Service Desk ticket for this step. Once your project has been registered you will be provided with a webhook-secret.</p> </li> <li> <p>Set up CI: Head to the CI overview page, login with your CSCS credentials, and go to the newly\u00a0registered project.</p> </li> <li> <p>Add FirecREST tokens: Expand the <code>Admin config</code>, and follow the guide (click on the small black triangle next to Firecrest Consumer Key). Enter all fields for FirecREST, i.e.,</p> <ul> <li>Consumer Key</li> <li>Consumer Secret</li> <li>default Slurm account for job submission (what you normally provide in the <code>--account</code>/<code>-A</code> flag to Slurm) If you don't already know how to obtain FirecREST credentials, you can find more information on How to create FirecREST clients on the Developer Portal</li> </ul> </li> </ol> <p>Todo</p> <p>replace link to mkdocs firecrest docs</p> <ol> <li> <p>(Optional) Private project: If your Git repository is a private repository make sure to check the <code>Private repository</code> box and follow the instructions to add an SSH key to your Git repository.</p> </li> <li> <p>Add notification token: On the setup page you will also find the field <code>Notification token</code>. The token is live tested, and you will see a green checkmark when the token is valid and can be used by the CI. It is mandatory to add a token so that your Git repository will be notified about the status of the build jobs. You cannot save anything as long as the notification token is invalid. (Click on the small triangle to get further instructions)</p> </li> <li> <p>Add webhook: On the setup page you will find the <code>Setup webhook details</code> button. If you click on it you will see all the entries which have to be added to a new webhook in your Git repository. Follow the link given there to your repository, and add the webhook with the given entries.</p> </li> <li> <p>Default trusted users and default CI-enabled branches: Provide the default list of trusted users and CI-enabled branches. The global configuration will apply to all pipelines that do not overwrite it explicitly.</p> </li> <li> <p>Pipeline default: Your first pipeline has the name <code>default</code>. Click on <code>Pipeline default</code> to see the pipeline setup details. The name can be chosen freely but it cannot contain whitespaces (a short descriptive name). Update the entry point, trusted users and CI-enabled branches.</p> </li> <li> <p>Submit your changes</p> </li> <li> <p>(Optional) Add other pipelines: Add other pipelines with a different entry point if you need more pipelines.</p> </li> <li> <p>Add entry point yaml files to Git repository: Commit the yaml entry point files to your repository. You should get notifications about the build status in your repository if everything is correct. See the\u00a0Hello World Tutorial for a simple yaml-file.</p> </li> </ol>"},{"location":"services/cicd/#clarifications-and-pitfalls-to-the-above-mentioned-steps","title":"Clarifications and pitfalls to the above-mentioned steps","text":"<p>Info</p> <p>This section exemplifies on GitHub, but similar settings are available on GitLab and Bitbucket</p> <p>The <code>notification token</code> setup step is crucial, because this is the number one entrypoint for receiving initial feedback on any errors. You will not be able to save any changes on the CI setup page, as long as the notification token is invalid. The token is checked live, whether it can be used to do notifications.</p> <p>Notification tokens on GitHub can be setup using <code>Classic token</code> or <code>Fine-grained token</code>. We discourage the use of\u00a0fine-grained tokens. Fine-grained tokens are unsupported, and come with many pitfalls. They can work, but must be enabled at the organization level by an admin, and must be created in the correct organization. You must choose the correct resource owner, i.e., the organization that the project belongs to. If the organization is not listed, then it has disabled fine-grained tokens at the organization level. It can only be enabled globally on an organization by an admin. As for the repository you can restrict it to only the repository that you want to notify with this token or all repositories. Even if you choose \"All repositories\", it is still restricted to the organization, and does not grant the access to any repository outside of the resource owner.</p> <p>Another crucial setup step is the correct webhook setup. The repository provider (GitHub, GitLab, Bitbucket) gives you the ability to see what happened, when the webhook event was sent. If the webhook was not setup correctly, you will receive an HTTP error for the webhook events. The error message can be found in the webhook event response. As an example, here is how you would find it on GitHub: Settings &gt; Webhooks &gt; <code>Edit</code> button of the webhook &gt; <code>Recent Deliveries</code> tab &gt; Choose a webhook event from the list &gt; <code>Response</code> tab &gt; Check for potential error message.</p> <p>A typical error is accepting to defaults of GitHub for new webhooks, where only <code>Push</code> events are being sent. When you forget to select <code>Send me everything</code>, then some events will not trigger pipelines. Double check your webhook settings.</p> <p></p>"},{"location":"services/cicd/#understanding-when-ci-is-triggered","title":"Understanding when CI is triggered","text":""},{"location":"services/cicd/#push-events","title":"Push events","text":"<ul> <li>Every pipeline can define its own list of CI-enabled branches</li> <li>If a pipeline does not define a list of CI-enabled branches, the global list will be used</li> <li>If you push changes to a branch every pipeline that has this branch in its list of CI-enabled branches will be triggered</li> <li>If the global list and all pipelines have an empty list of CI-enabled branches, then CI will never be triggered on push events</li> </ul>"},{"location":"services/cicd/#pull-requests-merge-requests","title":"Pull requests (Merge requests)","text":"<ul> <li>For simplicity we use PR to mean Pull Request, although some providers call it a Merge request. It is the same thing.</li> <li>Every pipeline can define its own list of trusted users.</li> <li>If a pipeline does not define a list of trusted users, the global list will be used.</li> <li>If a PR is opened/edited and targets a CI-enabled branch, and the source branch is not from a fork, then all pipelines will be started that have the target branch in its list of CI-enabled branches.</li> <li>If a PR is opened/edited and targets a CI-enabled branch, but the source branch is from a fork, then a pipeline will be automatically started if and only if the fork is from a user in the pipeline's trusted user list and the target branch is in the pipeline's CI-enabled branches.</li> </ul>"},{"location":"services/cicd/#cscs-ci-run-comment","title":"<code>cscs-ci run</code> comment","text":"<ul> <li>You have an open PR</li> <li>You want to trigger a specific pipeline</li> <li>Write a comment inside the PR with the text   <pre><code>cscs-ci run PIPELINE_NAME_1,PIPELINE_NAME_2\n</code></pre></li> <li>Special case: You have only one pipeline, then you can skip the pipeline names and write only the comment <code>cscs-ci run</code></li> <li>The pipeline will only be triggered, if the commenting user is in the pipeline's trusted users list.</li> <li>Only the first line of the comment will be evaluated, i.e. you can add context from line 2 onwards.</li> <li>The target branch is ignored, i.e. you can test a pipeline even if the target branch is not in the pipeline's CI-enabled branches.</li> <li>Advanced <code>cscs-ci</code> run command is possible to inject variables into the pipeline (exposed as environment variables)<ul> <li>Triggering a pipeline with additional variables   <pre><code>cscs-ci run PIPELINE_NAME;MY_VARIABLE=some_value;ANOTHER_VAR=other_value\n</code></pre>   This will trigger the pipeline PIPELINE_NAME, and in your jobs there will be the environment variables MY_VARIABLE and ANOTHER_VAR available.</li> <li>Disallowed characters for PIPELINE_NAME, variable name and variable value are the characters <code>,;=</code> (comma, semicolon, equal), because they serve as separators of the different components.</li> </ul> </li> </ul>"},{"location":"services/cicd/#api-call-triggering","title":"API call triggering","text":"<ul> <li>It is possible to trigger a pipeline via an API call</li> <li>Create a file named <code>data.yaml</code>, with the content <pre><code>ref: main\npipeline: pipeline_name\nvariables:\n  MY_VARIABLE: some_value\n  ANOTHER_VAR: other_value\n</code></pre> Send a POST request to the middleware <pre><code>curl -X POST -u 'repository_id:webhook_secret' --data-binary @data.yaml https://cicd-ext-mw.cscs.ch/ci/pipeline/trigger\n</code></pre></li> <li>replace repository_id and webhook_secret with your repository id and the webhook secret.</li> </ul>"},{"location":"services/cicd/#understanding-the-underlying-workflow","title":"Understanding the underlying workflow","text":"<p>Typical users do not need to know the underlying workflow behind the scenes, so you can stop reading here. However, it might put the above-mentioned steps into perspective. It also can give you background for inquiring if and when something in the procedure does not go as expected.</p>"},{"location":"services/cicd/#workflow-exemplified-on-icon-exclaim","title":"Workflow (exemplified on icon-exclaim)","text":"<ol> <li>(Prerequisite) icon-exclaim will have a webhook set up</li> <li>You make some change in the icon-exclaim repository</li> <li>GitHub sends a webhook event to\u00a0<code>cicd-ext-mw.cscs.ch</code>\u00a0 (CI middleware)</li> <li>CI middleware fetches your repository from GitHub and pushes a \"mirror\" to GitLab</li> <li>GitLab sees a change in the repository and starts a pipeline (i.e. it uses the CI yaml as entry point)</li> <li>If the repository uses git submodules, <code>GIT_SUBMODULE_STRATEGY: recursive</code> has to be specified (see\u00a0GitLab documentation)</li> <li>The <code>container-builder</code>, which has as input a Dockerfile (specified in the variable <code>DOCKERFILE</code>), will take this Dockerfile and execute something similar to <code>docker build -f $DOCKERFILE .</code>, where the build context is the whole (recursively) cloned repository</li> </ol>"},{"location":"services/cicd/#ci-variables","title":"CI variables","text":"<p>Many variables exist during a pipeline run, they are documented at Gitlab's predefined variables. Additionally to CI variables available through Gitlab, there are a few CSCS specific pipeline variables:</p> Variable Value Additional information <code>CSCS_REGISTRY</code> jfrog.svc.cscs.ch CSCS internal registry, preferred registry to store your container images <code>CSCS_REGISTRY_PATH</code> jfrog.svc.cscs.ch/docker-ci-ext/ The prefix path in the CSCS internal container image registry, to which your pipeline has write access. Within this prefix, you can choose any directory structure. Images that are pushed to a path matching /public/ , can be pulled by anybody within CSCS network <code>CSCS_CI_MW_URL</code> https://cicd-ext-mw.cscs.ch/ci The URL of the middleware, the orchestrator software. <code>CSCS_CI_DEFAULT_SLURM_ACCOUNT</code> d123 The project to which accounting will go to. It is set up on the CI setup page in the Admin section. It can be overwritten via\u00a0SLURM_ACCOUNT for individual jobs. <code>CSCS_CI_ORIG_CLONE_URL</code> https://github.com/my-org/my-project (public) git@github.com:my-or/my-project (private) Clone URL for git. This is needed for some implementation details of the gitlab-runner custom executor. This is the clone URL of the registered project, i.e. this is not the clone URL of the mirror project."},{"location":"services/cicd/#containerized-ci-best-practices","title":"Containerized CI - best practices","text":""},{"location":"services/cicd/#multi-architecture-images","title":"Multi-architecture images","text":"<p>With the introduction of Grace-Hopper nodes, we have now <code>aarch64</code> and <code>x86_64</code> machines. This implies that the container images should be built for the correct architecture. This can be achieved by the following example <pre><code>include:\n  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'\n\nstages:\n  - build\n  - make_multiarch\n  - run\n\n.build:\n  stage: build\n  variables:\n    DOCKERFILE: path/to/my_dockerfile\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/${ARCH}/my_image_name:${CI_COMMIT_SHORT_SHA}\nbuild aarch64:\n  extends: [.container-builder-cscs-gh200, .build]\nbuild x86_64:\n  extends: [.container-builder-cscs-zen2, .build]\n\nmake multiarch:\n  extends: .make-multiarch-image\n  stage: make_multiarch\n  variables:\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/my_multiarch_image:${CI_COMMIT_SHORT_SHA}\n    PERSIST_IMAGE_NAME_AARCH64: $CSCS_REGISTRY_PATH/aarch64/my_image_name:${CI_COMMIT_SHORT_SHA}\n    PERSIST_IMAGE_NAME_X86_64: $CSCS_REGISTRY_PATH/x86_64/my_image_name:${CI_COMMIT_SHORT_SHA}\n\n.run:\n  stage: run\n  image: $CSCS_REGISTRY_PATH/my_multiarch_image:${CI_COMMIT_SHORT_SHA}\n  script:\n    - uname -a\nrun aarch64:\n  extends: [.container-runner-daint-gh200, .run]\nrun x86_64:\n  extends: [.container-runner-eiger-mc, .run]\n</code></pre></p> <p>We first create two container images which have different names. Then we combine these two names to a single name, with both architectures. Finally in the run step we use the multi-architecture image, where the container runtime will pull the correct architecture.</p> <p>It is not mandatory to combine the container images to a multi-architecture image, i.e. a CI setup which consistently uses the correct architecture specific paths can work. A multi-architecture image is convenient when you plan to distribute it to other users.</p>"},{"location":"services/cicd/#dependency-management","title":"Dependency management","text":""},{"location":"services/cicd/#problem","title":"Problem","text":"<p>A common observation is that your software has many dependencies that are more or less static, i.e. they can change but do so very rarely. A common pattern one can observe to work around rebuilding base images unnecessarily is a multi-stage CI setup</p> <ol> <li>Build (rarely but manually) a base container with all static dependencies and push it to a public container registry</li> <li>Use the base container and build the software container</li> <li>Test the newly created software container</li> <li>Deploy the software container</li> </ol> <p>This works fine but has the drawback that one has to do a manual step whenever the dependencies change, e.g. when one wants to upgrade to new versions of the dependencies. Another drawback of this is that it allows to keep the recipe of the base container outside of the repository, which makes it harder to reproduce results, especially when colleagues want to reproduce a build.</p>"},{"location":"services/cicd/#solution","title":"Solution","text":"<p>A common solution to this problem is that you have a multi stage setup. Your repository should have (at least) two Dockerfiles, let us call them <code>Dockerfile.base</code> and <code>Dockerfile</code>.</p> <ul> <li><code>Dockerfile.base</code>: This dockerfile contains the recipe to build your base-container, it normally derives <code>FROM</code> a very basic container, e.g. <code>docker.io/ubuntu:24.04</code> or CSCS spack base containers. Let us call the container image that is built using this recipe <code>BASE_IMG</code>.</li> </ul> <p>Todo</p> <p>link to spack base containers</p> <ul> <li><code>Dockerfile</code>: This Dockerfile contains the recipe to build your software-container. It must start with <code>FROM $BASE_IMG</code>.</li> </ul> <p>The <code>.container-builder-cscs-*</code> blocks can be used to solve this problem. The runner supports the variable <code>CSCS_REBUILD_POLICY</code>, which by default is set to <code>if-not-exists</code>.</p> <p>This means that the runner will check the remote registry if the container image specified in <code>PERSIST_IMAGE_NAME</code> exists. A new container image is built only if it does not exist yet. Note: In case you have one build job, <code>PERSIST_IMAGE_NAME</code> can be specified in the <code>variables:</code> field of this build job or as a global variable, like in the Hello World example. In case you have multiple build jobs and you specify the <code>PERSIST_IMAGE_NAME</code> variable per build job, you need to specify the exact name of the image to be used in the <code>image</code> field of the test job.</p> <p>A CI YAML file would look in the simplest case like this:</p> <p><code>ci/cscs.yml</code> <pre><code>include:\n  - remote: 'https://gitlab.com/cscs-ci/recipes/-/raw/master/templates/v2/.ci-ext.yml'\n\nstages:\n  - build_base\n  - build\n  - test\n\nbuild base:\n  extends: .container-builder-cscs-zen2\n  stage: build_base\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile.base\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/base/my_base_container:1.0\n    CSCS_REBUILD_POLICY: if-not-exists # default anyway, only here for verbosity\n\nbuild software:\n  extends: .container-builder-cscs-zen2\n  stage: build\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/software/my_software:$CI_COMMIT_SHORT_SHA\n    DOCKER_BUILD_ARGS: '[\"BASE_IMG=$CSCS_REGISTRY_PATH/base/my_base_container:1.0\"]'\n\ntest software single node:\n  extends: .container-runner-daint-gpu\n  image: $CSCS_REGISTRY_PATH/software/my_software:$CI_COMMIT_SHORT_SHA\n  script:\n    - ./test_suite_1.sh\n    - ./test_suite_2.sh\n  variables:\n    SLURM_JOB_NUM_NODES: 1\n\ntest software multi:\n  extends: .container-runner-daint-gpu\n  image: $CSCS_REGISTRY_PATH/software/my_software:$CI_COMMIT_SHORT_SHA\n  script:\n    - ./test_suite_1.sh\n    - ./test_suite_2.sh\n  variables:\n    SLURM_JOB_NUM_NODES: 4\n</code></pre></p> <p><code>ci/docker/Dockerfile.base</code> <pre><code>FROM docker.io/finkandreas/spack:0.19.2-cuda11.7.1-ubuntu22.04\n\nARG NUM_PROCS\n\nRUN spack-install-helper daint-gpu \\\n    petsc \\\n    trilinos\n</code></pre></p> <p><code>ci/docker/Dockerfile</code> <pre><code>ARG BASE_IMG\nFROM $BASE_IMG\n\nARG NUM_PROCS\n\nRUN mkdir /build &amp;&amp; cd /build &amp;&amp; cmake /sourcecode &amp;&amp; make -j$NUM_PROCS\n</code></pre></p> <p>A setup like this would run the very first time and build the container image <code>$CSCS_REGISTRY_PATH/base/my_base_container:1.0</code>, followed by the job that builds the container image <code>$CSCS_REGISTRY_PATH/software/my_software:1.0</code>. The next time CI is triggered the <code>.container-builder-cscs-zen2</code> would check the remote repository if the target tag (<code>PERSIST_IMAGE_NAME</code>) exists, and only build a new container image if it does not exist yet. Since the tag for the job <code>build base</code> is static, i.e. it is the same for every run of CI, it would build the first time it is running, but not for subsequent runs. In contrast to this is the job <code>build software</code>: Here the tag changes with every CI run, since the variable <code>CI_COMMIT_SHORT_SHA</code> is different for every run.</p>"},{"location":"services/cicd/#manual-dependency-update","title":"Manual dependency update","text":"<p>At some point you realise that you have to update some of the dependencies. You can use a manual update process to update your base-container, where you ensure that you update all necessary image tags. In our example, this means updating in <code>ci/cscs.yml</code> all occurences of <code>$CSCS_REGISTRY_PATH/base/my_base_container:1.0</code> to <code>$CSCS_REGISTRY_PATH/base/my_base_container:2.0</code> (or any other versioning scheme - for all that matters is that the full name must change). Of course something in <code>Dockerfile.base</code> should change too, otherwise you are building the same artifact, with just a different name.</p>"},{"location":"services/cicd/#dynamic-dependency-update","title":"Dynamic dependency update","text":"<p>While manually updating image tags works fine, it has the drawback that it is error-prone. Take for example the situation where you update the tag in <code>build base</code>, but forget to change it in <code>build software</code>. Your pipeline would still run fine, because the dependency of <code>build software</code> exists. Since there is no explicit error for the inconsistencies it is hard to find the error.</p> <p>Therefore, there is also the possibility to have a dynamic way of naming your container images. The idea is the same, i.e. we build first a base-container, and use this base-container to build our software-container.</p> <p>The <code>build base</code> and <code>build software</code> jobs would look similar to this: <pre><code>build base:\n  extends: .container-builder-cscs-zen2\n  stage: build_base\n  before_script:\n    - DOCKER_TAG=`cat ci/docker/Dockerfile.base | sha256sum - | head -c 16`\n    - export PERSIST_IMAGE_NAME=$CSCS_REGISTRY_IMAGE/base/my_base_image:$DOCKER_TAG\n    - echo \"BASE_IMAGE=$PERSIST_IMAGE_NAME\" &gt; build.env\n  artifacts:\n    reports:\n      dotenv: build.env\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile.base # overwrite with the real path of the Dockerfile\n\nbuild software:\n  extends: .container-builder-cscs-zen2\n  stage: build\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/software/my_software:$CI_COMMIT_SHORT_SHA\n    DOCKER_BUILD_ARGS: '[\"BASE_IMG=$BASE_IMAGE\"]'\n</code></pre></p> <p>Let us walk through the changes in the <code>build base</code> job:</p> <ul> <li><code>DOCKER_TAG</code> is computed at runtime by the sha256sum of the <code>Dockerfile.base</code>, i.e. it would change, when you change the content of <code>Dockerfile.base</code> (we keep only the first 16 characters, this is random enough to guarantee that we have a unique name).</li> <li>We export <code>PERSIST_IMAGE_NAME</code> to the dynamic name with <code>DOCKER_TAG</code>.</li> <li>We write the dynamic name to the file <code>build.env</code></li> <li>We tell the CI system to keep the <code>build.env</code> as an artifact (see here the documentation of this)</li> </ul> <p>Note: The dotenv artifacts of a specific job for public projects is available at <code>https://gitlab.com/cscs-ci/ci-testing/webhook-ci/mirrors/&lt;project_id&gt;/&lt;pipeline_id&gt;/-/jobs/&lt;job_id&gt;/artifacts/download?file_type=dotenv</code>.</p> <p>Now let us look at the changes in the <code>build software</code> job:</p> <ul> <li><code>DOCKER_BUILD_ARGS</code> is now using <code>$BASE_IMAGE</code>. This variable exists, because we transferred the information via a <code>dotenv</code> artifact from <code>build base</code> to this job.</li> </ul> <p>In this example the names <code>BASE_IMG</code> and <code>BASE_IMAGE</code> are chosen to be different, for clarification where the different variables are set and used. Feel free to use the same names for consistent naming. The default behaviour is to import all artifacts from all previous jobs. If you want only specific artifacts in your job, you should have a look at dependencies.</p> <p>There is also a building block in the templates, name <code>.dynamic-image-name</code>, which you can use to get rid for most of the boilerplate. It is important to note that this building block will export the dynamic name under the hardcoded name <code>BASE_IMAGE</code> in the <code>dotenv</code> file. The jobs would look something like this: <pre><code>build base:\n  extends: [.container-builder-cscs-zen2, .dynamic-image-name]\n  stage: build_base\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile.base\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/base/my_base_image\n    WATCH_FILECHANGES: 'ci/docker/Dockerfile.base'\n\nbuild software:\n  extends: .container-builder-cscs-zen2\n  stage: build\n  variables:\n    DOCKERFILE: ci/docker/Dockerfile\n    PERSIST_IMAGE_NAME: $CSCS_REGISTRY_PATH/software/my_software:$CI_COMMIT_SHORT_SHA\n    DOCKER_BUILD_ARGS: '[\"BASE_IMG=$BASE_IMAGE\"]'\n</code></pre></p> <p><code>build base</code> is using additionally the building block <code>.dynamic-image-name</code>, while <code>build software</code> is unchanged. Have a look at the definition of the block <code>.dynamic-image-name</code> in the file .ci-ext.yml for further notes.</p>"},{"location":"services/cicd/#examples","title":"Examples","text":"<p>See for working examples these two yaml files (and check the respective Dockerfiles mentioned in the build jobs)</p> <ul> <li>dcomex-framework</li> <li>utopia</li> </ul>"},{"location":"services/cicd/#example-projects","title":"Example projects","text":"<p>Here are a couple of projects which use this CI setup. Please have a look there for more advanced usage:</p> <ul> <li>dcomex-framework: entry point is <code>ci/prototype.yml</code></li> <li>utopia: two pipelines, with entry points <code>ci/cscs/mc/gitlab-daint.yml</code> and <code>ci/cscs/gpu/gitlab-daint.yml</code></li> <li>mars: two pipelines, with entry points <code>ci/gitlab/cscs/gpu/gitlab-daint.yml</code> and <code>ci/gitlab/cscs/mc/gitlab-daint.yml</code></li> <li>sparse_accumulation: entry point is <code>ci/pipeline.yml</code></li> <li>gt4py: entry point is <code>ci/cscs-ci.yml</code></li> <li>SIRIUS: entry point is <code>ci/cscs-daint.yml</code></li> <li>sphericart: entry point is <code>ci/pipeline.yml</code></li> </ul>"},{"location":"services/firecrest/","title":"FirecREST","text":""},{"location":"services/firecrest/#firecrest","title":"FirecREST","text":"<p>FirecREST is a RESTful API for programmatically accessing High-Performance Computing resources, developed at CSCS.</p> <p>Users can make use of FirecREST to automate access to HPC, enabling CI/CD pipelines, workflow managers, and other tools against HPC resources.</p> <p>Additionally, scientific platform developers can integrate FirecREST into web-enabled portals and applications, allowing them to securely access authenticated and authorized CSCS services such as job submission and data transfer on HPC systems.</p> <p>Users can make HTTP requests to perform the following operations:</p> <ul> <li>basic system utilities like <code>ls</code>, <code>mkdir</code>, <code>mv</code>, <code>chmod</code>, <code>chown</code>, among others</li> <li>actions against the Slurm workload manager (submit, query, and cancel jobs of the user)</li> <li>internal (between CSCS systems) and external (to/from CSCS systems) data transfers</li> </ul>"},{"location":"services/firecrest/#firecrest-versions","title":"FirecREST versions","text":"<p>Starting early 2025, CSCS has introduced a new version of the API: FirecREST version 2.</p> <p>Version 2 is faster, easier to use, and more efficient in resource management than its predecessor, therefore, if you are new to FirecREST start directly using version 2 (if available for your platform).</p> <p>If you're using version 1, we recommend you to port your applications to use the new version.</p> Version 2Version 1 <p>Warning</p> <p>Documentation for version 2 is still work in progress</p> <p>For a full feature set, have a look at the latest FirecREST version 2 API specification deployed at CSCS.</p> <p>Please refer to the FirecREST-v2 documentation for detailed documentation.</p> <p>For a full feature set, have a look at the latest FirecREST version 1 API specification deployed at CSCS.</p> <p>Please refer to the FirecREST-v1 documentation for detailed documentation.</p>"},{"location":"services/firecrest/#firecrest-deployment-on-alps","title":"FirecREST Deployment on Alps","text":"<p>FirecREST is available for all three major Alps platforms, with a different API endpoint and versions for each platform.</p> PlatformVersionAPI EndpointClusters HPC Platformv1https://api.cscs.ch/hpc/firecrest/v1Daint, Eiger v2https://api.cscs.ch/hpc/firecrest/v2 ML Platformv1https://api.cscs.ch/ml/firecrest/v1Bristen, Clariden CW Platformv1https://api.cscs.ch/cw/firecrest/v1Santis v2https://api.cscs.ch/cw/firecrest/v2"},{"location":"services/firecrest/#accessing-firecrest","title":"Accessing FirecREST","text":""},{"location":"services/firecrest/#clients-and-access-tokens","title":"Clients and access tokens","text":"<p>For authenticating requests to FirecREST, client applications use an access token instead of directly using the user's credentials. The access token is a signed JSON Web Token (JWT) which contains user information and is only valid for a short time (5 minutes). Behind the API, all commands launched by the client will use the account of the user that registered the client, inheriting their access rights.</p> <p>Every client has a client ID (Consumer Key) and a secret (Consumer Secret) that are used to get a short-lived access token with an HTTP request.</p> <code>curl</code> call to fetch the access token <pre><code>curl -s -X POST https://auth.cscs.ch/auth/realms/firecrest-clients/protocol/openid-connect/token \\\n     --data \"grant_type=client_credentials\" \\\n     --data \"client_id=&lt;your_client&gt;\" \\\n     --data \"client_secret=&lt;your_secret&gt;\"\n</code></pre> <p>You can manage your client application on the CSCS Developer Portal.</p> <p></p>"},{"location":"services/firecrest/#cscs-developer-portal","title":"CSCS Developer Portal","text":"<p>The Developer Portal facilitates CSCS users to manage subscriptions to an API at CSCS (such as FirecREST v1/v2).</p> <p>Start by browsing to developer.cscs.ch, then sign in by clicking the \"SIGN-IN\" button on the top right hand corner of the page.</p> <p>Once logged in, you will see a list of APIs that are available to your user.</p> <p>Warning</p> <p>You might not see version 1 or version 2 of some API. You will be able to see all the versions when you subscribe your Application to the API.</p>"},{"location":"services/firecrest/#creating-an-application","title":"Creating an Application","text":"<p>Click on the \"Applications\" button at the top of the screen to manage your Applications.</p> <p></p> <p>To create a new application, click on the \"ADD NEW APPLICATION\" button at the top of the Applications page, and complete the mandatory fields (marked with <code>*</code>). Make sure to give the application a unique name and select the number of requests per minute. When finished, click on the \"Save\" button.</p> <p>Note</p> <p>To subscribe to an API you need at least one application, for which it is possible to use the DefaultApplication.</p> <p>Note</p> <p>The quota of requests per minute will be shared by all subscribers to the Application over all APIs.</p>"},{"location":"services/firecrest/#configuring-production-keys","title":"Configuring Production Keys","text":"<p>Once the Application is created, create the Production Keys (<code>Client ID</code> and <code>Client Secret</code>) by clicking on \"Production Keys\" </p> <p></p> <p>Use this if this is your first FirecREST application, or if you wish to create new keys.</p> <ul> <li>click on the \"Generate Keys\" button at the bottom of the page</li> </ul> <p></p> <p>Once the keys are generated, you will see the pair \"Consumer Key\" and \"Consumer Secret\".</p> <p></p> <p>Warning</p> <p>Store this pair of credentials securely, these are the access keys to your resources at CSCS.</p>"},{"location":"services/firecrest/#subscribe-to-an-api","title":"Subscribe to an API","text":"<p>Once you have set up your Application, is time to subscribe it to an API.</p> <p>To do so:</p> <ul> <li>(8a) click on the \"Subscriptions\" option on the left panel</li> <li>(8b) click the  \"Subscribe APIS\" button</li> <li>(8c) choose the API you want to subscribe to by clicking the \"Subscribe\" button</li> </ul> <p></p> <p>Back on the Subscription Management page, you can review your active subscriptions and APIs that your Application has access to.</p> <p></p> <p>To use your Application to access FirecREST, follow the API documentation.</p>"},{"location":"services/firecrest/#getting-started","title":"Getting Started","text":""},{"location":"services/firecrest/#using-the-python-interface","title":"Using the Python Interface","text":"<p>One way to get started is by using pyFirecREST, a Python package with a collection of wrappers for the different functionalities of FirecREST. This package simplifies the usage of FirecREST by making multiple requests in the background for more complex workflows as well as by refreshing the access token before it expires.</p> Version 2Version 1 Try FirecREST using pyFirecREST v2 <pre><code>import json\nimport firecrest as f7t\n\nclient_id = \"&lt;client_id&gt;\"\nclient_secret = \"&lt;client_secret&gt;\"\ntoken_uri = \"https://auth.cscs.ch/auth/realms/firecrest-clients/protocol/openid-connect/token\"\n\n# Setup the client for the specific account\n# For instance, for the Alps HPC Platform system Daint:\n\nclient = f7t.v2.Firecrest(\n    firecrest_url=\"https://api.cscs.ch/hpc/firecrest/v2\",\n    authorization=f7t.ClientCredentialsAuth(client_id, client_secret, token_uri)\n)\n\n# Status of the systems, filesystems and schedulers:\nprint(json.dumps(client.systems(), indent=2))\n\n# Output: information about systems and health status of the infrastructure\n# [\n#   {\n#     \"name\": \"daint\",\n#     \"ssh\": {                           # --&gt; SSH settings\n#       \"host\": \"daint.alps.cscs.ch\",\n#       \"port\": 22,\n#       \"maxClients\": 100,\n#       \"timeout\": {\n#         \"connection\": 5,\n#         \"login\": 5,\n#         \"commandExecution\": 5,\n#         \"idleTimeout\": 60,\n#         \"keepAlive\": 5\n#       }\n#     },\n#     \"scheduler\": {                     # --&gt; Scheduler settings\n#       \"type\": \"slurm\",\n#       \"version\": \"24.05.4\",\n#       \"apiUrl\": null,\n#       \"apiVersion\": null,\n#       \"timeout\": 10\n#     },\n#     \"servicesHealth\": [                # --&gt; Health status of services\n#       {\n#         \"serviceType\": \"scheduler\",\n#         \"lastChecked\": \"2025-03-18T23:34:51.167545Z\",\n#         \"latency\": 0.4725925922393799,\n#         \"healthy\": true,\n#         \"message\": null,\n#         \"nodes\": {\n#           \"available\": 21,\n#           \"total\": 858\n#         }\n#       },\n#       {\n#         \"serviceType\": \"ssh\",\n#         \"lastChecked\": \"2025-03-18T23:34:52.054056Z\",\n#         \"latency\": 1.358715295791626,\n#         \"healthy\": true,\n#         \"message\": null\n#       },\n#       {\n#         \"serviceType\": \"filesystem\",\n#         \"lastChecked\": \"2025-03-18T23:34:51.969350Z\",\n#         \"latency\": 1.2738196849822998,\n#         \"healthy\": true,\n#         \"message\": null,\n#         \"path\": \"/capstor/scratch/cscs\"\n#       },\n#     (...)\n#     \"fileSystems\": [                   # --&gt; Filesystem settings\n#       {\n#         \"path\": \"/capstor/scratch/cscs\",\n#         \"dataType\": \"scratch\",\n#         \"defaultWorkDir\": true\n#       },\n#       {\n#         \"path\": \"/users\",\n#         \"dataType\": \"users\",\n#         \"defaultWorkDir\": false\n#       },\n#       {\n#         \"path\": \"/capstor/store/cscs\",\n#         \"dataType\": \"store\",\n#         \"defaultWorkDir\": false\n#       }\n#     ]    \n#   }\n# ]\n\n# List content of directories\nprint(json.dumps(client.list_files(\"daint\", \"/capstor/scratch/cscs/&lt;username&gt;\"),\n                                indent=2))\n\n# [\n#   {\n#     \"name\": \"directory\",\n#     \"type\": \"d\",\n#     \"linkTarget\": null,\n#     \"user\": \"&lt;username&gt;\",\n#     \"group\": \"&lt;project&gt;\",\n#     \"permissions\": \"rwxr-x---+\",\n#     \"lastModified\": \"2024-09-02T12:34:45\",\n#     \"size\": \"4096\"\n#   },\n#   {\n#     \"name\": \"file.txt\",\n#     \"type\": \"-\",\n#     \"linkTarget\": null,\n#     \"user\": \"&lt;username&gt;\",\n#     \"group\": \"&lt;project&gt;\",\n#     \"permissions\": \"rw-r-----+\",\n#     \"lastModified\": \"2024-09-02T08:26:04\",\n#     \"size\": \"131225\"\n#   }\n# ]\n</code></pre> Try FirecREST using pyFirecREST v1 <pre><code>import json\nimport firecrest as f7t\n\nclient_id = \"&lt;client_id&gt;\"\nclient_secret = \"&lt;client_secret&gt;\"\ntoken_uri = \"https://auth.cscs.ch/auth/realms/firecrest-clients/protocol/openid-connect/token\"\n\n# Setup the client for the specific account\n# For instance, for the Alps HPC Platform system Daint:\n\nclient = f7t.v1.Firecrest(\n    firecrest_url=\"https://api.cscs.ch/hpc/firecrest/v1\",\n    authorization=f7t.ClientCredentialsAuth(client_id, client_secret, token_uri)\n)\n\nprint(json.dumps(client.all_systems(), indent=2))\n# Output: (one dictionary per system)\n# [{\n#      'description': 'System ready',\n#      'status': 'available',\n#      'system': 'daint'      \n# }]\n\nprint(json.dumps(client.list_files('daint', '/capstor/scratch/cscs/&lt;username&gt;'),\n                                indent=2))\n# Example output: (one dictionary per file)\n# [\n#   {\n#       'name': 'directory',\n#       'user': '&lt;username&gt;'\n#       'last_modified': '2024-04-20T11:22:41',\n#       'permissions': 'rwxr-xr-x',\n#       'size': '4096',\n#       'type': 'd',\n#       'group': '&lt;project&gt;',\n#       'link_target': '',\n#   }\n#   {\n#      'name': 'file.txt',\n#      'user': '&lt;username&gt;'\n#      'last_modified': '2024-09-02T08:26:04',\n#      'permissions': 'rw-r--r--',\n#      'size': '131225',\n#      'type': '-',\n#      'group': '&lt;project&gt;',\n#      'link_target': '',\n#   }\n# ]\n</code></pre> <p>The tutorial is written for a generic instance of FirecREST but if you have a valid user at CSCS you can test it directly with your resource allocation on the exposed systems.</p>"},{"location":"services/firecrest/#data-transfer-with-firecrest","title":"Data transfer with FirecREST","text":"<p>In addition to the external transfer methods at CSCS, FirecREST provides automated data transfer within the API.</p> <p>A staging area is used for external transfers and downloading/uploading a file from/to a CSCS filesystem.</p> <p>Note</p> <p>pyFirecREST (both v1 and v2) hides this complexity to the user. We strongly recommend to use this library for these tasks.</p> Version 2Version 1 <p>Please follow the steps below to download a file:</p> <ol> <li>Request FirecREST to move the file to the staging area: a download link will be provided</li> <li>The file will remain in the staging area for 7 days or until the link gets invalidated with a request to the <code>/storage/xfer-external/invalidate</code> endpoint or through the pyfirecrest method</li> <li>The staging area is common for all users, therefore users should invalidate the link as soon as the download has been completed</li> </ol> <p>You can see the full process in this tutorial.</p> <p>We may be forced to delete older files sooner than 7 days whenever large files are moved to the staging area and the link is not invalidated after the download, to avoid issues for other users: we will contact the user in this case.</p> <p>When uploading files through the staging area, you don't need to invalidate the link. FirecREST will do it automatically as soon as it transfers the file to the filesystem of CSCS.</p> <p>There is also a constraint on the size of a single file to transfer externally to our systems via FirecREST: 5 GB.</p> <p>If you wish to transfer data bigger than the limit mentioned above, you can check the compress and extract endpoints or follow the following example on how to split large files and download/upload them using FirecREST.</p> <p>The limit on the time and size of files that can be download/uploaded via FirecREST might change if needed.</p> <p>Checking the current values in the parameters endpoint</p> <pre><code>&gt;&gt;&gt; print(json.dumps(client.parameters(), indent = 2))\n{\n(...)\n\n\"storage\": [\n    {\n    \"description\": \"Type of object storage, like `swift`, `s3v2` or `s3v4`.\",\n    \"name\": \"OBJECT_STORAGE\",\n    \"unit\": \"\",\n    \"value\": \"s3v4\"\n    },\n    {\n    \"description\": \"Expiration time for temp URLs.\",\n    \"name\": \"STORAGE_TEMPURL_EXP_TIME\",\n    \"unit\": \"seconds\",\n    \"value\": \"604800\"  ## &lt;-------- 7 days\n    },\n    {\n    \"description\": \"Maximum file size for temp URLs.\",\n    \"name\": \"STORAGE_MAX_FILE_SIZE\",\n    \"unit\": \"MB\",\n    \"value\": \"5120\"   ## &lt;--------- 5 GB\n    }\n(...)\n}\n</code></pre> <p>Job submission through FirecREST</p> <p>FirecREST provides an abstraction for job submission using in the backend the SLURM scheduler of the vCluster.</p> <p>When submitting a job via the different endpoints, you should pass the <code>-l</code> option to the <code>/bin/bash</code> command on the batch file.</p> <pre><code>#!/bin/bash -l\n\n#SBATCH --nodes=1\n...\n</code></pre> <p>This option ensures that the job submitted uses the same environment as your login shell to access the system-wide profile (<code>/etc/profile</code>) or to your profile (in files like <code>~/.bash_profile</code>, <code>~/.bash_login</code>, or <code>~/.profile</code>).</p>"},{"location":"services/firecrest/#upload","title":"Upload","text":"<p>Upload a large file using FirecREST-v2</p> <pre><code>import firecrest as f7t\n\n(...)\n\nsystem = \"daint\"\nsource_path = \"/path/to/local/file\"\ntarget_dir = \"/capstor/scratch/cscs/&lt;username&gt;\"\ntarget_file = \"file\"\naccount = \"&lt;project&gt;\"\n\n\nupload_task = client.upload(system,\n                            local_file=source_path,\n                            directory=target_dir,\n                            filename=target_file,\n                            account=account,\n                            blocking=True)    \n</code></pre>"},{"location":"services/firecrest/#download","title":"Download","text":"<p>Download a large file using FirecREST-v2</p> <pre><code>import firecrest as f7t\n\n(...)\n\nsystem = \"daint\"\nsource_path = \"/capstor/scratch/cscs/&lt;username&gt;/file\"\ntarget_path = \"/path/to/local/file\"\naccount = \"&lt;project&gt;\"\n\n\ndownload_task = client.download(system,\n                                source_path=source_path,\n                                target_path=target_path,\n                                account=account,\n                                blocking=True)\n</code></pre>"},{"location":"services/firecrest/#further-information","title":"Further Information","text":"<ul> <li>FirecREST UI for HPC Platform</li> <li>FirecREST OpenAPI Specification Version 1</li> <li>FirecREST OpenAPI Specification Version 2</li> <li>FirecREST Docs Version 1</li> <li>FirecREST Docs Version 2</li> <li>Documentation of pyFirecREST (v1/v2)</li> <li>FirecREST repository Version 1</li> <li>FirecREST repository Version 2</li> <li>What are JSON Web Tokens</li> <li>Python Requests</li> <li>Python Async API Calls</li> </ul>"},{"location":"software/","title":"Index","text":""},{"location":"software/#software","title":"Software","text":"<p>CSCS provides a catalogue of software on Alps, include scientific applications, tools and programming environments.</p>"},{"location":"software/container-engine/","title":"Container Engine","text":""},{"location":"software/container-engine/#container-engine","title":"Container Engine","text":"<p>The Container Engine (CE) toolset is designed to enable computing jobs to seamlessly run inside Linux application containers, thus providing support for containerized user environments.</p>"},{"location":"software/container-engine/#concept","title":"Concept","text":"<p>Containers effectively encapsulate a software stack; however, to be useful in HPC computing environments, they often require the customization of bind mounts, environment variables, working directories, hooks, plugins, etc. To simplify this process, the Container Engine (CE) toolset supports the specification of user environments through Environment Definition Files.</p> <p>An Environment Definition File (EDF) is a text file in the TOML format that declaratively and prescriptively represents the creation of a computing environment based on a container image. Users can create their own custom environments and share, edit, or build upon already existing environments.</p> <p>The Container Engine (CE) toolset leverages its tight integration with the Slurm workload manager to parse EDFs directly from the command line or batch script and instantiate containerized user environments seamlessly and transparently.</p> <p>Through the EDF, container use cases can be abstracted to the point where end users perform their workflows as if they were operating natively on the computing system.</p> <p>Key Benefits</p> <ul> <li>Freedom: Container gives users full control of the user space. The user can decide what to install without involving a sysadmin.</li> <li>Reproducibility: Workloads consistently run in the same environment, ensuring uniformity across job experimental runs.</li> <li>Portability: The self-contained nature of containers simplifies the deployment across architecture-compatible HPC systems.</li> <li>Seamless Access to HPC Resources: CE facilitates native access to specialized HPC resources like GPUs, interconnects, and other system-specific tools crucial for performance</li> </ul>"},{"location":"software/container-engine/#quick-start","title":"Quick Start","text":"<p>Let's set up a containerized Ubuntu 24.04 environment using a scratch folder as the working directory.</p>"},{"location":"software/container-engine/#example-edf","title":"Example EDF","text":"<pre><code>image = \"library/ubuntu:24.04\"\nmounts = [\"/capstor/scratch/cscs/${USER}:/capstor/scratch/cscs/${USER}\"]\nworkdir = \"/capstor/scratch/cscs/${USER}\"\n</code></pre> <p>Note: Ensure that your <code>${USER}</code> environment variable is defined with your actual username.</p> <p>Save this file as\u00a0<code>ubuntu.toml</code> file in <code>$HOME/.edf</code> directory (which is the default location of EDF files). A more detailed explanation of each entry for the EDF can be seen in the\u00a0EDF reference</p>"},{"location":"software/container-engine/#running-the-environment","title":"Running the environment","text":"<p>Use Slurm in the cluster login node to start the Ubuntu environment that was just defined as follows:</p> <pre><code>$ srun --environment=ubuntu --pty bash\n</code></pre> <p>Since the <code>ubuntu.toml</code> file is located in the EDF search path, the filename can be passed to the option without the file extension.</p> <p>launching a containerized environment</p> <p>The above terminal snippet demonstrates how to launch a containerized environment using Slurm with the <code>--environment</code> option. Click on the  icon for information on each command.</p> <pre><code>[daint-ln002]$ srun --environment=ubuntu --pty bash   # (1)\n\n[nid005333]$ pwd                                    # (2)\n/capstor/scratch/cscs/&lt;username&gt;\n\n[nid005333]$ cat /etc/os-release                    # (3)\nPRETTY_NAME=\"Ubuntu 24.04 LTS\"\nNAME=\"Ubuntu\"\nVERSION_ID=\"24.04\"\nVERSION=\"24.04 LTS (Noble Numbat)\"\nVERSION_CODENAME=noble\nID=ubuntu\nID_LIKE=debian\nHOME_URL=\"https://www.ubuntu.com/\"\nSUPPORT_URL=\"https://help.ubuntu.com/\"\nBUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\nPRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\nUBUNTU_CODENAME=noble\nLOGO=ubuntu-logo\n\n[nid005333]$ exit                                  # (4)\n[daint-ln002]$\n</code></pre> <ol> <li>Starting an interactive shell session within the Ubuntu 24.04 container deployed on a compute node using <code>srun --environment=ubuntu --pty bash</code>.</li> <li>Confirm the working directory inside the container (<code>pwd</code>) and that it is set to the user's scratch folder, as per EDF.</li> <li>Show the OS version of your container\u00a0(using <code>cat /etc/os-release</code>)\u00a0based on Ubuntu 24.04 LTS.</li> <li>Exiting the container (<code>exit</code>), returning to the login node.</li> </ol> <p>Note that the image pull and the container start happen automatically, streamlining the usage of the CE.</p>"},{"location":"software/container-engine/#running-containerized-environments","title":"Running containerized environments","text":"<p>A job is run in a containerized environment by passing the <code>--environment</code> option to the <code>srun</code> or <code>salloc</code> Slurm commands. The option takes a file path to the EDF describing the environment in which the job should be executed, for example:</p> <pre><code>&gt; srun --environment=$SCRATCH/edf/debian.toml cat /etc/os-release\nPRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"12\"\nVERSION=\"12 (bookworm)\"\nVERSION_CODENAME=bookworm\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n\n`--environment` can be a relative path from the current working directory (i.e., where the Slurm command is entered). A relative path should be prepended by `./`. For example:\n\n&gt; ls\ndebian.toml\n&gt; srun --environment=./debian.toml cat /etc/os-release\nPRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"12\"\nVERSION=\"12 (bookworm)\"\nVERSION_CODENAME=bookworm\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n</code></pre> <p>If a file is located in the EDF search path, the argument to the command line option can be just the environment name, that is the name of the file without the <code>.toml</code> extension, for example:</p> <pre><code>&gt; srun --environment=debian cat /etc/os-release\nPRETTY_NAME=\"Debian GNU/Linux 12 (bookworm)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"12\"\nVERSION=\"12 (bookworm)\"\nVERSION_CODENAME=bookworm\nID=debian\nHOME_URL=\"https://www.debian.org/\"\nSUPPORT_URL=\"https://www.debian.org/support\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\n</code></pre>"},{"location":"software/container-engine/#use-from-batch-scripts","title":"Use from batch scripts","text":"<p>In principle, the <code>--environment</code> option can also be used within batch scripts as an <code>#SBATCH</code> option. It is important to note that in such a case, all the contents of the script are executed within the containerized environment: the CE toolset gives access to the Slurm workload manager within containers via the Slurm hook, see section Container Hooks (controlled by the\u00a0<code>ENROOT_SLURM_HOOK</code> environment variable and activated by default on most vClusters). Only with it, calls to Slurm commands (for example <code>srun</code> or <code>scontrol</code>) within the batch script will work.</p> <p>Tip</p> <p>For the time being, if the script requires to invoke Slurm commands, the recommended approach is to use <code>--environment</code> as part of the commands, for example, when launching job steps the following slurm batch job:</p> <pre><code>#!/bin/bash -l\n#SBATCH --job-name=edf-example\n#SBATCH --time=0:01:00\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --partition=&lt;vcluster&gt;\n#SBATCH --output=slurm-%x.out\n\n# Run job step\nsrun --environment=debian cat /etc/os-release\n</code></pre> <p></p>"},{"location":"software/container-engine/#the-edf-search-path","title":"The EDF search path","text":"<p>By default, the EDFs for each user are looked up in <code>$HOME/.edf</code>. The search path for EDFs can be controlled through the <code>EDF_PATH</code> environment variable. <code>EDF_PATH</code> must be a colon-separated list of absolute paths to directories where the CE looks for TOML files, similar to the <code>PATH</code> and L<code>D_LIBRARY_PATH</code> variables. If a file is located in the search path, its name can be used in <code>--environment</code> options without the <code>.toml</code> extension, for example:</p> <pre><code>&gt; ls -l ~/.edf\ntotal 8\n-rw-r--r-- 1 &lt;username&gt; csstaff  27 Sep  6 15:19 debian.toml\n\n&gt; ls -l ~/example-project/\ntotal 4\n-rw-r-----+ 1 &lt;username&gt; csstaff 28 Oct 26 17:44 fedora-env.toml\n\n&gt; export EDF_PATH=$HOME/example-project/\n\n&gt; srun --environment=fedora-env cat /etc/os-release\nNAME=\"Fedora Linux\"\nVERSION=\"40 (Container Image)\"\nID=fedora\nVERSION_ID=40\nVERSION_CODENAME=\"\"\nPLATFORM_ID=\"platform:f40\"\nPRETTY_NAME=\"Fedora Linux 40 (Container Image)\"\n[...]\n</code></pre>"},{"location":"software/container-engine/#image-management","title":"Image Management","text":""},{"location":"software/container-engine/#image-cache","title":"Image cache","text":"<p>Info</p> <p>The image caching functionality is only available on the Bristen vCluster as technical preview.</p> <p>By default, images defined in the EDF as remote registry references (e.g. a Docker reference) are automatically pulled and locally cached. A cached image would be preferred to pulling the image again in later usage.\u00a0</p> <p>An image cache is automatically created at\u00a0<code>.edf_imagestore</code> in the user's scratch folder (i.e.,\u00a0<code>$SCRATCH/.edf_imagestore</code>), under which cached images are stored in a corresponding CPU architecture subfolder (e.g., <code>x86</code> and <code>aarch64</code>). Users should regularly remove unused cache images to limit the cache size.</p> <p>Should users want to re-pull a cached image, they have to remove the corresponding image in the cache.</p> <p>To choose an alternative image store path (e.g., to use a directory owned by a group and not to an individual user), users can specify an image cache path explicitly by defining the environment variable <code>EDF_IMAGESTORE</code>.\u00a0<code>EDF_IMAGESTORE</code> must be an absolute path to an existing folder.</p> <p>Note</p> <p>If the CE cannot create a directory for the image cache, it operates in cache-free mode, meaning that it pulls an ephemeral image before every container launch and discards it upon termination.</p>"},{"location":"software/container-engine/#pulling-images-manually","title":"Pulling images\u00a0manually","text":"<p>To work with images stored from the NGC Catalog, please refer also to the next section \"Using images from third party registries and private repositories\".</p> <p>To bypass any caching behavior, users can manually pull an image and directly plug it into their EDF. To do so, users may execute <code>enroot import docker://[REGISTRY#]IMAGE[:TAG]</code> to pull container images from OCI registries to the current directory.</p> <p>For example, the command below pulls an <code>nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04</code> image.</p> <pre><code>$ enroot import docker://nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n</code></pre> Image import w/ full output <pre><code>&gt; srun enroot import docker://nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n[INFO] Querying registry for permission grant\n[INFO] Authenticating with user: &lt;anonymous&gt;\n[INFO] Authentication succeeded\n[INFO] Fetching image manifest list\n[INFO] Fetching image manifest\n[INFO] Downloading 13 missing layers...\n[INFO] Extracting image layers...\n[INFO] Converting whiteouts...\n[INFO] Creating squashfs filesystem...\nParallel mksquashfs: Using 64 processors\nCreating 4.0 filesystem on /scratch/aistor/&lt;username&gt;/nvidia+cuda+11.8.0-cudnn8-devel-ubuntu22.04.sqsh, block size 131072.\n\nExportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072\n    uncompressed data, compressed metadata, compressed fragments,\n    compressed xattrs, compressed ids\n    duplicates are removed\nFilesystem size 9492185.87 Kbytes (9269.71 Mbytes)\n    98.93% of uncompressed filesystem size (9594893.12 Kbytes)\nInode table size 128688 bytes (125.67 Kbytes)\n    17.47% of uncompressed inode table size (736832 bytes)\nDirectory table size 132328 bytes (129.23 Kbytes)\n    46.42% of uncompressed directory table size (285091 bytes)\nNumber of duplicate files found 1069\nNumber of inodes 13010\nNumber of files 10610\nNumber of fragments 896\nNumber of symbolic links  846\nNumber of device nodes 0\nNumber of fifo nodes 0\nNumber of socket nodes 0\nNumber of directories 1554\nNumber of ids (unique uids + gids) 1\nNumber of uids 1\n    root (0)\nNumber of gids 1\n    root (0)\n</code></pre> <p>After the import is complete, images are available in Squashfs format in the current directory and can be used in EDFs, for example:</p> <pre><code>&gt; ls -l *.sqsh\n-rw-r--r-- 1 &lt;username&gt; csstaff 9720037376 Sep 11 14:46 nvidia+cuda+11.8.0-cudnn8-devel-ubuntu22.04.sqsh\n\n&gt; realpath nvidia+cuda+11.8.0-cudnn8-devel-ubuntu22.04.sqsh \u00a0/capstor/scratch/cscs/&lt;username&gt;/nvidia+cuda+11.8.0-cudnn8-devel-ubuntu22.04.sqsh\n\n&gt; cat $HOME/.edf/cudnn8.toml\nimage = \"/capstor/scratch/cscs/&lt;username&gt;/nvidia+cuda+11.8.0-cudnn8-devel-ubuntu22.04.sqsh\"\n</code></pre> <p>Note</p> <p>It is recommended to save images in <code>/capstor/scratch/cscs/&lt;username&gt;</code> or its subdirectories before using them with the CE.</p> <p></p>"},{"location":"software/container-engine/#third-party-and-private-registries","title":"Third-party and private registries","text":"<p>Docker Hub is the default registry from which remote images are imported.</p> <p>Registry rate limits</p> <p>Some registries will rate limit image pulls by IP address. Since public IPs are a shared resource we recommend authenticating even for publicly available images. For example, Docker Hub applies its rate limits per user when authenticated.</p> <p>To use an image from a different registry, the corresponding registry URL has to be prepended to the image reference, using a hash character (#) as a separator. For example:</p> <pre><code># Usage within an EDF\n&gt; cat $HOME/.edf/nvhpc-23.7.toml\nimage = \"nvcr.io#nvidia/nvhpc:23.7-runtime-cuda11.8-ubuntu22.04\"\n\n# Usage on the command line\n&gt; srun enroot import docker://nvcr.io#nvidia/nvhpc:23.7-runtime-cuda11.8-ubuntu22.04\n</code></pre> <p>To import images from private repositories, access credentials should be configured by individual users in the <code>$HOME/.config/enroot/.credentials</code> file, following the netrc file format. Using the <code>enroot import</code> documentation page as a reference, some examples could be:</p> <p><pre><code># NVIDIA NGC catalog (both endpoints are required)\nmachine nvcr.io login $oauthtoken password &lt;token&gt;\nmachine authn.nvidia.com login $oauthtoken password &lt;token&gt;\n\n# DockerHub\nmachine auth.docker.io login &lt;login&gt; password &lt;password&gt;\n\n# Google Container Registry with OAuth\nmachine gcr.io login oauth2accesstoken password $(gcloud auth print-access-token)\n# Google Container Registry with JSON\nmachine gcr.io login _json_key password $(jq -c '.' $GOOGLE_APPLICATION_CREDENTIALS | sed 's/ /\\\\u0020/g')\n\n# Amazon Elastic Container Registry\nmachine 12345.dkr.ecr.eu-west-2.amazonaws.com login AWS password $(aws ecr get-login-password --region eu-west-2)\n\n# Azure Container Registry with ACR refresh token\nmachine myregistry.azurecr.io login 00000000-0000-0000-0000-000000000000 password $(az acr login --name myregistry --expose-token --query accessToken  | tr -d '\"')\n# Azure Container Registry with ACR admin user\nmachine myregistry.azurecr.io login myregistry password $(az acr credential show --name myregistry --subscription mysub --query passwords[0].value | tr -d '\"')\n\n# Github.com Container Registry (GITHUB_TOKEN needs read:packages scope)\nmachine ghcr.io login &lt;username&gt; password &lt;GITHUB_TOKEN&gt;\n\n# GitLab Container Registry (GITLAB_TOKEN needs a scope with read access to the container registry)\n# GitLab instances often use different domains for the registry and the authentication service, respectively\n# Two separate credential entries are required in such cases, for example:\n# Gitlab.com\nmachine registry.gitlab.com login &lt;username&gt; password &lt;GITLAB TOKEN&gt;\nmachine gitlab.com login &lt;username&gt; password &lt;GITLAB TOKEN&gt;\n\n# ETH Zurich GitLab registry\nmachine registry.ethz.ch login &lt;username&gt; password &lt;GITLAB_TOKEN&gt;\nmachine gitlab.ethz.ch login &lt;username&gt; password &lt;GITLAB_TOKEN&gt;  \n</code></pre> </p>"},{"location":"software/container-engine/#annotations","title":"Annotations","text":"<p>Annotations define arbitrary metadata for containers in the form of key-value pairs. Within the EDF, annotations are designed to be similar in appearance and behavior to those defined by the OCI Runtime Specification. Annotation keys usually express a hierarchical namespace structure, with domains separated by \".\"\u00a0(full stop) characters.</p> <p>As annotations are often used to control hooks, they have a deep nesting level. For example, to execute the SSH hook described below, the annotation\u00a0<code>com.hooks.ssh.enabled</code> must be set to the string <code>true</code>.</p> <p>EDF files support setting annotations through the <code>annotations</code> table. This can be done in multiple ways in TOML: for example, both of the following usages are equivalent:</p> <ul> <li> <p>Case: nest levels in the TOML key.     <pre><code>[annotations]\ncom.hooks.ssh.enabled = \"true\"\n</code></pre></p> </li> <li> <p>Case: nest levels in the TOML table name.     <pre><code>[annotations.com.hooks.ssh]\nenabled = \"true\"\n</code></pre></p> </li> </ul> <p>To avoid mistakes, notice a few key features of TOML:</p> <ul> <li>All property assignments belong to the section immediately preceding them (the statement in square brackets), which defines the table they refer to.</li> <li>Tables, on the other hand, do not automatically belong to the tables declared before them; to nest tables, their name has to list their parents using the dot notations (so the previous example defines the table <code>ssh</code> inside <code>hooks</code>, which in turn is inside <code>com</code>, which is inside <code>annotations</code>).</li> <li>An assignment can implicitly define subtables if the key you assign is a dotted list. As a reference, see the examples made earlier in this section, where assigning a string to the <code>com.hooks.ssh.enabled</code> attribute within the <code>[annotations]</code> table is exactly equivalent to assigning to the <code>enabled</code> attribute within the <code>[annotations.com.hooks.ssh]</code> subtable.</li> <li> <p>Attributes can be added to a table only in one place in the TOML file. In other words, each table must be defined in a single square bracket section. For example, Case 3 in the example below is invalid because the <code>ssh</code> table was doubly defined both in the <code>[annotations]</code>\u00a0and in the <code>[annotations.com.hooks.ssh]</code> sections. See the TOML format spec for more details.</p> <ul> <li> <p>Case 1 (valid):     <pre><code>[annotations.com.hooks.ssh]\nauthorize_ssh_key = \"/capstor/scratch/cscs/&lt;username&gt;/tests/edf/authorized_keys\"\nenabled = \"true\"\n</code></pre></p> </li> <li> <p>Case 2 (valid):     <pre><code>[annotations]\ncom.hooks.ssh.authorize_ssh_key = \"/capstor/scratch/cscs/&lt;username&gt;/tests/edf/authorized_keys\"\ncom.hooks.ssh.enabled = \"true\"\n</code></pre></p> </li> <li> <p>Case 3 (invalid):     <pre><code>[annotations]\ncom.hooks.ssh.authorize_ssh_key = \"/capstor/scratch/cscs/&lt;username&gt;/tests/edf/authorized_keys\"\n\n[annotations.com.hooks.ssh]\nenabled = \"true\"\n</code></pre></p> </li> </ul> </li> </ul>"},{"location":"software/container-engine/#accessing-native-resources","title":"Accessing native resources","text":""},{"location":"software/container-engine/#nvidia-gpus","title":"NVIDIA GPUs","text":"<p>The Container Engine leverages components from the NVIDIA Container Toolkit to expose NVIDIA GPU devices inside containers. GPU device files are always mounted in containers, and the NVIDIA driver user space components are\u00a0 mounted if the <code>NVIDIA_VISIBLE_DEVICES</code> environment variable is not empty, unset or set to <code>void</code>. \u00a0<code>NVIDIA_VISIBLE_DEVICES</code> is already set in container images officially provided by NVIDIA to enable all GPUs available on the host system. Such images are frequently used to containerize CUDA applications, either directly or as a base for custom images, thus in many cases no action is required to access GPUs. For example, on a cluster with 4 GH200 devices per compute node:</p> <pre><code>&gt; cat .edf/cuda12.5.1.toml\u00a0\nimage = \"nvidia/cuda:12.5.1-devel-ubuntu24.04\"\n\n&gt; srun --environment=cuda12.5.1 nvidia-smi\nThu Oct 26 17:59:36 2023 \u00a0 \u00a0 \u00a0\u00a0\n+------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03          Driver Version: 535.129.03   CUDA Version: 12.5     |\n|--------------------------------------+----------------------+----------------------+\n| GPU  Name              Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf       Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                      |                      |               MIG M. |\n|======================================+======================+======================|\n|   0  GH200 120GB                 On  | 00000009:01:00.0 Off |                    0 |\n| N/A   24C    P0           89W / 900W |     37MiB / 97871MiB |      0%   E. Process |\n|                                      |                      |             Disabled |\n+--------------------------------------+----------------------+----------------------+\n|   1  GH200 120GB                 On  | 00000019:01:00.0 Off |                    0 |\n| N/A   24C    P0           87W / 900W |     37MiB / 97871MiB |      0%   E. Process |\n|                                      |                      |             Disabled |\n+--------------------------------------+----------------------+----------------------+\n|   2  GH200 120GB                 On  | 00000029:01:00.0 Off |                    0 |\n| N/A   24C    P0           83W / 900W |     37MiB / 97871MiB |      0%   E. Process |\n|                                      |                      |             Disabled |\n+--------------------------------------+----------------------+----------------------+\n|   3  GH200 120GB                 On  | 00000039:01:00.0 Off |                    0 |\n| N/A   24C    P0           85W / 900W |     37MiB / 97871MiB |      0%   E. Process |\n|                                      |                      |             Disabled |\n+--------------------------------------+----------------------+----------------------+\n\n+------------------------------------------------------------------------------------+\n| Processes:                                                                         |\n|  GPU   GI   CI        PID   Type   Process name                         GPU Memory |\n|        ID   ID                                                          Usage      |\n|====================================================================================|\n|  No running processes found                                                        |\n+------------------------------------------------------------------------------------+\n</code></pre> <p>It is possible to use environment variables to control which capabilities of the NVIDIA driver are enabled inside containers. Additionally, the NVIDIA Container Toolkit can enforce specific constraints for the container, for example, on versions of the CUDA runtime or driver, or on the architecture of the GPUs. For the full details about using these features, please refer to the official documentation: Driver Capabilities, Constraints.</p> <p></p>"},{"location":"software/container-engine/#hpe-slingshot-interconnect","title":"HPE Slingshot interconnect","text":"<p>The Container Engine provides a hook to allow containers relying on\u00a0libfabric to leverage the HPE Slingshot 11 high-speed interconnect. This component is commonly referred to as the \"CXI hook\", taking its name from the CXI libfabric provider required to interface with Slingshot 11. The hook leverages bind-mounting the custom host libfabric library into the container (in addition to all the required dependency libraries and devices as well). If a libfabric library is already present in the container filesystem (for example, it's provided by the image), it is replaced with its host counterpart, otherwise the host libfabric is just added to the container.</p> <p>Note</p> <p>Due to the nature of Slingshot and the mechanism implemented by the CXI hook, container applications need to use a communication library which supports libfabric in order to benefit from usage of the hook.</p> <p>Libfabric support might have to be defined at compilation time (as is the case for some MPI implementations, like MPICH and OpenMPI) or could be dynamically available at runtime (as is the case with NCCL - see also this section for more details).</p> <p>The hook is activated by setting the\u00a0<code>com.hooks.cxi.enabled</code> annotation, which can be defined in the EDF, as shown in the following example:</p> <pre><code># Without the CXI hook\n&gt; cat $HOME/.edf/osu-mb.toml\u00a0\nimage = \"quay.io#madeeks/osu-mb:6.2-mpich4.1-ubuntu22.04-arm64\"\n\n[annotations]\ncom.hooks.cxi.enabled = \"false\"\n\n&gt; srun -N2 --mpi=pmi2 --environment=osu-mb ./osu_bw\n# OSU MPI Bandwidth Test v6.2\n# Size      Bandwidth (MB/s)\n1                       0.22\n2                       0.40\n4                       0.90\n8                       1.82\n16                      3.41\n32                      6.81\n64                     13.18\n128                    26.74\n256                    11.95\n512                    38.06\n1024                   39.65\n2048                   83.22\n4096                  156.14\n8192                  143.08\n16384                  53.78\n32768                 106.77\n65536                  49.88\n131072                871.86\n262144                780.97\n524288                694.58\n1048576               831.02\n2097152              1363.30\n4194304              1279.54\n\n\n# With the CXI hook enabling access to the Slingshot high-speed network\n&gt; cat .edf/osu-mb-cxi.toml\u00a0\nimage = \"quay.io#madeeks/osu-mb:6.2-mpich4.1-ubuntu22.04\"\n\n[annotations]\ncom.hooks.cxi.enabled = \"true\"\n\n&gt; srun -N2 --mpi=pmi2 --environment=osu-mb-cxi ./osu_bw\n# OSU MPI Bandwidth Test v6.2\n# Size      Bandwidth (MB/s)\n1                       1.21\n2                       2.32\n4                       4.85\n8                       8.38\n16                     19.36\n32                     38.47\n64                     76.28\n128                   151.76\n256                   301.25\n512                   604.17\n1024                 1145.03\n2048                 2367.25\n4096                 4817.16\n8192                 8633.36\n16384               16971.18\n32768               18740.55\n65536               21978.65\n131072              22962.31\n262144              23436.78\n524288              23672.92\n1048576             23827.78\n2097152             23890.95\n4194304             23925.61\n</code></pre> <p>Tip</p> <p>On several vClusters, the CXI hook for Slingshot connectivity is enabled implicitly by default or by other hooks. Therefore, entering the enabling annotation in the EDF is unnecessary in many cases.</p> <p></p>"},{"location":"software/container-engine/#container-hooks","title":"Container Hooks","text":"<p>Container hooks let you customize container behavior to fit system-specific needs, making them especially valuable for High-Performance Computing.</p> <ul> <li>What they do: Hooks extend container runtime functionality by enabling custom actions during a container's lifecycle.</li> <li>Use for HPC: HPC systems rely on specialized hardware and fine-tuned software, unlike generic containers. Hooks bridge this gap by allowing containers to access these system-specific resources or enable custom features.</li> </ul> <p>Info</p> <p>This section outlines all hooks supported in production by the Container Engine. However, specific Alps vClusters may support only a subset or use custom configurations. For details about available features in individual vClusters, consult platform documentation or contact CSCS support.</p> <p></p>"},{"location":"software/container-engine/#aws-ofi-nccl-hook","title":"AWS OFI NCCL Hook","text":"<p>The\u00a0AWS OFI NCCL plugin is a software extension that allows the NCCL\u00a0and RCCL libraries to use libfabric as a network provider and, through libfabric, to access the Slingshot high-speed interconnect.</p> <p>The Container Engine includes a hook program to inject the AWS OFI NCCL plugin in containers; since the plugin must also be compatible with the GPU programming software stack being used, the\u00a0<code>com.hooks.aws_ofi_nccl.variant</code> annotation is used to specify a plugin variant suitable for a given container image. At the moment of writing, 4 plugin variants are configured:\u00a0<code>cuda11</code>, <code>cuda12</code> (to be used on NVIDIA GPU nodes), <code>rocm5</code>, and <code>rocm6</code> (to be used on AMD GPU nodes alongside RCCL). For example, the following EDF enables the hook and uses it to mount the plugin in a CUDA 11 image:</p> <pre><code>image = \"nvcr.io#nvidia/pytorch:22.12-py3\"\nmounts = [\"/capstor/scratch/cscs/amadonna:/capstor/scratch/cscs/amadonna\"]\nentrypoint = false\n\n[annotations]\ncom.hooks.aws_ofi_nccl.enabled = \"true\"\ncom.hooks.aws_ofi_nccl.variant = \"cuda11\"\n</code></pre> <p>The AWS OFI NCCL hook also takes care of the following aspects:</p> <ul> <li>It implicitly enables the CXI hook, therefore exposing the Slingshot interconnect to container applications. In other words, when enabling the AWS OFI NCCL hook, it's unnecessary to also enable the CXI hook separately in the EDF.</li> <li>It sets environment variables to control the behavior of NCCL and the libfabric CXI provider for Slingshot. In particular, the <code>NCCL_NET_PLUGIN</code> variable (link) is set to force NCCL to load the specific network plugin mounted by the hook. This is useful because certain container images (for example, those from NGC repositories) might already ship with a default NCCL plugin. Other environment variables help prevent application stalls and improve performance when using GPUDirect for RDMA communication.</li> </ul> <p></p>"},{"location":"software/container-engine/#ssh-hook","title":"SSH Hook","text":"<p>The SSH hook runs a lightweight, statically-linked SSH server (a build of Dropbear) inside the container. It can be useful to add SSH connectivity to containers (for example, enabling remote debugging) without bundling an SSH server into the container image or creating ad-hoc image variants for such purposes.</p> <p>The\u00a0<code>com.hooks.ssh.authorize_ssh_key</code> annotation allows the authorization of a custom public SSH key for remote connections. The annotation value must be the absolute path to a text file containing the public key (just the public key without any extra signature/certificate). After the container starts, it is possible to get a remote shell inside the container by connecting with\u00a0SSH to the listening port.</p> <p>By default, the server started by the SSH hook listens to port 15263, but this setting can be controlled through the <code>com.hooks.ssh.port</code> annotation in the EDF.</p> <p>Note</p> <p>To use the SSH hook, it is\u00a0required to keep the container writable.</p> <p>The following EDF file shows an example of enabling the SSH hook and authorizing a user-provided public key:</p> <pre><code>&gt; cat $HOME/.edf/ubuntu-ssh.toml\nimage = \"ubuntu:latest\"\nwritable = true\n\n[annotations.com.hooks.ssh]\nenabled = \"true\"\nauthorize_ssh_key = \"&lt;public key file&gt;\"\n</code></pre> <p>Using the previous EDF, a container can be started as follows. Notice that the\u00a0<code>--pty</code> option for the <code>srun</code> command is currently required in order for the hook to initialize properly:</p> <pre><code>&gt; srun --environment=ubuntu-ssh --pty &lt;command&gt;\n</code></pre> <p>While the container is running, it's possible to connect to it from a remote host using a private key matching the public one authorized in the EDF annotation. For example, in a host where such private key is the default identity file, the following command could be used:</p> <pre><code>ssh -p 15263 &lt;host-of-container&gt;\n</code></pre> <p>Info</p> <p>In order to establish connections through Visual Studio Code Remote - SSH extension, the <code>scp</code> program must be available within the container. This is required to send and establish the VS Code Server into the remote container.</p>"},{"location":"software/container-engine/#nvidia-cuda-mps-hook","title":"NVIDIA CUDA MPS Hook","text":"<p>On several Alps vClusters, NVIDIA GPUs by default operate in \"Exclusive process\" mode, that is, the CUDA driver is configured to allow only one process at a time to use a given GPU. For example, on a node with 4 GPUs, a maximum of 4 CUDA processes can run at the same time:</p> <pre><code>&gt; nvidia-smi -L\nGPU 0: GH200 120GB (UUID: GPU-...)\nGPU 1: GH200 120GB (UUID: GPU-...)\nGPU 2: GH200 120GB (UUID: GPU-...)\nGPU 3: GH200 120GB (UUID: GPU-...)\n\n# This EDF uses the CUDA vector addition sample from NVIDIA's NGC catalog\n&gt; cat $HOME/.edf/vectoradd-cuda.toml\nimage = \"nvcr.io#nvidia/k8s/cuda-sample:vectoradd-cuda12.5.0-ubuntu22.04\"\n\n# 4 processes run successfully\n&gt; srun -t2 -N1 -n4 --environment=vectoradd-cuda /cuda-samples/vectorAdd | grep \"Test PASSED\"\nTest PASSED\nTest PASSED\nTest PASSED\nTest PASSED\n\n# More than 4 concurrent processes result in oversubscription errors\n&gt; srun -t2 -N1 -n5 --environment=vectoradd-cuda /cuda-samples/vectorAdd | grep \"Test PASSED\"\nFailed to allocate device vector A (error code CUDA-capable device(s) is/are busy or unavailable)!\nsrun: error: [...]\n[...]\n</code></pre> <p>In order to run multiple processes concurrently on the same GPU (one example could be running multiple MPI ranks on the same device), the NVIDIA CUDA Multi-Process Service (or MPS, for short) must be started on the compute node.</p> <p>The Container Engine provides a hook to automatically manage the setup and removal of the NVIDIA CUDA MPS components within containers. The hook can be activated by setting the <code>com.hooks.nvidia_cuda_mps.enabled</code> to the string <code>true</code>.</p> <p>Note</p> <p>To use the CUDA MPS hook, it is required to keep the container writable.</p> <p>The following is an example of using the NVIDIA CUDA MPS hook:</p> <pre><code>&gt; cat $HOME/.edf/vectoradd-cuda-mps.toml\nimage = \"nvcr.io#nvidia/k8s/cuda-sample:vectoradd-cuda12.5.0-ubuntu22.04\"\nwritable = true\n\n[annotations]\ncom.hooks.nvidia_cuda_mps.enabled = \"true\"\n\n&gt; srun -t2 -N1 -n8 --environment=vectoradd-cuda-mps /cuda-samples/vectorAdd | grep \"Test PASSED\" | wc -l\n8\n</code></pre> <p>Info</p> <p>When using the NVIDIA CUDA MPS hook it is not necessary to use other wrappers or scripts to manage the Multi-Process Service, as is documented for native jobs on some vClusters.</p> <p></p>"},{"location":"software/container-engine/#edf-reference","title":"EDF Reference","text":"<p>EDF files use the TOML format. For details about the data types used by the different parameters, please refer to the TOML spec webpage.</p> <p>In the following, the default value is none (i.e., the empty value of the corresponding type) if not specified.</p>"},{"location":"software/container-engine/#array-or-string-base_environment","title":"(ARRAY or STRING) base_environment","text":"<p>Ordered list of EDFs that this file inherits from. Parameters from listed environments are evaluated sequentially. Supports up to 10 levels of recursion.</p> Note <ul> <li>Parameters from the listed environments are evaluated sequentially, adding new entries or overwriting previous ones, before evaluating the parameters from the current EDF. In other words, the current EDF inherits the parameters from the EDFs listed in <code>base_environment</code>. When evaluating <code>mounts</code> or <code>env</code> parameters, values from downstream EDFs are appended to inherited values.</li> <li>The individual EDF entries in the array follow the same search rules as the arguments of the\u00a0<code>--environment</code> CLI option for Slurm; they can be either file paths or filenames without extension if the file is located in the EDF search path.</li> <li>This parameter can be a string if there is only one base environment.</li> </ul> Example <ul> <li> <p>Single environment inheritance:     <pre><code>base_environment = \"common_env\"\n</code></pre></p> </li> <li> <p>Multiple environment inheritance:     <pre><code>base_environment = [\"common_env\", \"ml_pytorch_env1\"]\n</code></pre></p> </li> </ul>"},{"location":"software/container-engine/#string-image","title":"(STRING) image","text":"<p>The container image to use. Can reference a remote Docker/OCI registry or a local Squashfs file as a filesystem path.</p> Note <ul> <li>The full format for remote references is <code>[USER@][REGISTRY#]IMAGE[:TAG]</code>.<ul> <li><code>[REGISTRY#]</code>: (optional) registry URL, followed by\u00a0#. Default: Docker Hub.</li> <li><code>IMAGE</code>: image name.</li> <li><code>[:TAG]</code>: (optional) image tag name, preceded by\u00a0:.</li> </ul> </li> <li>The registry user can also be specified in the\u00a0<code>$HOME/.config/enroot/.credentials</code> file.</li> </ul> Example <ul> <li> <p>Reference of Ubuntu image in the Docker Hub registry (default registry)     <pre><code>image = \"library/ubuntu:24.04\"\n</code></pre></p> </li> <li> <p>Explicit reference of Ubuntu image in the Docker Hub registry     <pre><code>image = \"docker.io#library/ubuntu:24.04\"\n</code></pre></p> </li> <li> <p>Reference to PyTorch image from NVIDIA Container Registry (nvcr.io)     <pre><code>image = \"nvcr.io#nvidia/pytorch:22.12-py3\"\n</code></pre></p> </li> <li> <p>Image from third-party quay.io registry     <pre><code>image = \"quay.io#madeeks/osu-mb:6.2-mpich4.1-ubuntu22.04-arm64\"\n</code></pre></p> </li> <li> <p>Reference to a manually pulled image stored in parallel FS     <pre><code>image = \"/path/to/image.squashfs\"\n</code></pre></p> </li> </ul>"},{"location":"software/container-engine/#string-workdir","title":"(STRING) workdir","text":"<p>Initial working directory when the container starts. Default: inherited from image.</p> Example <ul> <li>Workdir pointing to a user defined project path\u00a0     <pre><code>workdir = \"/home/user/projects\"\n</code></pre></li> <li>Workdir pointing to the <code>/tmp</code> directory     <pre><code>workdir = \"/tmp\"\n</code></pre></li> </ul>"},{"location":"software/container-engine/#bool-entrypoint","title":"(BOOL) entrypoint","text":"<p>If true, run the entrypoint from the container image. Default: true.</p> Example <pre><code>entrypoint = false\n</code></pre>"},{"location":"software/container-engine/#bool-writable","title":"(BOOL) writable","text":"<p>If false, the container filesystem is read-only. Default: false.</p> Example <pre><code>writable = true\n</code></pre>"},{"location":"software/container-engine/#array-mounts","title":"(ARRAY) mounts","text":"<p>List of bind mounts in the format <code>SOURCE:DESTINATION[:FLAGS]</code>. Flags are optional and can include <code>ro</code>, <code>private</code>, etc.</p> Note <ul> <li>Mount flags are separated with a plus symbol, for example:\u00a0<code>ro+private</code>.</li> <li>Optional flags from docker format or OCI (need reference)</li> </ul> Example <ul> <li> <p>Literal fixed mount map     <pre><code>mounts = [\"/capstor/scratch/cscs/amadonna:/capstor/scratch/cscs/amadonna\"]\n</code></pre></p> </li> <li> <p>Mapping path with <code>env</code> variable expansion     <pre><code>mounts = [\"/capstor/scratch/cscs/${USER}:/capstor/scratch/cscs/${USER}\"]\n</code></pre></p> </li> <li> <p>Mounting the scratch filesystem using a host environment variable     <pre><code>mounts = [\"${SCRATCH}:/scratch\"]\n</code></pre></p> </li> </ul>"},{"location":"software/container-engine/#table-env","title":"(TABLE) env","text":"<p>Environment variables to set in the container. Null-string values will unset the variable. Default: inherited from the host and the image.</p> Note <ul> <li>By default, containers inherit environment variables from the container image and the host environment, with variables from the image taking precedence.</li> <li>The\u00a0env\u00a0table can be used to further customize the container environment by setting, modifying, or unsetting variables.</li> <li>Values of the table entries must be strings. If an entry has a null value, the variable corresponding to the entry key is unset in the container.</li> </ul> Example <ul> <li> <p>Basic <code>env</code> block     <pre><code>[env]\nMY_RUN = \"production\",\nDEBUG = \"false\"\n</code></pre></p> </li> <li> <p>Use of environment variable expansion     <pre><code>[env]\nMY_NODE = \"${VAR_FROM_HOST}\",\nPATH = \"${PATH}:/custom/bin\", \nDEBUG = \"true\"\n</code></pre></p> </li> </ul>"},{"location":"software/container-engine/#table-annotations","title":"(TABLE) annotations","text":"<p>OCI-like annotations for the container. For more details, refer to the Annotations section.</p> Example <ul> <li> <p>Disabling the CXI hook     <pre><code>[annotations]\ncom.hooks.cxi.enabled = \"false\"\n</code></pre></p> </li> <li> <p>Control of SSH hook parameters via annotation and variable expansion     <pre><code>[annotations.com.hooks.ssh]\nauthorize_ssh_key = \"/capstor/scratch/cscs/${USER}/tests/edf/authorized_keys\"\nenabled = \"true\"\n</code></pre></p> </li> <li> <p>Alternative example for usage of annotation with fixed path     <pre><code>[annotations]\ncom.hooks.ssh.authorize_ssh_key = \"/path/to/authorized_keys\"\ncom.hooks.ssh.enabled = \"true\"\n</code></pre></p> </li> </ul> <p>Note</p> <p>Environment variable expansion and relative paths expansion are only available on the Bristen vCluster as technical preview.</p>"},{"location":"software/container-engine/#environment-variable-expansion","title":"Environment Variable Expansion","text":"<p>Environment variable expansion allows for dynamic substitution of environment variable values within the EDF (Environment Definition File). This capability applies across all configuration parameters in the EDF, providing flexibility in defining container environments.</p> <ul> <li>Syntax. Use ${VAR} to reference an environment variable VAR. The variable's value is resolved from the combined environment, which includes variables defined in the host and the container image, the later taking precedence.</li> <li>Scope. Variable expansion is supported across all EDF parameters. This includes EDF\u2019s parameters like mounts, workdir, image, etc. For example, ${SCRATCH} can be used in mounts to reference a directory path.</li> <li>Undefined Variables. Referencing an undefined variable results in an error. To safely handle undefined variables, you can use the syntax ${VAR:-}, which evaluates to an empty string if VAR is undefined.</li> <li>Preventing Expansion. To prevent expansion, use double dollar signs $$. For example,\u00a0$$${VAR} will render as the literal string ${VAR}.</li> <li>Limitations<ul> <li>Variables defined within the [env] EDF table cannot reference other entries from [env] tables in the same or other EDF files (e.g. the ones entered as base environments) . Therefore, only environment variables from the host or image can be referenced.</li> </ul> </li> <li>Environment Variable Resolution Order. The environment variables are resolved based on the following order:<ul> <li>TOML env: Variable values as defined in EDF\u2019s env.</li> <li>Container Image: Variables defined in the container image's environment take precedence.</li> <li>Host Environment: Environment variables defined in the host system.</li> </ul> </li> </ul>"},{"location":"software/container-engine/#relative-paths-expansion","title":"Relative paths expansion","text":"<p>Relative filesystem paths can be used within EDF parameters, and will be expanded by the CE at runtime. The paths are interpreted as relative to the working directory of the process calling the CE, not to the location of the EDF file.</p>"},{"location":"software/container-engine/#known-issues","title":"Known Issues","text":""},{"location":"software/container-engine/#compatibility-with-alpine-linux","title":"Compatibility with Alpine Linux","text":"<p>Alpine Linux is incompatible with some hooks, causing errors when used with Slurm. For example,</p> <pre><code>&gt; cat alpine.toml\nimage = \"alpine: *19\"\n&gt; srun -lN1 --environment=alpine.toml echo \"abc\"\n0: slurmstepd: error: pyxis: container start failed with error code: 1\n0: slurmstepd: error: pyxis: printing enroot log file:\n0: slurmstepd: error: pyxis:     [ERROR] Failed to refresh the dynamic linker cache\n0: slurmstepd: error: pyxis:     [ERROR] /etc/enroot/hooks.d/87-slurm.sh exited with return code 1\n0: slurmstepd: error: pyxis: couldn't start container\n0: slurmstepd: error: spank: required plugin spank_pyxis.so: task_init() failed with rc=-1\n0: slurmstepd: error: Failed to invoke spank plugin stack\n</code></pre> <p>This is because some hooks (e.g., Slurm and CXI hooks) leverage <code>ldconfig</code> (from Glibc) when they bind-mount host libraries inside containers; since Alpine Linux provides an alternative\u00a0<code>ldconfig</code> (from Musl Libc), it does not work as intended by hooks. As a workaround, users may disable problematic hooks. For example,</p> <pre><code>&gt; cat alpine_workaround.toml\nimage = \"alpine: *19\"\n[annotations]\ncom.hooks.slurm.enabled = \"false\"\ncom.hooks.cxi.enabled = \"false\"\n&gt; srun -lN1 --environment=alpine_workaround.toml echo \"abc\"\nabc\n</code></pre> <p>Notice the section <code>[annotations]</code> disabling Slurm and CXI hooks.</p>"},{"location":"software/uenv/","title":"uenv","text":""},{"location":"software/uenv/#uenv","title":"uenv","text":"<p>Uenv are user environments that provide scientific applications, libraries and tools. This page will explain how to find, dowload and use uenv on the command line, and how to enable them in SLURM jobs.</p> <p>Uenv are typically application-specific, domain-specific or tool-specific - each uenv contains only what is required for the application or tools that it provides.</p> <p>Each uenv is packaged in a single file (in the Squashfs file format), that stores a compressed directory tree that contains all of the software, tools and other information like modules, required to provide a rich environment.</p> <p>Each environment contains a software stack, comprised of compilers, libraries, tools and scientific applications - built using Spack.</p> <p>uenv on Eiger and Balfrin</p> <p>The uenv tool available on Eiger and Balfrin is a different version than the one described below. Some commands will be different. Please refer to <code>uenv --help</code> for the correct usage.</p>"},{"location":"software/uenv/#getting-started","title":"Getting started","text":"<p>After logging into an Alps cluster, you can quickly check the availability of uenv with the following commands:</p> <pre><code>$ uenv status\nthere is no uenv loaded\n$ uenv --version\n7.0.0\n</code></pre>"},{"location":"software/uenv/#uenv-labels","title":"uenv Labels","text":"<p>Uenv are referred to using labels, where a label has the following form <code>name/version:tag@system%uarch</code>, for example <code>prgenv-gnu/24.11:v2@todi%gh200</code>.</p>"},{"location":"software/uenv/#name","title":"<code>name</code>","text":"<p>the name of the uenv. In this case <code>prgenv-gnu</code>.</p>"},{"location":"software/uenv/#version","title":"<code>version</code>","text":"<p>The version of the uenv. The format of <code>version</code> depends on the specific uenv. Often they use the <code>yy.mm</code> format, though they may also use the version of the software being packaged. For example the <code>namd/3.0.1</code> uenv packages version 3.0.1 of the popular NAMD simulation tool.</p>"},{"location":"software/uenv/#tag","title":"<code>tag</code>","text":"<p>Used to differentiate between releases of a versioned uenv. Some examples of tags include:</p> <ul> <li><code>rc1</code>, <code>rc2</code>: release candidates.</li> <li><code>v1</code>: a first release typically made after some release candidates.</li> <li><code>v2</code>: a second release, that might fix issues in the first release.</li> </ul>"},{"location":"software/uenv/#system","title":"<code>system</code>","text":"<p>The name of the Alps cluster for which the uenv was built.</p> <p></p>"},{"location":"software/uenv/#uarch","title":"<code>uarch</code>","text":"<p>The node type (microarchitecture) that the uenv is built for:</p> uarch CPU GPU comment gh200 4 72-core NVIDIA Grace (<code>aarch64</code>) 4 NVIDIA H100 GPUs zen2 2 64-core AMD Rome (<code>zen2</code>) - used in Eiger a100 1 64-core AMD Milan (<code>zen3</code>) 4 NVIDIA A100 GPUs mi200 1 64-core AMD Milan (<code>zen3</code>) 4 AMD Mi250x GPUs zen3 2 64-core AMD Milan (<code>zen3</code>) - only in MCH system"},{"location":"software/uenv/#using-labels","title":"Using labels","text":"<p>The uenv command line has a flexible interface for filtering uenv by providing only part of the full label:</p> <pre><code># search for all uenv on the current system that have the name prgenv-gnu\nuenv image find prgenv-gnu\n\n# search for all uenv with version 24.11\nuenv image find /24.11\n\n# search for all uenv with tag v1\nuenv image find :v1\n\n# search for a specific version\nuenv image find prgenv-gnu/24.11:v1\n</code></pre> <p>By default, the <code>uenv</code> filters results to uenv that were built on the current cluster. The name of the current cluster is always available via the <code>CLUSTER_NAME</code> environment variable.</p> <pre><code># log into the eiger vCluster\nssh eiger\n\n# this command will search for all prgenv-gnu uenv on _eiger_\nuenv image find prgenv-gnu\n\n# use @ to search on a specific system, e.g. on daint:\nuenv image find prgenv-gnu@daint\n\n# this can be used to search for all uenv on daint:\nuenv image find @daint\n\n# the '*' is a wildcard used meaning \"all systems\"\n# this will show all images on all systems\n# NOTE: the * character must be quoted in single quotes\nuenv image find @'*'\n\n# search for all images on Alps that were built for gh200 nodes.\nuenv image find @'*'%gh200\n</code></pre> <p>Note</p> <p>The wild card <code>*</code> used for \"all systems\" must always be escaped in single quotes: <code>@'*'</code>.</p>"},{"location":"software/uenv/#finding-uenv","title":"Finding uenv","text":"<p>Uenv for programming environments, tools and applications are provided by CSCS on each Alps system.</p> <p>Info</p> <p>The same uenv are not installed on every system. Instead uenv that are supported for the users of that platform are provided.</p> <p>The available uenv images are stored in a registry, that can be queried using the <code>uenv image find</code>  command:</p> <p>uenv image find</p> <pre><code>$ uenv image find\nuenv                       arch  system  id                size(MB)  date\ncp2k/2024.1:v1             zen2  eiger   2a56f1df31a4c196   2,693    2024-07-01\ncp2k/2024.2:v1             zen2  eiger   f83e95328d654c0f   2,739    2024-08-23\ncp2k/2024.3:v1             zen2  eiger   7c7369b64b5fabe5   2,740    2024-09-18\neditors/24.7:rc1           zen2  eiger   e5fb284962908eed   1,030    2024-07-18\neditors/24.7:v2            zen2  eiger   4f0f2770616135b1   1,062    2024-09-04\njulia/24.9:v1              zen2  eiger   0ff97a74dfcaa44e     539    2024-11-09\nlinalg/24.11:rc1           zen2  eiger   b69f4664bf0cd1c4     770    2024-11-20\nlinalg/24.11:v1            zen2  eiger   c11f6c85028abf5b     776    2024-12-03\nlinalg-complex/24.11:v1    zen2  eiger   846a04b4713d469b     792    2024-12-03\nlinaro-forge/24.0.2:v1     zen2  eiger   65734ce35494a5f5     313    2024-07-18\nlinaro-forge/24.1:v1       zen2  eiger   b65d7c85adfb317a     344    2024-11-27\nnetcdf-tools/2024:v1       zen2  eiger   e7e508c34cf40ccd   3,706    2024-11-14\nprgenv-gnu/24.11:rc4       zen2  eiger   811469b00f030493     570    2024-11-21\nprgenv-gnu/24.11:v1        zen2  eiger   0b6ab5fc4907bb38     572    2024-11-27\nprgenv-gnu/24.7:v1         zen2  eiger   7f68f4c8099de257     478    2024-07-01\nquantumespresso/v7.3.1:v1  zen2  eiger   61d1f21881a65578     864    2024-11-08\n</code></pre> <p>The output above shows that there are 12 uenv (<code>prgenv-gnu</code>, <code>namd</code> , <code>cp2k</code> and <code>arbor</code>).</p>"},{"location":"software/uenv/#downloading-uenv","title":"Downloading uenv","text":"<p>Note</p> <p>In order to pull uenv images, a local directory for storing the images must first be created, otherwise you will receive an error message that the repository does not exist.</p> <p>To create a repo in the default location, use the following command:</p> Create default uenv image repository<pre><code>uenv repo create\n</code></pre> <p>To use a uenv, it first has to be pulled from the registry to local storage where you can access it. For example, to use the <code>prgenv-gnu</code> uenv, use the uenv image pull command:</p> <p>uenv image pull</p> <pre><code># The following commands have the same effect\n\n# method 1: pull using the name of the uenv\nuenv image pull prgenv-gnu/24.2:v1\n\n# method 2: pull using the id of the image\nuenv image pull 3ea1945046d884ee\n</code></pre> <p>Some images can be large, over 10 GB, and it can take a while to download them from the registry.</p> <p>To view all uenv that have been pulled, and are ready to use use the <code>uenv image ls</code> command:</p> <p>listing downloaded uenv</p> <pre><code>$ uenv image ls\nuenv                           arch   system  id                size(MB)  date\neditors/24.7:v2                gh200  daint   e7b0d930df729da5   1,270    2024-09-04\ngromacs/2024:v1                gh200  daint   b58e6406810279d5   3,658    2024-09-12\njulia/24.9:v1                  gh200  daint   7a4269abfdadc046   3,939    2024-11-09\nlinalg/24.11:v1                gh200  daint   e1640cf6aafdca01   4,461    2024-12-03\nlinaro-forge/23.1.2:v1         gh200  daint   fd67b726a90318d6     341    2024-08-26\nnamd/3.0:v3                    gh200  daint   49bc65c6905eb5da   4,028    2024-12-12\nnetcdf-tools/2024:v1           gh200  daint   2a799e99a12b7c13   1,260    2024-09-04\nprgenv-gnu/24.11:v1            gh200  daint   b81fd6ba25e88782   4,191    2024-11-27\nprgenv-gnu/24.7:v3             gh200  daint   b50ca0d101456970   3,859    2024-08-23\nprgenv-nvfortran/24.11:v1      gh200  daint   d2afc254383cef20   8,703    2025-01-30\n</code></pre>"},{"location":"software/uenv/#accessing-restricted-software","title":"Accessing restricted software","text":"<p>By default, uenv can be pulled by all users on a system, with no restrictions.</p> <p>Some uenv are not available to all users, for example the <code>vasp</code> images are only available for users with a VASP license, who are added to the <code>vasp</code> group once then have provided CSCS with a copy of their license.</p> <p>To be able to pull such images a token that authorizes access must be provided. Tokens are created by CSCS, and stored on SCRATCH in a file that only users who have access to the software can read.</p> <p>using a token to access VASP</p> <pre><code>uenv image pull \\\n    --token=/capstor/scratch/cscs/bcumming/tokens/vasp6 \\\n    --username=vasp6 \\\n    vasp/v6.4.3:v1\n</code></pre> <p>Note</p> <p>As of March 2025, the only restricted software is VASP.</p> <p>Note</p> <p>Better token management is under development - tokens will be stored in a central location and will be easier to use.</p> <p></p>"},{"location":"software/uenv/#starting-a-uenv-session","title":"Starting a uenv session","text":"<p>The <code>uenv start</code> command will start a new shell with one or more uenv images mounted. This is very useful for interactive sessions, for example if you want to work in the terminal to compile an application, or set up a Python virtual environment.</p> <p>start an interactive shell to compile an application</p> <p>Here we want to compile an MPI + CUDA application \"affinity\".</p> <pre><code># start the prgenv-gnu uenv, which provides MPI, cuda and CMake\n# use the \"default\" view, which will load all of the software in the uenv\n$ uenv start prgenv-gnu/24.11:v1 --view=default\n\n# clone the software and set up the build directory\n$ git clone https://github.com/bcumming/affinity.git\n$ mkdir -p affinity/build\n$ cd affinity/build/\n\n# configure the build with CMake, then call make to build\n# mpicc, mpic++ and cmake are all provided by the uenv\n$ CXX=mpic++ CC=mpicc cmake ..\n$ make -j\n\n# run the affinity executable on two nodes - note how the uenv is\n# automatically loaded by slurm on the compute nodes, because CUDA and MPI from\n# the uenv are required to run.\n$ srun -n2 -N2 ./affinity.cuda\nGPU affinity test for 2 MPI ranks\nrank      0 @ nid005636\n cores   : 0-287\n gpu   0 : GPU-13a62579-bf3c-fb6b-667f-f2c588f4667b\n gpu   1 : GPU-74968c03-7401-9013-0590-8445b3623208\n gpu   2 : GPU-dfbd9ec1-a4b7-4a8d-603e-ebcc360f55a3\n gpu   3 : GPU-6a44522d-bf84-9864-decf-6d3e85078442\nrank      1 @ nid006322\n cores   : 0-287\n gpu   0 : GPU-6d96b1d5-69e9-7bd4-f59a-a37ec1f5da1c\n gpu   1 : GPU-c0508d69-a357-934e-87a0-be04adf4eee9\n gpu   2 : GPU-02a7fd85-ff41-1d81-d010-d7a85f6134d8\n gpu   3 : GPU-e07d996e-4d67-c9f4-cf75-81cfd45a1ae1\n\n# finish the uenv session\n$ exit\n</code></pre> <p>which shell is used</p> <p><code>uenv start</code> starts a new shell, and by default it will use the default shell for the user. You can see the default shell by looking at the <code>$SHELL</code> environment variable. If you want to force a different shell: <pre><code>SHELL=`which zsh` uenv start ...\n</code></pre></p> <p>C Shell / tcsh users</p> <p>uenv is tested extensively with bash (the default shell), and zsh. C shell is not tested properly, and we will not make significant changes to uenv to maintain support for C shell.</p> <p>If your are one of the handful of users using <code>tcsh</code> (C shell) and you want to use uenv, we strongly recommend creating a request at the CSCS service desk to change to either bash or zsh as your default.</p> <p>Failed to unshare the mount namespace</p> <p>If you get the following error message when starting a uenv: <pre><code>$ uenv start linalg/24.11:v1\nsquashfs-mount: Failed to unshare the mount namespace: Operation not permitted\n</code></pre> you most likely already have a uenv mounted. The <code>uenv status</code> command will report that you have a uenv loaded if that is the case: <pre><code>$ uenv status\nprgenv-gnu:/user-environment\n  GNU Compiler toolchain with cray-mpich, Python, CMake and other development tools.\n  views:\n    spack: configure spack upstream\n    modules: activate modules\n    default:\n</code></pre> Unload the active uenv by exiting the current shell before loading the new uenv.</p> <p>The basic syntax of uenv start is <code>uenv start image</code> where <code>image</code> is the uenv to start. The image can be a label, the hash/id of the uenv, or a file:</p> <p>uenv start</p> <pre><code># start the image using the name of the uenv\n$ uenv start netcdf-tools/2024:v1\n\n# or use the unqique id of the uenv\n$ uenv start 499c886f2947538e\n\n# or provide the path to a squashfs file\n$ uenv start $SCRATCH/my-uenv/gromacs.squashfs\n</code></pre> what does 'uenv start' actually do? <p>uenv are squashfs images, which are a compressed file that contains a directory tree. The squashfs image of a uenv is a directory that contains all of the software provided by the uenv, along with useful meta data. When you run <code>uenv start</code> (or <code>uenv run</code>, or use the <code>--uenv</code> flag with SLURM) the squashfs file is mounted at the mount location for the uenv, which is most often <code>/user-environment</code>.</p> <pre><code># log into daint\n$ ssh daint.alps.cscs.ch\n\n# /user-environment is empty\n$ ls -l /user-environment\ntotal 0\n\n# start a uenv\n$ uenv start prgenv-nvfortran/24.11:v1\n\n# the uenv software is now available\n$ ls /user-environment/\nbin  config  env  linux-sles15-neoverse_v2  meta  modules  repo\n\n# findmnt verifies that a squashfs image has been mounted\n$ findmnt /user-environment\nTARGET            SOURCE      FSTYPE   OPTIONS\n/user-environment /dev/loop25 squashfs ro,nosuid,nodev,relatime,errors=continue\n\n# end the session and verify that the uenv is not longer mounted\n$ exit\n$ ls -l /user-environment\ntotal 0\n</code></pre> <p>Loading an environment has no impact on other users or other terminal sessions that you have open on the same node \u2013 the mounted environment is only visible in your terminal. This means that multiple users on a login node can mount their own environment at the same mount point, without interfering with one-another.</p>"},{"location":"software/uenv/#views","title":"Views","text":"<p>Running <code>uenv start $label</code> on its own will create a shell with the software at <code>/user-environment</code> or <code>/user-tools</code>, however no changes are made to environment variables like <code>$PATH</code>.</p> <p>Uenv images provide views, which will set environment variables that load the software into your environment. Views are loaded using the <code>--view</code> flag for <code>uenv start</code> (also for <code>uenv run</code> and the SLURM plugin, documented below)</p> <p>loading views</p> <pre><code># activate the view named default in prgenv-gnu\n$ uenv start --view=default prgenv-gnu/24.11:v1\n\n# activate both the spack and modules views in prgenv-gnu using\n# a comma-separated list of view names\n$ uenv start --view=spack,modules prgenv-gnu/24.11:v1\n\n# when starting multiple uenv, you can disambiguate using uenvname:viewname\n$ uenv start --view=prgenv-gnu:default,editors:ed prgenv-gnu/24.11:v1,editors\n</code></pre>"},{"location":"software/uenv/#modules","title":"Modules","text":"<p>Most uenv provide the modules, that can be accessed using the <code>module</code> command. By default, the modules are not activated when a uenv is started, and need to be explicitly activated using the <code>module</code> view.</p> <p>using the module view</p> <pre><code>$ uenv start prgenv-gnu/24.11:v1 --view=modules\n$ module avail\n---------------------------- /user-environment/modules ----------------------------\n   aws-ofi-nccl/git.v1.9.2-aws_1.9.2    lua/5.4.6\n   boost/1.86.0                         lz4/1.10.0\n   cmake/3.30.5                         meson/1.5.1\n   cray-mpich/8.1.30                    nccl-tests/2.13.6\n   cuda/12.6.2                          nccl/2.22.3-1\n   fftw/3.3.10                          netlib-scalapack/2.2.0\n   fmt/11.0.2                           ninja/1.12.1\n   gcc/13.3.0                           openblas/0.3.28\n   gsl/2.8                              osu-micro-benchmarks/5.9\n   hdf5/1.14.5                          papi/7.1.0\n   kokkos-kernels/4.4.01                python/3.12.5\n   kokkos-tools/develop                 superlu/5.3.0\n   kokkos/4.4.01                        zlib-ng/2.2.1\n   libtree/3.1.1\n$ module load cuda gcc cmake\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCuda compilation tools, release 12.6, V12.6.77\n$ gcc --version\ngcc (Spack GCC) 13.3.0\n$ cmake --version\ncmake version 3.30.5\n</code></pre>"},{"location":"software/uenv/#spack","title":"Spack","text":"<p>uenv images provide a full upstream Spack configuration to facilitate building your own software with Spack using the packages installed inside as dependencies. No view needs to be loaded to use Spack, however all uenv provide a <code>spack</code> view that sets some environment variables that contain useful information like the location of the Spack configuration, and the version of Spack that was used to build the uenv. For more information, see our guide on building software with Spack and uenv.</p> <p></p>"},{"location":"software/uenv/#running-a-uenv","title":"Running a uenv","text":"<p>The <code>uenv run</code> command can be used to run an application or script in a uenv environment, and return control to the calling shell when the command has finished running.</p> how is <code>uenv run</code> different from <code>uenv start</code>? <p><code>uenv start</code> sets up the uenv environment, then starts an interactive shell in that environment. When you are finished, you can type <code>exit</code> to finish the session.</p> <p><code>uenv run</code> is more generic - instead of running a shell in environment, it takes the executable and arguments to run in the shell. The following commands are equivalent:</p> <pre><code># start a new bash shell in prgenv-gnu\nuenv start prgenv-gnu/24.11\n# start a new bash shell in prgenv-gnu\nuenv run prgenv-gnu/24.11 -- bash\n</code></pre> <p>running cmake</p> <p>Call <code>cmake</code> to configure a build with the <code>default</code> view loaded <pre><code># run a command\n$ uenv run prgenv-gnu/24.11:v1 --view=default -- cmake -DUSE_GPU=cuda ..\n</code></pre></p> <p>running an application executable</p> <p>Run the GROMACS executable from inside the <code>gromacs</code> uenv. <pre><code># run an executable:\n$ uenv run --view=gromacs gromacs/2024:v1 -- gmx_mpi\n</code></pre></p> <p>running applications with different environments</p> <p><code>uenv run</code> is useful for running multiple applications or scripts in a pipeline or workflow, where each application has separate requirements. In this example the pre and post processing stages use <code>prgenv-gnu</code>, while the simulation stage uses the <code>gromacs</code> uenv. <pre><code># run multiple applications, one after the other, that have different requirements\n$ uenv run --view=default prgenv-gnu/24.11:v1 -- ./pre-processing-script.sh\n$ uenv run --view=gromacs gromacs/2024:v1 -- gmx_mpi $gromacs_args\n$ uenv run --view=default prgenv-gnu/24.11:v1 -- ./post-processing-script.sh\n</code></pre></p>"},{"location":"software/uenv/#building-uenv","title":"Building uenv","text":"<p>CSCS provides a build service for uenv that takes as its input a uenv recipe, and builds the uenv using the same pipeline used to build the officially supported uenv.</p> <p>The command takes two arguments:</p> <ul> <li><code>recipe</code>: the path to the recipe<ul> <li>A uenv recipe is a description of the software to build in the uenv.   See the stackinator documentation for more information.</li> </ul> </li> <li><code>label</code>: the label to attach, of the form <code>name/version@system%uarch</code> where:<ul> <li><code>name</code> is the name, e.g. <code>prgenv-gnu</code>, <code>gromacs</code>, <code>vistools</code>.</li> <li><code>version</code> is a version string, e.g. <code>24.11</code>, <code>v1.2</code>, <code>2025-rc2</code></li> <li><code>system</code> is the CSCS cluster to build on (e.g. <code>daint</code>, <code>santis</code>, <code>clariden</code>, <code>eiger</code>)</li> <li><code>uarch</code> is the micro-architecture.</li> </ul> </li> </ul> <p>building a uenv</p> <p>Call the  <pre><code>uenv build $SCRATCH/recipes/myapp myapp/v3@daint%gh200\n</code></pre></p> <p>The image will be built on <code>daint</code>. The build tool gives you a url to a status page, that shows the progress of the build. After a successful build, the uenv can be pulled: <pre><code>uenv image pull service::myapp/v3:1669479716\n</code></pre></p> <p>Note that the image is given a unique numeric tag, that you can find on the status page for the build.</p> <p>Info</p> <p>To use an existing uenv recipe as the starting point for a custom recipe, <code>uenv start</code> the uenv and take the contents of the <code>meta/recipe</code> path in the mounted image (this is the recipe that was used to build the uenv).</p> <p>All uenv built by <code>uenv build</code> are pushed into the <code>service</code> namespace, where they can be accessed by all users logged in to CSCS. This makes it easy to share your uenv with other users, by giving them the name, version and tag of the image.</p> <p>Warning</p> <p>If, for whatever reason, your uenv can not be made publicly available, do not use the build service.</p> <p>search user-built uenv</p> <p>To view all of the uenv on daint that have been built by the service: <pre><code>uenv image find service::@daint\n</code></pre></p> <p></p>"},{"location":"software/uenv/#slurm-integration","title":"SLURM integration","text":"<p>The environment to load can be provided directly to SLURM via three arguments:</p> <ul> <li><code>--uenv</code>:  a comma-separated list of uenv to mount</li> <li><code>--view</code>:  a comma-separated list of views to load</li> <li><code>--repo</code>:  an alternative (if not set, the default repo in <code>$SCRATCH/.uenv-images</code> is used)</li> </ul> <p>For example, the flags can be used with srun : <pre><code># mount the uenv prgenv-gnu with the view named default\n&gt; srun --uenv=prgenv-gnu/24.7:v3 --view=default ...\n\n# mount an image at an explicit location (/user-tools)\n&gt; srun --uenv=$IMAGES/myenv.squashfs:/user-tools ...\n\n# mount multiple images: use a comma to separate the different options\n&gt; srun --uenv=prgenv-gnu/24.7:v3,editors/24.7:v2 --view=default,editors:modules ...\n</code></pre></p> <p>The commands can also be used in sbatch scripts to have fine-grained control:</p> <p>sbatch script for uenv</p> <p>It is possible to provide a uenv that is loaded inside the script, and will be loaded by default by all srun commands that do not override it with their own <code>--uenv</code> parameters. <pre><code>#!/bin/bash\n\n#SBATCH --uenv=editors/24.7:v2\n#SBATCH --view=editors:ed\n#SBATCH --ntasks=4\n#SBATCH --nodes=1\n#SBATCH --output=out-%j.out\n#SBATCH --error=out-%j.out\n\necho \"==== test in script ====\"\n# the fd command is provided by the ed view\n# use it to inspect the meta data in the mounted image\nfd . /user-tools/meta/recipe\n\necho \"==== test in srun ====\"\n# use srun to launch the parallel job\nsrun -n4 bash -c 'echo $SLURM_PROCID on $(hostname): $(which emacs)'\n\necho \"==== alternative mount ====\"\nsrun -n4 --uenv=prgenv-gnu --view=prgenv-gnu:default bash -c 'echo $SLURM_PROCID on $(hostname): $(which mpicc)'\nsbatch output\n</code></pre></p> <p>The sbatch job above would generate output like the following: <pre><code>==== test in script ====\n/user-tools/meta/recipe/compilers.yaml\n/user-tools/meta/recipe/config.yaml\n/user-tools/meta/recipe/environments.yaml\n/user-tools/meta/recipe/modules.yaml\n==== test in srun ====\n1 on nid007144: /user-tools/env/ed/bin/emacs\n3 on nid007144: /user-tools/env/ed/bin/emacs\n0 on nid007144: /user-tools/env/ed/bin/emacs\n2 on nid007144: /user-tools/env/ed/bin/emacs\n==== alternative mount ====\n0 on nid007144: /user-environment/env/default/bin/mpicc\n1 on nid007144: /user-environment/env/default/bin/mpicc\n2 on nid007144: /user-environment/env/default/bin/mpicc\n3 on nid007144: /user-environment/env/default/bin/mpicc\n</code></pre></p> <p>In the example above, the <code>#SBATCH --uenv</code>  and <code>#SBATCH --view</code>  parameters in the preamble of the sbatch script set the default uenv to <code>editors</code> with the view <code>ed</code>.</p> <ul> <li><code>editors</code> is mounted and the view set in the script (the \"test in script\" part)</li> <li><code>editors</code> is also mounted in the first call to srun (which does not provide a <code>`\u2013-uenv</code> flag)</li> </ul> <p>it is possible to override the default uenv by passing a different <code>--uenv</code>  and <code>--view</code>  flags to an <code>srun</code>  call inside the script, as is done in the second <code>srun</code>  call.</p> <ul> <li>Note how the second call has access to <code>mpicc</code>, provided by <code>prgenv-gnu</code>.</li> </ul> <p></p>"},{"location":"software/uenv/#installing-the-uenv-tool","title":"Installing the uenv tool","text":"<p>The command line tool can be installed from source, if you are working on a cluster that does not have uenv installed, or if you need to test a new version.</p> <p>Note</p> <p>uenv is installed already on CSCS clusters, so installation is not required.</p> <p>Only follow these steps if you are advised to test out a new version (e.g. if it has a fix for an issues that you are encountering).</p> <pre><code>git clone https://github.com/eth-cscs/uenv2.git\ncd uenv2\n\n./install-alps-local.sh # (1)!\n\n# update bashrc\necho \"export PATH=\\$HOME/.local/\\$(uname -m)/bin:\\$PATH\" &gt;&gt; $HOME/.bashrc \necho \"unset -f uenv\" &gt;&gt; $HOME/.bashrc\n</code></pre> <ol> <li>Run installation script. This will install uenv in <code>$HOME/.local/$(uname -m)/bin/</code>.</li> </ol> <p>Warning</p> <p>Before uenv can be used, you need to log out then back in again and type <code>which uenv</code> to verify that uenv has been installed in your <code>$HOME</code> path.</p>"},{"location":"software/communication/","title":"Index","text":""},{"location":"software/communication/#communication-libraries","title":"Communication Libraries","text":"<p>CSCS provides common communication libraries optimized for the Slingshot 11 network on Alps.</p> <p>For most scientific applications relying on MPI, Cray MPICH is recommended. OpenMPI may also be used, with limitations. Both Cray MPICH and OpenMPI make use of libfabric to interact with the underlying network.</p> <p>Most machine learning applications rely on NCCL or RCCL for high-performance implementations of collectives. NCCL and RCCL have to be configured with a plugin using libfabric to make full use of the Slingshot network.</p> <p>See the individual pages for each library for information on how to use and best configure the libraries.</p> <ul> <li>Cray MPICH</li> <li>OpenMPI</li> <li>NCCL</li> <li>RCCL</li> <li>libfabric</li> </ul>"},{"location":"software/communication/cray-mpich/","title":"Cray MPICH","text":""},{"location":"software/communication/cray-mpich/#cray-mpich","title":"Cray MPICH","text":"<p>Cray MPICH is the recommended MPI implementation on Alps. It is available through uenvs like prgenv-gnu and the application-specific uenvs.</p> <p>The Cray MPICH documentation contains detailed information about Cray MPICH. On this page we outline the most common workflows and issues that you may encounter on Alps.</p> <p></p>"},{"location":"software/communication/cray-mpich/#gpu-aware-mpi","title":"GPU-aware MPI","text":"<p>We recommend using GPU-aware MPI whenever possible, as it almost always provides a significant performance improvement compared to communication through CPU memory. To use GPU-aware MPI with Cray MPICH</p> <ol> <li>the application must be linked to the GTL library, and</li> <li>the <code>MPICH_GPU_SUPPORT_ENABLED=1</code> environment variable must be set.</li> </ol> <p>If either of these are missing, the application will fail to communicate GPU buffers.</p> <p>In supported uenvs, Cray MPICH is built with GPU support (on clusters that have GPUs). This means that Cray MPICH will automatically be linked to the GTL library, which implements the the GPU support for Cray MPICH.</p> Checking that the application links to the GTL library <p>To check if your application is linked against the required GTL library, running <code>ldd</code> on your executable <code>myexecutable</code> should print something similar to: <pre><code>$ ldd myexecutable | grep gtl\n        libmpi_gtl_cuda.so =&gt; /user-environment/linux-sles15-neoverse_v2/gcc-13.2.0/cray-gtl-8.1.30-fptqzc5u6t4nals5mivl75nws2fb5vcq/lib/libmpi_gtl_cuda.so (0x0000ffff82aa0000)\n</code></pre></p> <p>The path may be different, but the <code>libmpi_gtl_cuda.so</code> library should be printed when using CUDA. In ROCm environments the <code>libmpi_gtl_hsa.so</code> library should be linked. If the GTL library is not linked, nothing will be printed.</p> <p>In addition to linking to the GTL library, Cray MPICH must be configured to be GPU-aware at runtime by setting the <code>MPICH_GPU_SUPPORT_ENABLED=1</code> environment variable. On some CSCS systems this option is set by default. See this page for more information on configuring SLURM to use GPUs.</p> <p>Segmentation faults when trying to communicate GPU buffers without <code>MPICH_GPU_SUPPORT_ENABLED=1</code></p> <p>If you attempt to communicate GPU buffers through MPI without setting <code>MPICH_GPU_SUPPORT_ENABLED=1</code>, it will lead to segmentation faults, usually without any specific indication that it is the communication that fails. Make sure that the option is set if you are communicating GPU buffers through MPI.</p> <p>Error: \"<code>GPU_SUPPORT_ENABLED</code> is requested, but GTL library is not linked\"</p> <p>If <code>MPICH_GPU_SUPPORT_ENABLED</code> is set to <code>1</code> and your application does not link against one of the GTL libraries you will get an error similar to the following during MPI initialization: <pre><code>MPICH ERROR [Rank 0] [job id 410301.1] [Thu Feb 13 12:42:18 2025] [nid005414] - Abort(-1) (rank 0 in comm 0): MPIDI_CRAY_init: GPU_SUPPORT_ENABLED is requested, but GTL library is not linked\n (Other MPI error)\n\naborting job:\nMPIDI_CRAY_init: GPU_SUPPORT_ENABLED is requested, but GTL library is not linked\n</code></pre></p> <p>This means that the required GTL library was not linked to the application. In supported uenvs, GPU support is enabled by default. If you believe a uenv should have GPU support but you are getting the above error, feel free to get in touch with us to understand whether there is an issue with the uenv or something else in your environment.  If you are using Cray modules you must load the corresponding accelerator module, e.g. <code>craype-accel-nvidia90</code>, before compiling your application.</p> <p>Alternatively, if you wish to not use GPU-aware MPI, either unset <code>MPICH_GPU_SUPPORT_ENABLED</code> or explicitly set it to <code>0</code> in your launch scripts.</p>"},{"location":"software/communication/cray-mpich/#known-issues","title":"Known issues","text":"<p>This section documents known issues related to Cray MPICH on Alps. Resolved issues are also listed for reference.</p>"},{"location":"software/communication/cray-mpich/#existing-issues","title":"Existing Issues","text":""},{"location":"software/communication/cray-mpich/#cray-mpich-hangs","title":"Cray MPICH hangs","text":"<p>Cray MPICH may sometimes hang on larger runs.</p> <p>Workaround</p> <p>There are many possible reasons why an application would hang, many unrelated to Cray MPICH. However, if you are experiencing hangs the issue may be worked around by setting: <pre><code>export FI_MR_CACHE_MONITOR=disabled\n</code></pre></p> <p>Performance may be negatively affected by this option.</p>"},{"location":"software/communication/cray-mpich/#resolved-issues","title":"Resolved issues","text":""},{"location":"software/communication/cray-mpich/#cxil_map-write-error-when-doing-inter-node-gpu-aware-mpi-communication","title":"<code>\"cxil_map: write error\"</code> when doing inter-node GPU-aware MPI communication","text":"The issue has been resolved on the 7th of October 2024 with a system update <p>The issue was caused by a system misconfiguration.</p> <p>When doing inter-node GPU-aware communication with Cray MPICH after the update on the 30th of September 2024 on Alps, applications will fail with: <pre><code>cxil_map: write error\n</code></pre></p> Workaround <p>The only workaround is to not use inter-node GPU-aware MPI.</p> Workaround for CP2K <p>For users of CP2K encountering this issue, one can disable the use of COSMA, which uses GPU-aware MPI, by placing the following in the <code>&amp;GLOBAL</code> section of your input file:  <pre><code>&amp;FM\nTYPE_OF_MATRIX_MULTIPLICATION SCALAPACK\n&amp;END FM\n</code></pre></p> <p>Unless you run RPA calculations, this should have limited impact on performance.</p>"},{"location":"software/communication/cray-mpich/#mpi_thread_multiple-does-not-work","title":"<code>MPI_THREAD_MULTIPLE</code> does not work","text":"<p>The issue has been resolved in Cray MPICH version 8.1.30</p> <p>When using <code>MPI_THREAD_MULTIPLE</code> on GH200 systems Cray MPICH may fail with an assertion that looks similar to: <pre><code>Assertion failed [...]: (&amp;MPIR_THREAD_GLOBAL_ALLFUNC_MUTEX)-&gt;count == 0\n</code></pre></p> <p>or</p> <pre><code>Assertion failed [...]: MPIR_THREAD_GLOBAL_ALLFUNC_MUTEX.count == 0\n</code></pre> Workaround <p>The issue can be worked around by falling back to a less optimized implementation of <code>MPICH_THREAD_MULTIPLE</code> by setting <code>MPICH_OPT_THREAD_SYNC=0</code>.</p>"},{"location":"software/communication/libfabric/","title":"libfabric","text":""},{"location":"software/communication/libfabric/#libfabric","title":"Libfabric","text":"<p>Libfabric, or Open Fabrics Interfaces (OFI), is a low level networking library that abstracts away various networking backends. It is used by Cray MPICH, and can be used together with OpenMPI, NCCL, and RCCL to make use of the Slingshot network on Alps.</p> <p>Todo</p>"},{"location":"software/communication/nccl/","title":"NCCL","text":""},{"location":"software/communication/nccl/#nccl","title":"NCCL","text":"<p>NCCL is an optimized inter-GPU communication library for NVIDIA GPUs. It is commonly used in machine learning frameworks, but traditional scientific applications can also benefit from NCCL.</p> <p>Todo</p> <ul> <li>high level description</li> <li>libfabric/aws-ofi-nccl plugin</li> <li>configuration options</li> </ul>"},{"location":"software/communication/openmpi/","title":"OpenMPI","text":""},{"location":"software/communication/openmpi/#openmpi","title":"OpenMPI","text":"<p>Cray MPICH is the recommended MPI implementation on Alps. However, OpenMPI can be used as an alternative in some cases, with limited support from CSCS.</p> <p>To use OpenMPI on Alps, it must be built against libfabric with support for the Slingshot 11 network.</p> <p>Todo</p> <p>Building OpenMPI for Alps is still work in progress: https://eth-cscs.github.io/cray-network-stack/.</p>"},{"location":"software/communication/rccl/","title":"RCCL","text":""},{"location":"software/communication/rccl/#rccl","title":"RCCL","text":"<p>RCCL is an optimized inter-GPU communication library for AMD GPUs. It provides equivalent functionality to NCCL for AMD GPUs.</p> <p>Todo</p> <ul> <li>high level description</li> <li>libfabric/aws-ofi-rccl plugin</li> <li>configuration options</li> </ul>"},{"location":"software/devtools/","title":"Index","text":""},{"location":"software/devtools/#debugging-and-performance-analysis-tools","title":"Debugging and Performance Analysis tools","text":"<p>Debugging and performance analysis tools can assist users in developing and optimizing scientific parallel applications, especially in a high-performance computing (HPC) environment. These tools can significantly improve workflows and save valuable computational resources.</p> <p>CSCS provides debuggers and performance analysis tools on Alps clusters.</p> <p>Get in touch</p> <p>If you have issues or questions about debugging or performance analysis tools, please do not hesitate to contact us.</p> <p></p>"},{"location":"software/devtools/#debugging","title":"Debugging","text":"<p>Parallel debugging tools designed for parallel and distributed applications can help you diagnose issues and verify the correctness of your code - whether you are using MPI, OpenMP, or accelerated programming models.</p> <p>Learning to debug a code effectively will not only help you quickly resolve issues but also build a deeper understanding of how your code interacts with the underlying hardware. In this section we introduce the various debugging tools available at CSCS.</p> <ul> <li>Linaro Forge DDT</li> </ul> <p></p>"},{"location":"software/devtools/#performance-analysis","title":"Performance Analysis","text":"<p>Performance analysis tools are essential to gain insight into how an application leverages a distributed system with CPUs and GPUs and should be integrated to the development and optimization workflow of the application. This ensures that computational resources are utilized to their fullest potential.</p> <p>Learning to analyze the performance of an applications effectively is crucial to build a deeper understanding of how your code interacts with the underlying hardware. In this section we introduce the various performance analysis solutions available at CSCS.</p> <ul> <li>Linaro Forge MAP</li> <li>NVIDIA Nsight Developer Tools</li> </ul>"},{"location":"software/devtools/linaro-ddt/","title":"Using Linaro debugger","text":""},{"location":"software/devtools/linaro-ddt/#linaro-ddt","title":"Linaro DDT","text":"<p>DDT allows source-level debugging of Fortran, C, C++ and Python codes. It can be used for debugging serial, multi-threaded (OpenMP), multi-process (MPI), and accelerated (CUDA, OpenACC) programs running on research and production systems, including the CSCS Alps system. DDT can be executed either with its graphical user interface or from the command-line.</p> <p>Note</p> <p>Linaro DDT is provided in the <code>linaro-forge</code> uenv. Before using DDT, please read the <code>linaro-forge</code> uenv documentation, which explains how to download and set up the latest version.</p>"},{"location":"software/devtools/linaro-ddt/#user-guide","title":"User guide","text":"<p>The following guide will walk through the steps required to build and debug an application using DDT.</p>"},{"location":"software/devtools/linaro-ddt/#set-up-the-user-environment-and-build-the-executable","title":"Set up the user environment and build the executable","text":"<p>Once the uenv is loaded and activated, the program to debug must be compiled with the <code>-g</code> (for CPU) and <code>-G</code> (for GPU) debugging flags. For example, we can build a CUDA test with a user environment:</p> <pre><code>uenv start prgenv-gnu:24.11:v1 --view=default\nnvcc -c -arch=sm_90 -g -G test_gpu.cu\nmpicxx -g test_cpu.cpp test_gpu.o -o myexe\n</code></pre>"},{"location":"software/devtools/linaro-ddt/#launch-linaro-ddt","title":"Launch Linaro DDT","text":"<p>To use the DDT client with uenv, it must be launched in <code>Manual Launch</code> mode (assuming that it is connected to Alps via <code>Remote Launch</code>):</p> On local machineOn Alps <p>Start DDT, and connect to the target cluster using the drop down menu for <code>Remote Launch</code>. If you don't have a target cluster, the <code>linaro-forge</code> uenv documentation explains how to set up the connection the first time.</p> <p>Click on <code>Manual launch</code>, set the number of processes to listen to, then wait for the Slurm job to start  (see the \"on Alps\" tab for how to start the Slurm job).</p> <p></p> <p>Log into the system and launch with the <code>srun</code> command:</p> <pre><code>$ uenv start prgenv-gnu/24.11:v1,linaro-forge/24.1.1:v1 --view=prgenv-gnu:default # (1)!\n$ source /user-tools/activate\n$ srun -N1 -n4 -t15 -pdebug ./cuda_visible_devices.sh   ddt-client   ./myexe\n</code></pre> <ol> <li>Start a session with both the uenv used to build your application and the <code>linaro-forge</code> uenv mounted.</li> </ol>"},{"location":"software/devtools/linaro-ddt/#start-debugging","title":"Start debugging","text":"<p>By default, DDT will pause execution on the call to <code>MPI_Init</code>: </p> <p>There are two mechanisms for controlling program execution:</p> BreakpointStop at <p>Breakpoint(s) can be set by clicking in the margin to the left of the line number:</p> <p></p> <p>Execution can be paused in every CUDA kernel launch by activating the default breakpoints from the <code>Control</code> menu:</p> <p></p> Debugging with 128 GPUs <p>This screenshot shows a debugging session on 128 GPUs:</p> <p></p> <p>More informations regarding how to use Linaro DDT are provided in the Forge User Guide.</p>"},{"location":"software/devtools/linaro-ddt/#troubleshooting","title":"Troubleshooting","text":"<p>See the troubleshooting guide for the <code>linaro-forge</code> uenv.</p>"},{"location":"software/devtools/linaro-map/","title":"Using Linaro performance analysis tool","text":""},{"location":"software/devtools/linaro-map/#linaro-forge-map-and-performance-reports","title":"Linaro Forge MAP and Performance Reports","text":"<p>Linaro MAP can be used for profiling serial, multi-threaded (OpenMP), multi-process (MPI) and accelerated (Cuda, OpenACC) programs running on research and production systems, including the CSCS Alps system. MAP can be executed either with its graphical user interface or from the command-line. In the first case, the user can set the profiling configuration using the GUI and then see the results. In the latter (recommended) case, the user can use the MAP executable to launch the application they want to profile which will generate a report file that can then be opened from the locally installed client.</p> <p>Note</p> <p>Linaro MAP is provided in the <code>linaro-forge</code> uenv. Before using MAP, please read the <code>linaro-forge</code> uenv documentation, which explains how to download and set up the latest version.</p>"},{"location":"software/devtools/linaro-map/#linaro-forge-map","title":"Linaro Forge MAP","text":"<p>We will focus here on the profiling using MAP from the CLI but the same configuration applies in the other case as well. To debug an MPI application on Alps the following script is necessary:</p> <pre><code>&gt; map -n &lt;num_of_procs&gt; --mpi=slurm --mpiargs=\"&lt;slurm_arguments&gt;\" \\\n  --profile &lt;executable&gt; &lt;executable_arguments&gt;\n</code></pre> <p>This will generate a profile report in a binary file with suffix <code>.map</code>.</p> <p>To open this file we can open the Linaro Forge Client on our local machine, navigate to the <code>Linaro MAP</code> tab, connect to the corresponding <code>Remote</code> and then select <code>LOAD PROFILE DATA FILE</code> to locate the file.</p> <p>After loading the report file we will be in the home of Linaro MAP.</p> <p></p>"},{"location":"software/devtools/linaro-map/#linaro-forge-performance-reports","title":"Linaro Forge Performance Reports","text":"<p>Linaro MAP also allows the generation of a high level Performance Report in HTML format that shows key metrics of the profiled application. To see this we can click in the toolbar <code>Reports &gt; View HTML Performance Report in browser</code>.</p> <p>This will look like the following:</p> <p> </p> <p>More informations regarding how to use Linaro MAP and Performance Reports are provided in the Forge User Guide.</p>"},{"location":"software/devtools/linaro-map/#troubleshooting","title":"Troubleshooting","text":"<p>See the troubleshooting guide for the <code>linaro-forge</code> uenv.</p>"},{"location":"software/devtools/linaro-uenv/","title":"Linaro uenv","text":""},{"location":"software/devtools/linaro-uenv/#linaro-forge","title":"Linaro Forge","text":"<p>Linaro Forge is a suite of profiling and debugging tools, that includes the DDT debugger and the MAP performance analysis tool.</p> <p>Linaro DDT debugger and Linaro MAP performance analysis tool</p> <p>We have separate user guides for the tools provided by the <code>linaro-forge</code> uenv. The documentation here shows how to download the uenv, and how to set up your environment.</p> <p>Once you are set up, follow the specific guides:</p> <ul> <li>DDT debugger,</li> <li>MAP performance analysis tool.</li> </ul>"},{"location":"software/devtools/linaro-uenv/#quickstart-guide","title":"Quickstart guide","text":"<p>The Linaro uenv is named <code>linaro-forge</code>, and the available versions can be determined using the <code>uenv image find</code> command, as explained in the uenv documentation.</p> Finding available <code>linaro-forge</code> versions <pre><code>$ uenv image find linaro-forge\nuenv                    arch   system  id                size(MB)  date\nlinaro-forge/24.1.1:v1  gh200  daint   e0e79f5c3e6a8ee0  365       2025-02-12\n\n$ uenv image pull linaro-forge/24.1.1:v1\npulling e0e79f5c3e6a8ee0 100.00% --- 365/365 (0.00 MB/s)\n</code></pre> <p>This uenv is configured to be mounted in the <code>/user-tools</code> path so that they can be used alongside application and development uenv mounted at <code>/user-environment</code>.</p> <p>When using alongside another uenv, start a uenv session with both uenv. In the following example, the <code>prgenv-gnu</code> and <code>linaro-forge</code> uenv will be mounted at <code>/user-environment</code> and <code>/user-tools</code>  respectively:</p> <pre><code>$ uenv start prgenv-gnu/24.11,linaro-forge/24.1.1 \\\n    --view=prgenv-gnu:default,forge\n\n$ uenv status # (1)!\n\n$ ddt --version # (2)!\nLinaro DDT Part of Linaro Forge.\nCopyright (c) 2023-2024 Linaro Limited. All rights reserved.\nVersion: 24.1.1\n</code></pre> <ol> <li>Test that everything has been mounted correctly by looking at <code>uenv status</code>.    There will be warnings if there are problems.</li> <li>Check that the DDT debugger is in the path.</li> </ol> <p>Note</p> <p>The <code>linaro-forge</code> uenv is always mounted at the <code>/user-tools</code> mount point, and a script <code>/user-tools/activate</code> is provided to load both ddt and map into your environment, without needing to use a view.</p> <pre><code>$ uenv start linaro-forge/14.1.1\n$ source /user-tools/activate\n$ ddt --version\nLinaro DDT Part of Linaro Forge.\nCopyright (c) 2023-2024 Linaro Limited. All rights reserved.\nVersion: 24.1.1\n</code></pre>"},{"location":"software/devtools/linaro-uenv/#install-and-configure-the-linaro-client-on-your-local-machine","title":"Install and configure the Linaro client on your local machine","text":"<p>We recommend installing the Linaro desktop client on your local workstation or laptop. It can be downloaded for a selection of operating systems.</p> <p>Warning</p> <p>Make sure you download the Linaro desktop client matching the version of the <code>linaro-forge</code> uenv you are planning to use.</p> <p>Mismatched desktop client and uenv versions</p> <p>Mismatches between the client and the uenv version will lead to the following error when trying to establish a remote connection:</p> <pre><code>The local version of Linaro DDT (24.0.6) is not compatible with the remote version (24.1.1).\n</code></pre> <p>The client can be configured to connect with the debug jobs running on Alps, offering a better user experience compared to running with X11 forwarding. Once installed, the client needs to be configured to connect to the vCluster on which you are working.</p> <p>First, start the client on your laptop:</p> LinuxmacOS <p>Warning</p> <p>The path will change if you have installed a different version, or if it has been installed in a non-standard installation location.</p> <pre><code>$HOME/linaro/forge/24.1.1/bin/ddt\n</code></pre> <p>Warning</p> <p>The path will change if you have installed a different version, or if it has been installed in a non-standard installation location. Please use the appropriate path and version for your installation.</p> <pre><code>open /Applications/Linaro\\ Forge\\ Client\\ 24.1.1.app/\n</code></pre> <p>Next, configure a connection to the target system. Open the <code>Remote Launch</code> menu and click on <code>Configure...</code> then <code>Add</code>. Examples of the settings are below.</p> DaintSantisClaridenEiger <p>Warning</p> <p>The <code>Remote Installation Directory</code> will change if you are using a different version of the <code>linaro-forge</code> uenv. Please use the appropriate version for your setup.</p> Field Value Connection <code>daint</code> Host Name <code>cscsusername@ela.cscs.ch cscsusername@daint.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/24.1.1:/user-tools -- /user-tools/env/forge/</code> Private Key <code>~/.ssh/cscs-key</code> <p>Warning</p> <p>The <code>Remote Installation Directory</code> will change if you are using a different version of the <code>linaro-forge</code> uenv. Please use the appropriate version for your setup.</p> Field Value Connection <code>santis</code> Host Name <code>cscsusername@ela.cscs.ch cscsusername@santis.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/24.1.1:/user-tools -- /user-tools/env/forge/</code> Private Key <code>~/.ssh/cscs-key</code> <p>Warning</p> <p>The <code>Remote Installation Directory</code> will change if you are using a different version of the <code>linaro-forge</code> uenv. Please use the appropriate version for your setup.</p> Field Value Connection <code>clariden</code> Host Name <code>cscsusername@ela.cscs.ch cscsusername@clariden.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/24.1.1:/user-tools -- /user-tools/env/forge/</code> Private Key <code>~/.ssh/cscs-key</code> <p>Warning</p> <p>The <code>Remote Installation Directory</code> will change if you are using a different version of the <code>linaro-forge</code> uenv. Please use the appropriate version for your setup.</p> Field Value Connection <code>eiger</code> Host Name <code>cscsusername@ela.cscs.ch cscsusername@eiger.cscs.ch</code> Remote Installation Directory <code>uenv run linaro-forge/24.1.1:/user-tools -- /user-tools/env/forge/</code> Private Key <code>~/.ssh/cscs-key</code> <p>Tip</p> <p>It is recommended to log into Alps using <code>ela.cscs.ch</code> as a ssh Jump host, as explained here. In that case, you can remove <code>cscsusername@ela.cscs.ch</code> from the Linaro client configuration above.</p> <p>Some notes on the examples above:</p> <ul> <li>SSH forwarding via <code>ela.cscs.ch</code> is used to access the cluster;</li> <li>replace the username <code>cscsusername</code> with your CSCS user name that you would normally use to open an SSH connection to CSCS;</li> <li><code>Remote Installation Directory</code> is pointing to the install directory of DDT inside the uenv image;</li> <li>private keys should be the ones generated for CSCS MFA, and this field does not need to be set if you have added the key to your SSH agent.</li> </ul> <p>Once configured, test and save the configuration:</p> <ol> <li>check whether the configuration is correct by clicking <code>Test Remote Launch</code> (and then <code>OK</code> when the test is successful),</li> <li>click on <code>OK</code> and then <code>Close</code> to save the configuration.</li> <li>You can now connect by going to <code>Remote Launch</code> and choose the entry (<code>Connection</code> name) you added.    If the client fails to connect, look at the error message, check your SSH    configuration and make sure you can SSH without the client.</li> </ol> <p></p>"},{"location":"software/devtools/linaro-uenv/#troubleshooting","title":"Troubleshooting","text":"<p>Notes about known issues.</p> <p>The proxy type is invalid for this operation</p> <p>If the tool fails to launch with the following error message: </p> <pre><code>Error communicating with Licence Server velan.cscs.ch:\nThe proxy type is invalid for this operation\nAttempting again while ignoring proxies.\n</code></pre> <p>Proxy environment variables need to be set to let the tool connect to the license server, as explained in Compute node proxy configuration.</p> <p>AMD GPU support</p> <p>CSCS does not currently have a Linaro license for AMD GPUs.</p>"},{"location":"software/devtools/nvidia-nsight/","title":"NVIDIA Nsight","text":""},{"location":"software/devtools/nvidia-nsight/#nvidia-nsight","title":"NVIDIA Nsight","text":""},{"location":"software/devtools/nvidia-nsight/#nvidia-nsight-systems","title":"NVIDIA Nsight Systems","text":"<p>NVIDIA Nsight Systems is a system-wide performance analysis tool that enables developers to gain a deep understanding of how their applications utilize computing resources, such as CPUs, GPUs, memory, and I/O. The tool provides a unified view of application performance across the entire system, capturing detailed trace information that allows users to analyze how different components interact and where performance issues might arise.</p> <p>A key advantage of Nsight Systems is its ability to provide detailed traces of GPU activity, offering deeper insights into GPU utilization. It features a timeline-based visualization, enabling developers to inspect the execution flow, pinpoint latencies, and correlate events across different system components. As a sampling profiler, it can be easily used to profile applications written in C, C++, Python, Fortran, or Julia by wrapping the application with the Nsight Systems profiler executable.</p> <p>Note</p> <p>NVIDIA Nsight Systems is available with any uenv that comes with a CUDA compiler, for instance <code>prgenv-gnu</code>.</p>"},{"location":"software/devtools/nvidia-nsight/#nvidia-nsight-compute","title":"NVIDIA Nsight Compute","text":"<p>NVIDIA Nsight Compute is a performance analysis tool specifically designed for optimizing GPU-accelerated applications. It focuses on providing detailed metrics and insights into the performance of CUDA kernels, helping developers identify performance bottlenecks and improve the efficiency of their GPU code. Nsight Compute offers a kernel-level profiler with customizable reports, enabling in-depth analysis of memory usage, compute utilization, and instruction throughput. As a sampling profiler, it can be easily used to profile applications written in C, C++, Python, Fortran, or Julia by wrapping the application with the Nsight Compute profiler executable.</p> <p>Note</p> <p>NVIDIA Nsight Systems is available with any uenv that comes with a CUDA compiler, for instance <code>prgenv-gnu</code>.</p>"},{"location":"software/devtools/nvidia-nsight/#known-issues-and-common-problems","title":"Known Issues and Common Problems","text":"CrashReporter: Qt initialization failed, Failed to load Qt platform plugin: xcb <p>While we recommend using <code>ncu</code> instead of <code>ncu-ui</code>, it is possible to use X11 to launch ncu-ui. However, this will fail with the following error message: <code>Failed to load Qt platform plugin: \"xcb\"</code>.</p> <p>To workaround this issue, you can follow these instructions:</p> <pre><code>ssh -Y -J &lt;user&gt;@ela.cscs.ch &lt;user&gt;@daint.alps.cscs.ch\n# the -Y ssh flag enables trusted X11 forwarding.\nwget https://jfrog.svc.cscs.ch/artifactory/cscs-reframe-tests/nvidia/ncu_deps.tar.gz\ntar xf ncu_deps.tar.gz\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$PWD/usr/lib64\nncu-ui &amp;\n</code></pre>"},{"location":"software/prgenv/","title":"Index","text":""},{"location":"software/prgenv/#programming-environments","title":"Programming Environments","text":"<p>CSCS provides \"programming environments\" on Alps vClusters that provide compilers, MPI, and commonly used libraries and packages, that can be used to build applications from source.</p> <ul> <li> <p> prgenv-gnu</p> <p>Provides compilers, MPI, tools and libraries built around the GNU compiler toolchain. It is the go to programming environment on all systems and target node types, that is it is the first that you should try out when starting to compile an application or create a python virtual environment.</p> </li> <li> <p> prgenv-nvfortran</p> <p>Provides a set of tools and libraries for building applications that need the NVIDIA Fortran compiler, commonly required for OpenACC and CUDA-Fortran applications.</p> </li> <li> <p> linalg</p> <p>Provides linear compilers, MPI and Python, along with algebra and mesh partitioning libraries for a broad range of use cases.</p> </li> </ul>"},{"location":"software/prgenv/linalg/","title":"linalg","text":""},{"location":"software/prgenv/linalg/#linalg","title":"linalg","text":"<p>The <code>linalg</code> and <code>linalg-complex</code> uenvs are similar to the <code>prgenv-gnu</code> and <code>prgenv-nvfortran</code> uenvs in that they don't provide a specific application, but common libraries useful as a base for building other applications. They contain linear algebra and mesh partitioning libraries for a broad range of use cases.</p> <p>The two uenvs contain otherwise identical packages, except that <code>linalg-complex</code> contains <code>petsc</code> and <code>trilinos</code> with complex types enabled, but without the <code>hypre</code> package. <code>hypre</code> only supports double precision. See below for the full list of packages in each version of the uenv. Note that many of the packages available in <code>linalg</code> and <code>linalg-complex</code> are also available in <code>prgenv-gnu</code>.</p>"},{"location":"software/prgenv/linalg/#versioning","title":"Versioning","text":"<p>The uenvs are available in the following versions on the following systems:</p> version node types system 24.11 gh200, zen2 daint, eiger 24.11 <p>In version 24.11, the common set of packages in both uenvs is:</p> <ul> <li>arpack-ng</li> <li>aws-ofi-nccl</li> <li>blaspp</li> <li>blt</li> <li>boost</li> <li>camp</li> <li>cmake</li> <li>cuda</li> <li>dla-future-fortran</li> <li>dla-future</li> <li>eigen</li> <li>fftw</li> <li>fmt</li> <li>gsl</li> <li>hdf5</li> <li>hwloc</li> <li>kokkos-kernels</li> <li>kokkos-tools</li> <li>kokkos</li> <li>lapackpp</li> <li>libtree</li> <li>lua</li> <li>lz4</li> <li>meson</li> <li>metis</li> <li>mimalloc</li> <li>mumps</li> <li>nccl-tests</li> <li>nccl</li> <li>nco</li> <li>netcdf-c</li> <li>netlib-scalapack</li> <li>ninja</li> <li>openblas</li> <li>osu-micro-benchmarks</li> <li>p4est</li> <li>papi</li> <li>parmetis</li> <li>petsc</li> <li>pika</li> <li>python</li> <li>slepc</li> <li>spdlog</li> <li>stdexec</li> <li>suite-sparse</li> <li>superlu-dist</li> <li>superlu</li> <li>swig</li> <li>trilinos</li> <li>umpire</li> <li>whip</li> <li>zlib-ng</li> </ul>"},{"location":"software/prgenv/linalg/#how-to-use","title":"How to use","text":"<p>Using the <code>linalg</code> and <code>linalg-complex</code> uenvs is similar to <code>prgenv-gnu</code>. Like <code>prgenv-gnu</code>, the <code>linalg</code> and <code>linalg-complex</code> uenvs provide <code>default</code> and <code>modules</code> views. Please see the <code>prgenv-gnu</code> documentation for details on different ways of accessing the packages available in the uenv. You can for example load the <code>modules</code> view to see the exact versions of the packages available in the uenv.</p>"},{"location":"software/prgenv/prgenv-gnu/","title":"prgenv-gnu","text":""},{"location":"software/prgenv/prgenv-gnu/#prgenv-gnu","title":"prgenv-gnu","text":"<p>Provides a set of tools and libraries built around the GNU compiler toolchain. It is the go to programming environment on all systems and target node types, that is it is the first that you should try out when starting to compile an application or create a python virtual environment.</p> <p>alternatives to prgenv-gnu</p> <p>The <code>prgenv-nvfortran</code> is for applications that require the NVIDIA Fortran compiler - typically because they need to use OpenACC or CUDA Fortran.</p> <p>The <code>linalg</code> environment is similar to prgenv-gnu, with additional linear algebra and mesh partitioning algorithms.</p>"},{"location":"software/prgenv/prgenv-gnu/#versioning","title":"Versioning","text":"<p>The naming scheme is <code>prgenv-gnu/&lt;version&gt;</code>, where <code>&lt;version&gt;</code> has the <code>YY.M[M]</code> format, for example November 2024 is <code>24.11</code>, and January 2025 would be <code>25.1</code>.</p> <p>The release schedule is not fixed, with new versions will be released roughly every 3-6 months, when there is a compelling reason to update.</p> version node types system 24.7 gh200, zen2 daint, eiger, todi 24.11 a100, gh200, zen2 daint, eiger, santis, clariden, bristen 24.11 <p>The key updates in version 24.11:v1 from the 24.7 version were:</p> <ul> <li>upgrading the versions of gcc@13 and cuda@12.6</li> <li>upgrading cray-mpich to version 8.1.30</li> <li>adding kokkos</li> <li>adding gsl</li> </ul> all packages exposed via the <code>default</code> and <code>modules</code> views in <code>v1</code> <ul> <li>aws-ofi-nccl@git.v1.9.2-aws_1.9.2</li> <li>boost@1.86.0</li> <li>cmake@3.30.5</li> <li>cray-mpich@8.1.30</li> <li>cuda@12.6.2<ul> <li>in the <code>gh200</code> and <code>a100</code> images</li> </ul> </li> <li>fftw@3.3.10</li> <li>fmt@11.0.2</li> <li>gcc@13.3.0</li> <li>gsl@2.8</li> <li>hdf5@1.14.5</li> <li>kokkos-kernels@4.4.01</li> <li>kokkos-tools@develop</li> <li>kokkos@4.4.01</li> <li>libtree@3.1.1</li> <li>lua@5.4.6</li> <li>lz4@1.10.0</li> <li>meson@1.5.1</li> <li>nccl-tests@2.13.6</li> <li>nccl@2.22.3-1</li> <li>netlib-scalapack@2.2.0</li> <li>ninja@1.12.1</li> <li>openblas@0.3.28<ul> <li>built with the OpenMP threading back end</li> </ul> </li> <li>osu-micro-benchmarks@5.9</li> <li>papi@7.1.0</li> <li>python@3.12.5</li> <li>superlu@5.3.0</li> <li>zlib-ng@2.2.1</li> </ul> 24.7:v2 changelog <p>The <code>v2</code> update added <code>netcdf</code>, specifically the following packages:</p> <ul> <li>netcdf-c@4.9.2</li> <li>netcdf-cxx@4.2</li> <li>netcdf-fortran@4.6.1</li> </ul> <p></p>"},{"location":"software/prgenv/prgenv-gnu/#how-to-use","title":"How to use","text":"<p>The environment is designed as a fairly minimal set of </p> <p>There are three ways to access the software provided by prgenv-gnu, once it has been started.</p> the default viewmodulesSpack <p>The simplest way to get started is to use the <code>default</code> file system view, which automatically loads all of the packages when the uenv is started.</p> <p>test mpi compilers and python provided by prgenv-gnu/24.11</p> <pre><code># start using the default view\n$ uenv start --view=default prgenv-gnu/24.11:v1\n\n# the python executable provided by the uenv is the default, and is a recent version\n$ which python\n/user-environment/env/default/bin/python\n$ python --version \nPython 3.12.5\n\n# the mpi compiler wrappers are also available\n$ which mpicc\n/user-environment/env/default/bin/mpicc\n$ mpicc --version\ngcc (Spack GCC) 13.3.0\n$ gcc --version # the compiler wrapper uses the gcc provided by the uenv\ngcc (Spack GCC) 13.3.0\n</code></pre> <p>The uenv provides modules for all of the software packages, which can be made available by using the <code>modules</code> view in  No modules are loaded when a uenv starts, and have to be loaded individually using <code>module load</code>.</p> <p>starting prgenv-gnu and listing the provided modules</p> <pre><code>$ uenv start prgenv-gnu/24.11:v1 --view=modules\n$ module avail\n    ---------------------------- /user-environment/modules ----------------------------\n   aws-ofi-nccl/git.v1.9.2-aws_1.9.2    lua/5.4.6\n   boost/1.86.0                         lz4/1.10.0\n   cmake/3.30.5                         meson/1.5.1\n   cray-mpich/8.1.30                    nccl-tests/2.13.6\n   cuda/12.6.2                          nccl/2.22.3-1\n   fftw/3.3.10                          netlib-scalapack/2.2.0\n   fmt/11.0.2                           ninja/1.12.1\n   gcc/13.3.0                           openblas/0.3.28\n   gsl/2.8                              osu-micro-benchmarks/5.9\n   hdf5/1.14.5                          papi/7.1.0\n   kokkos-kernels/4.4.01                python/3.12.5\n   kokkos-tools/develop                 superlu/5.3.0\n   kokkos/4.4.01                        zlib-ng/2.2.1\n   libtree/3.1.1\n</code></pre> <p>The gnu programming environment is a very good base for building software with Spack, because it provides compilers, MPI, Python and common packages like hdf5.</p> <p>Check out the guide for using Spack with uenv.</p>"},{"location":"software/prgenv/prgenv-nvfortran/","title":"prgenv-nvfortran","text":""},{"location":"software/prgenv/prgenv-nvfortran/#prgenv-nvfortran","title":"prgenv-nvfortran","text":"<p>The <code>prgenv-nvfortran</code> uenv provides a set of tools and libraries for building applications that need the NVIDIA Fortran compiler Specifically, it is intended for building and running applications that require one of the following: * OpenACC for GPU acceleration; * CUDA Fortran for GPU acceleration.</p> <p>Note</p> <p>By default, use the <code>prgenv-gnu</code> toolchain for a generic environment for building GPU applications. It provides CUDA and libraries with GPU support enabled for the Grace-Hopper nodes, the gnu compiler toolchain that it provides has better C and C++ standards compliance, and it also provides more libraries and tools than this <code>nvfortran</code> uenv.</p>"},{"location":"software/prgenv/prgenv-nvfortran/#versioning","title":"Versioning","text":"<p>The naming scheme is <code>prgenv-nvfortran/&lt;version&gt;:v&lt;i&gt;</code>, where <code>&lt;version&gt;</code> matches the version of the NVIDIA HPC SDK.</p> <ul> <li>the SDK is released every two months, and is numbered in the <code>YY.M[M]</code> format, e.g. <code>24.1</code> and <code>24.11</code>.</li> <li>the <code>prgenv-nvfortran</code> will be released three times a year (every second NVHPC release).</li> </ul> <p>The currently supported versions are:</p> <code>prgenv-nvhpc</code> NVHPC 24.11 24.11 24.11 <p>Version 24.11 provides the following software:</p> <ul> <li>cmake@3.30.5</li> <li>cray-mpich@8.1.30</li> <li>cuda@12.6.0</li> <li>fftw@3.3.10</li> <li>fmt@11.0.2</li> <li>gcc@13.2.0</li> <li>hdf5@1.14.5</li> <li>libtree@3.1.1</li> <li>lua@5.4.6</li> <li>lz4@1.10.0</li> <li>meson@1.5.1</li> <li>netcdf-c@4.9.2</li> <li>netcdf-fortran@4.6.1</li> <li>ninja@1.12.1</li> <li>nvhpc@24.11</li> <li>nvpl-blas@0.3.0</li> <li>nvpl-lapack@0.2.3.1</li> <li>osu-micro-benchmarks@5.9</li> <li>python@3.12.5</li> <li>zlib-ng@2.2.1</li> </ul> I need a different version <p>If you need a version of the NVHPC SDK that is not provided, e.g. the 25.1 version that falls between 24.11 and 25.4, make a request on the CSCS service desk.</p>"},{"location":"software/prgenv/prgenv-nvfortran/#how-to-use","title":"How to use","text":"<p>The image is only provided on Alps systems that have NVIDIA GPUs. To see which versions have been installed on a system:</p> <pre><code># search for uenv on the current system\nuenv image find prgenv-nvfortran\n\n# search for uenv on all systems\nuenv image find prgenv-nvfortran@*\n\n# pull a version\nuenv image find prgenv-nvfortran/24.11:v1\n</code></pre> the nvfort viewthe modules view <p>The nvfort view loads all of the packages into your environment (equivalent to loading all the modules at once):</p> <pre><code>uenv start prgenv-nvfortran/24.11:v1 --view=nvfort\nmpif90 --version\n</code></pre> <p>The above example shows that the MPI compiler wrappers are using the underlying NVIDIA compiler. The following wrappers are available:</p> <ul> <li><code>mpif77</code></li> <li><code>mpif90</code></li> <li><code>mpifort</code></li> </ul> <p>And the following C/C++ wrappers are available:</p> <ul> <li><code>mpicc</code></li> <li><code>mpicxx</code></li> </ul> <p>The modules view will start the uenv, and make a set of modules available:</p> <pre><code>$ uenv start prgenv-nvfortran/24.11:v1 --view=nvfort,modules\n$ module avail\n---------------------------- /user-environment/modules ----------------------------\n   aws-ofi-nccl/master    libtree/3.1.1           ninja/1.12.1\n   cmake/3.30.5           lua/5.4.6               nvhpc/24.11\n   cray-mpich/8.1.30      lz4/1.10.0              nvpl-blas/0.3.0\n   cuda/12.6.0            meson/1.5.1             nvpl-lapack/0.2.3.1\n   fftw/3.3.10            nccl-tests/2.13.6       osu-micro-benchmarks/5.9\n   fmt/11.0.2             nccl/2.22.3-1           python/3.12.5\n   gcc/13.2.0             netcdf-c/4.9.2          zlib-ng/2.2.1\n   hdf5/1.14.5            netcdf-fortran/4.6.1\n</code></pre> <p>None of the modules are loaded by default, so you will have to load the required modules</p>"},{"location":"software/sciapps/","title":"Index","text":""},{"location":"software/sciapps/#scientific-applications","title":"Scientific Applications","text":"<p>CSCS provides and supports a selection of scientific applications on the computing systems: we usually build community codes that are adopted by several users on our systems.</p> <p>Please have a look at the individual application page on the menu to find out how to run in a production environment.</p> <p>CSCS staff can also help users with performance analysis to optimise their workflow in production.</p> <ul> <li>CP2K</li> <li>GROMACS</li> <li>LAMMPS</li> <li>NAMD</li> <li>Quantum ESPRESSO</li> <li>VASP</li> </ul> <p>Unsupported Applications</p> <p>Please note that Amber and CPMD previously provided on the Piz Daint XC system are not provided by CSCS on Alps.</p>"},{"location":"software/sciapps/cp2k/","title":"CP2K","text":""},{"location":"software/sciapps/cp2k/#cp2k","title":"CP2K","text":"<p>CP2K is a quantum chemistry and solid state physics software package that can perform atomistic simulations of solid state, liquid, molecular, periodic, material, crystal, and biological systems.</p> <p>CP2K provides a general framework for different modeling methods such as DFT using the mixed Gaussian and plane waves approaches GPW and GAPW. Supported theory levels include DFTB, LDA, GGA, MP2, RPA, semi-empirical methods (AM1, PM3, PM6, RM1, MNDO, \u2026), and classical force fields (AMBER, CHARMM, \u2026). CP2K can do simulations of molecular dynamics, metadynamics, Monte Carlo, Ehrenfest dynamics, vibrational analysis, core level spectroscopy, energy minimization, and transition state optimization using NEB or dimer method. See CP2K Features for a detailed overview.</p> <p>uenvs</p> <p>CP2K is provided on ALPS via uenv. Please have a look at the uenv documentation for more information about uenvs and how to use them.</p>"},{"location":"software/sciapps/cp2k/#dependencies","title":"Dependencies","text":"<p>On our systems, CP2K is built with the following dependencies:</p> <ul> <li>COSMA</li> <li>Cray MPICH</li> <li>DBCSR</li> <li>DLA-Future</li> <li>dftd4 (from <code>cp2k@2025.1</code> onwards)</li> <li>ELPA</li> <li>FFTW</li> <li>Libxc</li> <li>libint</li> <li>OpenBLAS</li> <li>PLUMED (from <code>cp2k@2024.1</code> onwards)</li> <li>ScaLAPACK</li> <li>SIRIUS</li> <li>Spglib</li> <li>spla</li> </ul> <p>GPU-aware MPI</p> <p>COSMA and DLA-Future are built with GPU-aware MPI, which requires setting <code>MPICH_GPU_SUPPORT_ENABLED=1</code>. On the HPC platform, <code>MPICH_GPU_SUPPORT_ENABLED=1</code> is set by default.</p> <p>CUDA cache path for JIT compilation</p> <p>DBCSR uses JIT compilation for CUDA kernels. The default location is in the home directory, which can put unnecessary burden on the filesystem and lead to performance degradation. Because of this we set <code>CUDA_CACHE_PATH</code> to point to the in-memory filesystem in <code>/dev/shm</code>. On the HPC platform, <code>CUDA_CACHE_PATH</code> is set to a directory under <code>/dev/shm</code> by default.</p> <p>BLAS/LAPACK on Eiger</p> <p>On Eiger, the default BLAS/LAPACK library is Intel oneAPI MKL (oneMKL) until <code>cp2k@2024.3</code>.  From <code>cp2k@2025.1</code> the default BLAS/LAPACK library is OpenBLAS.</p>"},{"location":"software/sciapps/cp2k/#running-cp2k","title":"Running CP2K","text":""},{"location":"software/sciapps/cp2k/#running-on-the-hpc-platform","title":"Running on the HPC platform","text":"<p>To start a job, two bash scripts are potentially required: a slurm submission script, and a wrapper to start the CUDA MPS daemon so that multiple MPI ranks can use the same GPU.</p> run_cp2k.sh<pre><code>#!/bin/bash -l\n\n#SBATCH --job-name=cp2k-job\n#SBATCH --time=00:30:00 (1)\n#SBATCH --nodes=4\n#SBATCH --ntasks-per-core=1\n#SBATCH --ntasks-per-node=32 (2)\n#SBATCH --cpus-per-task=8 (3)\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --hint=nomultithread\n#SBATCH --hint=exclusive\n#SBATCH --no-requeue\n#SBATCH --uenv=&lt;CP2K_UENV&gt;\n#SBATCH --view=cp2k\n\nexport CUDA_CACHE_PATH=\"/dev/shm/$USER/cuda_cache\" # (5)!\nexport MPICH_GPU_SUPPORT_ENABLED=1 # (6)!\nexport MPICH_MALLOC_FALLBACK=1\nexport OMP_NUM_THREADS=$((SLURM_CPUS_PER_TASK - 1)) # (4)!\n\nulimit -s unlimited\nsrun --cpu-bind=socket ./mps-wrapper.sh cp2k.psmp -i &lt;CP2K_INPUT&gt; -o &lt;CP2K_OUTPUT&gt;\n</code></pre> <ol> <li> <p>Time format: <code>HH:MM:SS</code></p> </li> <li> <p>Number of MPI ranks per node</p> </li> <li> <p>Number of CPUs per MPI ranks</p> </li> <li> <p>OpenBLAS spawns an extra thread, therefore it is necessary to set <code>OMP_NUM_THREADS</code> to <code>SLURM_CPUS_PER_TASK - 1</code>    for good performance. With Intel MKL, this is not necessary and one can set <code>OMP_NUM_THREADS</code> to    <code>SLURM_CPUS_PER_TASK</code>.</p> </li> <li> <p>DBCSR relies on extensive JIT compilation, and we store the cache in memory to avoid I/O overhead.    This is set by default on the HPC platform, but it's set here explicitly as it's essential to avoid performance degradation.</p> </li> <li> <p>CP2K's dependencies use GPU-aware MPI, which requires enabling support at runtime.    This is set by default on the HPC platform, but it's set here explicitly as it's a requirement in general for enabling GPU-aware MPI.</p> </li> <li> <p>Change  to your project account name <li>Change <code>&lt;CP2K_UENV&gt;</code> to the name (or path) of the actual CP2K uenv you want to use</li> <li>Change <code>&lt;PATH_TO_CP2K_DATA_DIR&gt;</code> to the actual path to the CP2K data directory</li> <li>Change <code>&lt;CP2K_INPUT&gt;</code> and <code>&lt;CP2K_OUTPUT&gt;</code> to the actual input and output files</li> <p>With the above scripts, you can launch a CP2K calculation on 4 nodes, with 32 MPI ranks per node and 8 OpenMP threads per rank with</p> <pre><code>sbatch run_cp2k.sh\n</code></pre> <p>Note</p> <p>The <code>mps-wrapper.sh</code> script, required to properly over-subscribe the GPU, is provided at the following page:  NVIDIA GH200 GPU nodes: multiple ranks per GPU.</p> <p>Warning</p> <p>The <code>--cpu-bind=socket</code> option is necessary to get good performance.</p> <p>Warning</p> <p>Each GH200 node has 4 modules, each of them composed of a ARM Grace CPU with 72 cores and a H200 GPU directly attached to it. Please see Alps hardware for more information. It is important that the number of MPI ranks passed to slurm with <code>--ntasks-per-node</code> is a multiple of 4.</p> Note <p>In the example above, we use 32 MPI ranks with 8 OpenMP threads, for a total of 64 cores per GPU and 256 cores  per node. Experiments have shown that CP2K performs and scales better when the number of MPI ranks is a power  of 2, even if some cores are left idling. </p> Running regression tests <p>If you want to run CP2K regression tests with the CP2K executable provided by the uenv, make sure to use the version of the regression tests corresponding to the version of CP2K provided by the uenv. The regression test data is sometimes adjusted, and using the wrong version of the regression tests can lead to test failures.</p> Scaling of <code>QS/H2O-1024</code> benchmark <p>The <code>QS/H2O-1024</code> benchmark is a DFT molecular dynamics simulation of liquid water. It relies on DBCSR for block sparse matrix-matrix multiplication.</p> <p>All calculations were run with 32 MPI ranks per node, and 8 OpenMP threads per rank (best configuration for this benchmark). </p> <p>Note</p> <p><code>H2O-102.inp</code> is the largest example of DFT molecular dynamics simulation of liquid water that fits on a single GH200 node.</p> Number of nodes Wall time (s) Speedup Efficiency 1 793.1 1.00 1.00 2 535.2 1.48 0.74 4 543.9 1.45 0.36 8 487.3 1.64 0.20 16 616.7 1.28 0.08 <p>Scaling is not ideal on more than two nodes.</p> Scaling of <code>QS_mp2_rpa/128-H2O/H2O-128-RI-MP2-TZ</code> benchmark <p>The <code>QS_mp2_rpa/128-H2O/H2O-128-RI-MP2-TZ</code> benchmark is a straightforward modification of the <code>QS_mp2_rpa/64-H2O/H2O-64-RI-MP2-TZ</code> benchmark.</p> <p>It is a RI-MP2 calculation of a water cluster with 128 atoms.</p> Input file <pre><code>&amp;GLOBAL\n  PRINT_LEVEL MEDIUM\n  PROJECT H2O-128-RI-MP2-TZ\n  RUN_TYPE ENERGY\n&amp;END GLOBAL\n\n&amp;FORCE_EVAL\n  METHOD Quickstep\n  &amp;DFT\n    BASIS_SET_FILE_NAME ./BASIS_H2O\n    POTENTIAL_FILE_NAME ./POTENTIAL_H2O\n    WFN_RESTART_FILE_NAME ./H2O-128-PBE-TZ-RESTART.wfn\n    &amp;MGRID\n      CUTOFF 800\n      REL_CUTOFF 50\n    &amp;END MGRID\n    &amp;QS\n      EPS_DEFAULT 1.0E-12\n    &amp;END QS\n    &amp;SCF\n      EPS_SCF 1.0E-6\n      MAX_SCF 30\n      SCF_GUESS RESTART\n      &amp;OT\n        MINIMIZER CG\n        PRECONDITIONER FULL_ALL\n      &amp;END OT\n      &amp;OUTER_SCF\n        EPS_SCF 1.0E-6\n        MAX_SCF 20\n      &amp;END OUTER_SCF\n      &amp;PRINT\n        &amp;RESTART OFF\n        &amp;END RESTART\n      &amp;END PRINT\n    &amp;END SCF\n    &amp;XC\n      &amp;HF\n        FRACTION 1.0\n        &amp;INTERACTION_POTENTIAL\n          CUTOFF_RADIUS 6.0\n          POTENTIAL_TYPE TRUNCATED\n          T_C_G_DATA ./t_c_g.dat\n        &amp;END INTERACTION_POTENTIAL\n        &amp;MEMORY\n          MAX_MEMORY 16384\n        &amp;END MEMORY\n        &amp;SCREENING\n          EPS_SCHWARZ 1.0E-8\n          SCREEN_ON_INITIAL_P TRUE\n        &amp;END SCREENING\n      &amp;END HF\n      &amp;WF_CORRELATION\n        MEMORY 1200\n        NUMBER_PROC 1\n        &amp;INTEGRALS\n          &amp;WFC_GPW\n            CUTOFF 300\n            EPS_FILTER 1.0E-12\n            EPS_GRID 1.0E-8\n            REL_CUTOFF 50\n          &amp;END WFC_GPW\n        &amp;END INTEGRALS\n        &amp;RI_MP2\n        &amp;END RI_MP2\n      &amp;END WF_CORRELATION\n      &amp;XC_FUNCTIONAL NONE\n      &amp;END XC_FUNCTIONAL\n    &amp;END XC\n  &amp;END DFT\n  &amp;SUBSYS\n    &amp;CELL\n      ABC 15.6404 15.6404 15.6404\n    &amp;END CELL\n    &amp;KIND H\n      BASIS_SET cc-TZ\n      BASIS_SET RI_AUX RI-cc-TZ\n      POTENTIAL GTH-HF-q1\n    &amp;END KIND\n    &amp;KIND O\n      BASIS_SET cc-TZ\n      BASIS_SET RI_AUX RI-cc-TZ\n      POTENTIAL GTH-HF-q6\n    &amp;END KIND\n    &amp;TOPOLOGY\n      COORD_FILE_FORMAT XYZ\n      COORD_FILE_NAME ./H2O-128.xyz\n    &amp;END TOPOLOGY\n  &amp;END SUBSYS\n&amp;END FORCE_EVAL\n</code></pre> <p>All calculations run for this scaling tests were using 32 MPI ranks per node and 8 OpenMP threads per rank.  The smallest amount of nodes necessary to run this calculation is 8.</p> Number of nodes Wall time (s) Speedup Efficiency 8 2037.0 1.00 1.00 16 1096.2 1.85 0.92 32 611.5 3.33 0.83 64 410.5 4.96 0.62 128 290.9 7.00 0.43 <p>MP2 calculations scale well on GH200, up to a large number of nodes (\\(&gt; 50\\%\\) efficiency with 64 nodes).</p> Scaling of <code>QS_mp2_rpa/128-H2O/H2O-128-RI-dRPA-TZ</code> benchmark <p>The <code>QS_mp2_rpa/128-H2O/H2O-128-RI-dRPA-T</code> benchmark is a RPA energy calculation, traditionally used to benchmark the performance of the COSMA library.  It a very large calculation, which requires at least 8 GH200 nodes to run.  The calculations were run with 16 MPI ranks per node and 16 OpenMP threads per rank. For RPA workloads, a higher ratio of threads per rank were beneficial.</p> Number of nodes Wall time (s) Speedup Efficiency 8 575.4 1.00 1.00 16 465.8 1.23 0.61 32 281.1 2.04 0.51 64 205.3 2.80 0.35 128 185.8 3.09 0.19 <p>This RPA input scales well until 32 GH200 nodes.</p>"},{"location":"software/sciapps/cp2k/#running-on-eiger","title":"Running on Eiger","text":"<p>On Eiger, a similar sbatch script can be used:</p> run_cp2k.sh<pre><code>#!/bin/bash -l\n#SBATCH --job-name=cp2k-job\n#SBATCH --time=00:30:00 (1)\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-core=1\n#SBATCH --ntasks-per-node=32 (2)\n#SBATCH --cpus-per-task=4 (3)\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --hint=nomultithread\n#SBATCH --hint=exclusive\n#SBATCH --constraint=mc\n#SBATCH --uenv=&lt;CP2K_UENV&gt;\n#SBATCH --view=cp2k\n\nexport OMP_NUM_THREADS=$((SLURM_CPUS_PER_TASK - 1)) # (4)!\n\nulimit -s unlimited\nsrun --cpu-bind=socket cp2k.psmp -i &lt;CP2K_INPUT&gt; -o &lt;CP2K_OUTPUT&gt;\n</code></pre> <ol> <li> <p>Time format: <code>HH:MM:SS</code></p> </li> <li> <p>Number of MPI ranks per node</p> </li> <li> <p>Number of CPUs per MPI ranks</p> </li> <li> <p>OpenBLAS spawns an extra thread, therefore it is necessary to set <code>OMP_NUM_THREADS</code> to <code>SLURM_CPUS_PER_TASK - 1</code>    for good performance. With Intel MKL, this is not necessary and one can set <code>OMP_NUM_THREADS</code> to    <code>SLURM_CPUS_PER_TASK</code>.</p> </li> <li> <p>Change  to your project account name <li>Change <code>&lt;CP2K_UENV&gt;</code> to the name (or path) of the actual CP2K uenv you want to use</li> <li>Change <code>&lt;PATH_TO_CP2K_DATA_DIR&gt;</code> to the actual path to the CP2K data directory</li> <li>Change <code>&lt;CP2K_INPUT&gt;</code> and <code>&lt;CP2K_OUTPUT&gt;</code> to the actual input and output files</li> <p>Warning</p> <p>The <code>--cpu-bind=socket</code> option is necessary to get good performance.</p> Running regression tests <p>If you want to run CP2K regression tests with the CP2K executable provided by the uenv, make sure to use the version of the regression tests corresponding to the version of CP2K provided by the uenv. The regression test data is sometimes adjusted, and using the wrong version of the regression tests can lead to test failures.</p>"},{"location":"software/sciapps/cp2k/#building-cp2k-from-source","title":"Building CP2K from Source","text":"<p>Warning</p> <p>The following installation instructions are up-to-date with the latest version of CP2K provided by the uenv. That is, they work when manually compiling the CP2K source code corresponding to the CP2K version provided by the uenv. They are not necessarily up-to-date with the latest version of CP2K available on the <code>master</code> branch.</p> <p>If you are trying to build CP2K from source, make sure you understand what is different in <code>master</code> compared to the latest version of CP2K provided by the uenv.</p> <p>The CP2K uenv provides all the dependencies required to build CP2K from source, with several optional features enabled. You can follow these steps to build CP2K from source:</p> <pre><code>uenv start --view=develop &lt;CP2K_UENV&gt; # (1)!\n\ncd &lt;PATH_TO_CP2K_SOURCE&gt; # (2)!\n\nmkdir build &amp;&amp; cd build\nCC=mpicc CXX=mpic++ FC=mpifort cmake \\\n    -GNinja \\\n    -DCMAKE_CUDA_HOST_COMPILER=mpicc \\ # (3)!\n    -DCP2K_USE_LIBXC=ON \\\n    -DCP2K_USE_LIBINT2=ON \\\n    -DCP2K_USE_SPGLIB=ON \\\n    -DCP2K_USE_ELPA=ON \\\n    -DCP2K_USE_SPLA=ON \\\n    -DCP2K_USE_SIRIUS=ON \\\n    -DCP2K_USE_COSMA=ON \\\n    -DCP2K_USE_PLUMED=ON \\\n    -DCP2K_USE_DFTD4=ON \\\n    -DCP2K_USE_DLAF=ON \\\n    -DCP2K_USE_ACCEL=CUDA -DCP2K_WITH_GPU=H100 \\ # (4)!\n    ..\n\nninja -j 32\n</code></pre> <ol> <li> <p>Start the CP2K uenv and load the <code>develop</code> view (which provides all the necessary dependencies)</p> </li> <li> <p>Go to the CP2K source directory</p> </li> <li> <p>The <code>H100</code> option enables the <code>sm_90</code> architecture for the CUDA backend</p> </li> </ol> <p>Eiger: <code>libxsmm</code></p> <p>On <code>x86</code> we deploy with <code>libxmm</code>. Add <code>-DCP2K_USE_LIBXSMM=ON</code> to the CMake invocation to use <code>libxsmm</code>.</p> Eiger: Intel MKL (before <code>cp2k@2025.1</code>) <p>On <code>x86</code> we deployed with <code>intel-oneapi-mkl</code> before <code>cp2k@2025.1</code>.  If you are using a pre-<code>cp2k@2025.1</code> uenv, add <code>-DCP2K_SCALAPACK_VENDOR=MKL</code> to the CMake invocation to find MKL.</p> CUDA architecture for <code>cp2k@2024.1</code> and earlier <p><code>cp2k@2024.1</code> (and earlier) does not support compiling for <code>cuda_arch=90</code>. Use <code>-DCP2K_WITH_GPU=A100</code> instead,  which enables the <code>sm_80</code> architecture.</p> <p>See manual.cp2k.org/CMake for more details.</p>"},{"location":"software/sciapps/cp2k/#known-issues","title":"Known issues","text":""},{"location":"software/sciapps/cp2k/#dla-future","title":"DLA-Future","text":"<p>The <code>cp2k/2025.1</code> uenv provides CP2K with DLA-Future support enabled. The DLA-Future library is initialized even if you don't explicitly ask to use it. This can lead to some surprising warnings and failures described below.</p>"},{"location":"software/sciapps/cp2k/#cusolver_status_internal_error-during-initialization","title":"<code>CUSOLVER_STATUS_INTERNAL_ERROR</code> during initialization","text":"<p>If you are heavily over-subscribing the GPU by running multiple ranks per GPU, you may encounter the following error:</p> <pre><code>created exception: cuSOLVER function returned error code 7 (CUSOLVER_STATUS_INTERNAL_ERROR): pika(bad_function_call)\nterminate called after throwing an instance of 'pika::cuda::experimental::cusolver_exception'\nwhat(): cuSOLVER function returned error code 7 (CUSOLVER_STATUS_INTERNAL_ERROR): pika(bad_function_call)\n</code></pre> <p>The reason is that too many cuSOLVER handles are created. If you don't need DLA-Future, you can manually set the number of BLAS and LAPACK handlers to 1 by setting the following environment variables:</p> <pre><code>DLAF_NUM_GPU_BLAS_HANDLES=1\nDLAF_NUM_GPU_LAPACK_HANDLES=1\n</code></pre>"},{"location":"software/sciapps/cp2k/#warning-about-pika-only-using-one-worker-thread","title":"Warning about pika only using one worker thread","text":"<p>When running CP2K with multiple tasks per node and only one core per task, the initialization of DLA-Future may trigger the following warning:</p> <pre><code>The pika runtime will be started with only one worker thread because the\nprocess mask has restricted the available resources to only one thread. If\nthis is unintentional make sure the process mask contains the resources\nyou need or use --pika:ignore-process-mask to use all resources. Use\n--pika:print-bind to print the thread bindings used by pika.\n</code></pre> <p>This warning is triggered because the runtime used by DLA-Future, pika, should typically be used with more than one thread and indicates a configuration mistake. However, if you are not using DLA-Future, the warning is harmless and can be ignored. The warning cannot be silenced.</p>"},{"location":"software/sciapps/cp2k/#dbcsr-gpu-scaling","title":"DBCSR GPU scaling","text":"<p>On the GH200 architecture, it has been observed that the GPU accelerated version of DBCSR does not perform optimally in some cases. For example, in the <code>QS/H2O-1024</code> benchmark above, CP2K does not scale well beyond 2 nodes.  The CPU implementation of DBCSR does not suffer from this. A workaround was implemented in DBCSR, in order to switch  GPU acceleration on/off with an environment variable:</p> <pre><code>export DBCSR_RUN_ON_GPU=0\n</code></pre> <p>While GPU acceleration is very good on few nodes, the CPU implementation scales better.  Therefore, for CP2K jobs running on many nodes, it is worth investigating the use of the <code>DBCSR_RUN_ON_GPU</code> environment variable.</p> <p>Some niche application cases such as the <code>QS_low_scaling_postHF</code> benchmarks only run efficiently with the CPU version of DBCSR. Generally, if the function <code>dbcsr_multiply_generic</code> takes a significant portion of the timing report (at the end of the CP2K output file), it is worth investigating the effect of the <code>DBCSR_RUN_ON_GPU</code> environment variable.</p>"},{"location":"software/sciapps/cp2k/#cuda-grid-backend-with-high-angular-momenta-basis-sets","title":"CUDA grid backend with high angular momenta basis sets","text":"<p>The CP2K grid CUDA backend is currently buggy on Alps. Using basis sets with high angular momenta (\\(l \\ge 3\\)) result in slow calculations, especially for force calculations with meta-GGA functionals. </p> <p>As a workaround, you can disable CUDA acceleration for the grid backend:</p> <pre><code>&amp;GLOBAL\n    &amp;GRID\n        BACKEND CPU\n    &amp;END GRID\n&amp;END GLOBAL\n</code></pre> Fix available upon request <p>A fix for this issue for the HIP backend is currently being tested by CSCS engineers. If you would like to test it, please contact us and we will be able to provide the source code. The fix will eventually land on the upstream CP2K repository.</p>"},{"location":"software/sciapps/gromacs/","title":"GROMACS","text":""},{"location":"software/sciapps/gromacs/#gromacs","title":"GROMACS","text":"<p>Todo</p> <p>complete docs</p>"},{"location":"software/sciapps/lammps/","title":"LAMMPS","text":""},{"location":"software/sciapps/lammps/#lammps","title":"LAMMPS","text":"<p>Todo</p> <p>complete docs</p>"},{"location":"software/sciapps/namd/","title":"NAMD","text":""},{"location":"software/sciapps/namd/#namd","title":"NAMD","text":"<p>NAMD is a parallel molecular dynamics code based on Charm++, designed for high-performance simulations of large biomolecular systems.</p> <p>Licensing Terms and Conditions</p> <p>NAMD is distributed free of charge for research purposes only and not for commercial use: users must agree to the NAMD license in order to use it at CSCS. Users agree to acknowledge use of NAMD in any reports or publications of results obtained with the Software (see NAMD Homepage for details).</p> <p>User Environments</p> <p>NAMD is provided on ALPS as a uenv. Please have a look at the uenv documentation for more information about UENVs and how to use them.</p> <p>NAMD is provided in two flavours on CSCS systems:</p> <ul> <li>Single-node build</li> <li>Multi-node build</li> </ul> <p>The single-node build works on a single node and benefits from the new GPU-resident mode (see NAMD 3.0b6 GPU-Resident benchmarking results for more details). The multi-node build works on multiple nodes and is based on Charm++'s MPI backend.</p> <p>Prefer the single-node build and exploit GPU-resident mode</p> <p>Unless you have good reasons to use the multi-node build, we recommend using the single-node build with the GPU-resident mode.</p>"},{"location":"software/sciapps/namd/#single-node-build","title":"Single-node build","text":"<p>The single-node build provides the following views:</p> <ul> <li><code>namd-single-node</code> (standard view, with NAMD)</li> <li><code>develop-single-node</code> (development view, without NAMD)</li> </ul>"},{"location":"software/sciapps/namd/#running-namd-on-a-single-node","title":"Running NAMD on a single node","text":"<p>The following sbatch script shows how to run NAMD on a single node with 4 GPUs:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"namd-example\"\n#SBATCH --time=00:10:00\n#SBATCH --account=&lt;ACCOUNT&gt;\n#SBATCH --nodes=1                    (1)\n#SBATCH --ntasks-per-node=1          (2)\n#SBATCH --cpus-per-task=288\n#SBATCH --gres=gpu:4                 (3)\n#SBATCH --uenv=&lt;NAMD_UENV&gt;           (4)\n#SBATCH --view=namd-single-node      (5)\n\n\nsrun namd3 +p 29 +pmeps 5 +setcpuaffinity +devices 0,1,2,3 &lt;NAMD_CONFIG_FILE&gt;\n</code></pre> <ol> <li>You can only use one node with the <code>single-node</code> build</li> <li>You can only use one task per node with the <code>single-node</code> build</li> <li>Make all GPUs visible to NAMD (by automatically setting <code>CUDA_VISIBLE_DEVICES=0,1,2,3</code>)</li> <li>Load the NAMD UENV (UENV name or path to the UENV)</li> <li> <p>Load the <code>namd-single-node</code> view</p> </li> <li> <p>Change <code>&lt;ACCOUNT&gt;</code> to your project account</p> </li> <li>Change <code>&lt;NAMD_UENV&gt;</code> to the name (or path) of the actual NAMD UENV you want to use</li> <li>Change <code>&lt;NAMD_CONFIG_FILE&gt;</code> to the name (or path) of the NAMD configuration file for your simulation </li> <li>Make sure you set <code>+p</code>, <code>+pmeps</code>, and other NAMD options optimally for your calculation</li> </ol> Scaling of STMV benchmark with GPU-resident mode from 1 to 4 GPUs <p>Scaling of the tobacco mosaic virus (STMV) benchmark with GPU-resident mode on our system is the following:</p> GPUs ns/day Speedup Parallel efficiency 1 31.1 - - 2 53.7 1.9 86% 4 92.7 3.5 74% 1 GPU2 GPUs4 GPUs <pre><code>srun namd3 +p 8 +setcpuaffinity +devices 0 &lt;NAMD_CONFIG_FILE&gt;\n</code></pre> <pre><code>srun namd3 +p 15 +pmeps 7 +setcpuaffinity +devices 0,1 &lt;NAMD_CONFIG_FILE&gt;\n</code></pre> <pre><code>srun namd3 +p 29 +pmeps 5 +setcpuaffinity +devices 0,1,2,3 &lt;NAMD_CONFIG_FILE&gt;\n</code></pre>"},{"location":"software/sciapps/namd/#building-namd-from-source-with-charms-multicore-backend","title":"Building NAMD from source with Charm++'s multicore backend","text":"<p>Action required</p> <p>According to the NAMD 3.0 release notes, TCL <code>8.6</code> is required. However, the source code for the <code>3.0</code> release still contains hard-coded flags for TCL <code>8.5</code>. The UENV provides <code>tcl@8.6</code>, therefore you need to manually modify NAMD 3.0's <code>arch/Linux-ARM64.tcl</code> file as follows: change <code>-ltcl8.5</code> to <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable.</p> <p>The NAMD <code>uenv</code> provides all the dependencies required to build NAMD from source.</p> GPU BuildCPU Build <p>Build NAMD:</p> <pre><code>export DEV_VIEW_NAME=\"develop-single-node\"\nexport PATH_TO_NAMD_SOURCE=&lt;PATH_TO_NAMD_SOURCE&gt;\n\n# Start uenv and load develop view\nuenv start --view=${DEV_VIEW_NAME} &lt;NAMD_UENV&gt;\n\n# Set variable VIEW_PATH to the view\nexport DEV_VIEW_PATH=/user-environment/env/${DEV_VIEW_NAME}\n\ncd ${PATH_TO_NAMD_SOURCE}\n</code></pre> <p>Action required</p> <p>Modify the <code>&lt;PATH_TO_NAMD_SOURCE&gt;/arch/Linux-ARM64.tcl</code> file now. Change <code>-ltcl8.5</code> with <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable.</p> <pre><code># Build bundled Charm++\ntar -xvf charm-8.0.0.tar &amp;&amp; cd charm-8.0.0\n./build charm++ multicore-linux-arm8 gcc --with-production --enable-tracing -j 32\n\n# Configure NAMD build for GPU\ncd .. \n./config Linux-ARM64-g++.cuda \\\n    --charm-arch multicore-linux-arm8-gcc --charm-base $PWD/charm-8.0.0 \\\n    --with-tcl --tcl-prefix ${DEV_VIEW_PATH} \\\n    --with-fftw --with-fftw3 --fftw-prefix ${DEV_VIEW_PATH} \\\n    --cuda-gencode arch=compute_90,code=sm_90 --with-single-node-cuda --with-cuda --cuda-prefix ${DEV_VIEW_PATH}\ncd Linux-ARM64-g++.cuda &amp;&amp; make -j 32\n\n# The namd3 executable (GPU-accelerated) will be built in the Linux-ARM64-g++.cuda directory\n</code></pre> <ul> <li>Change <code>&lt;PATH_TO_NAMD_SOURCE&gt;</code> to the path where you have the NAMD source code</li> <li>Change <code>&lt;NAMD_UENV&gt;</code> to the name (or path) of the actual NAMD UENV you want to use</li> </ul> <p>To run NAMD, make sure you load the same UENV and view you used to build NAMD, and set the following variable:</p> <pre><code>export LD_LIBRARY_PATH=\"${DEV_VIEW_PATH}/lib/\"\n</code></pre> <p>Some workflows, such as constant pH MD simulations, might require a CPU-only NAMD build which is used to drive the simulation.</p> <p>Use the CPU-only build only if needed</p> <p>The CPU-only build is optional and should be used only if needed. You should use it in conjunction with the GPU build to drive the simulation. Do not use the CPU-only build for actual simulations as it will be slower than the GPU build.</p> <p>You can build a CPU-only version of NAMD as follows:</p> <pre><code>export DEV_VIEW_NAME=\"develop-single-node\"\nexport PATH_TO_NAMD_SOURCE=&lt;PATH_TO_NAMD_SOURCE&gt;\n\n# Start uenv and load develop view\nuenv start --view=${DEV_VIEW_NAME} &lt;NAMD_UENV&gt;\n\n# Set variable VIEW_PATH to the view\nexport DEV_VIEW_PATH=/user-environment/env/${DEV_VIEW_NAME}\n\ncd ${PATH_TO_NAMD_SOURCE}\n</code></pre> <p>Action required</p> <p>Modify the <code>&lt;PATH_TO_NAMD_SOURCE&gt;/arch/Linux-ARM64.tcl</code> file now. Change <code>-ltcl8.5</code> with <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable.</p> <pre><code># Build bundled Charm++\ntar -xvf charm-8.0.0.tar &amp;&amp; cd charm-8.0.0\n./build charm++ multicore-linux-arm8 gcc --with-production --enable-tracing -j 32\n\n# Configure NAMD build for GPU\ncd ..\n./config Linux-ARM64-g++ \\\n    --charm-arch multicore-linux-arm8-gcc --charm-base $PWD/charm-8.0.0 \\\n    --with-tcl --tcl-prefix ${DEV_VIEW_PATH} \\\n    --with-fftw --with-fftw3 --fftw-prefix ${DEV_VIEW_PATH}\ncd Linux-ARM64-g++ &amp;&amp; make -j 32\n\n# The namd3 executable (CPU-only) will be built in the Linux-ARM64-g++ directory\n</code></pre> <ul> <li>Change <code>&lt;PATH_TO_NAMD_SOURCE&gt;</code> to the path where you have the NAMD source code</li> </ul> <p>To run NAMD, make sure you load the same UENV and view you used to build NAMD, and set the following variable:</p> <pre><code>export LD_LIBRARY_PATH=\"${DEV_VIEW_PATH}/lib/\"\n</code></pre>"},{"location":"software/sciapps/namd/#multi-node-build","title":"Multi-node build","text":"<p>The multi-node build provides the following views:</p> <ul> <li><code>namd</code> (standard view, with NAMD)</li> <li><code>develop</code> (development view, without NAMD)</li> </ul> <p>GPU-resident mode</p> <p>The multi-node build based on Charm++'s MPI backend can't take advantage of the new GPU-resident mode. Unless you require the multi-node build or you can prove it is faster for your use case, we recommend using the single-node build with the GPU-resident mode.</p>"},{"location":"software/sciapps/namd/#building-namd-from-source-with-charms-mpi-backend","title":"Building NAMD from source with Charm++'s MPI backend","text":"<p>TCL Version</p> <p>According to the NAMD 3.0 release notes, TCL <code>8.6</code> is required. However, the source code for the <code>3.0</code> release still contains hard-coded flags for TCL <code>8.5</code>. The UENV provides <code>tcl@8.6</code>, therefore you need to manually modify NAMD 3.0's <code>arch/Linux-ARM64.tcl</code> file as follows: change <code>-ltcl8.5</code> to <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable.</p> <p>The NAMD <code>uenv</code> provides all the dependencies required to build NAMD from source. You can follow these steps to build NAMD from source:</p> <pre><code>export DEV_VIEW_NAME=\"develop\"\nexport PATH_TO_NAMD_SOURCE=&lt;PATH_TO_NAMD_SOURCE&gt;\n\n# Start uenv and load develop view\nuenv start --view=${DEV_VIEW_NAME} &lt;NAMD_UENV&gt;\n\n# Set variable VIEW_PATH to the view\nexport DEV_VIEW_PATH=/user-environment/env/${DEV_VIEW_NAME}\n\ncd ${PATH_TO_NAMD_SOURCE}\n</code></pre> <p>Action required</p> <p>Modify the <code>&lt;PATH_TO_NAMD_SOURCE&gt;/arch/Linux-ARM64.tcl</code> file now. Change <code>-ltcl8.5</code> with <code>-ltcl8.6</code> in the definition of the <code>TCLLIB</code> variable.</p> <pre><code># Build bundled Charm++\ntar -xvf charm-8.0.0.tar &amp;&amp; cd charm-8.0.0\nenv MPICXX=mpicxx ./build charm++ mpi-linux-arm8 smp --with-production -j 32\n\n# Configure NAMD build for GPU\ncd .. \n./config Linux-ARM64-g++.cuda \\\n    --charm-arch mpi-linux-arm8-smp --charm-base $PWD/charm-8.0.0 \\\n    --with-tcl --tcl-prefix ${DEV_VIEW_PATH} \\\n    --with-fftw --with-fftw3 --fftw-prefix ${DEV_VIEW_PATH} \\\n    --cuda-gencode arch=compute_90,code=sm_90 --with-single-node-cuda --with-cuda --cuda-prefix ${DEV_VIEW_PATH}\ncd Linux-ARM64-g++.cuda &amp;&amp; make -j 32\n\n# The namd3 executable (GPU-accelerated) will be built in the Linux-ARM64-g++.cuda directory\n</code></pre> <ul> <li>Change <code>&lt;PATH_TO_NAMD_SOURCE&gt;</code> to the path where you have the NAMD source code</li> <li>Change <code>&lt;NAMD_UENV&gt;</code> to the name (or path) of the actual NAMD UENV you want to use</li> </ul> <p>To run NAMD, make sure you load the same UENV and view you used to build NAMD, and set the following variable:</p> <pre><code>export LD_LIBRARY_PATH=\"${DEV_VIEW_PATH}/lib/\"\n</code></pre>"},{"location":"software/sciapps/namd/#useful-links","title":"Useful Links","text":"<ul> <li>NAMD Homepage</li> <li>NAMD Tutorials</li> <li>Running Charm++ Programs</li> <li>What you should know about NAMD and Charm++ but were hoping to ignore by J. C. Phillips</li> <li>NAMD Spack package</li> <li>Charm++ Spack package</li> </ul>"},{"location":"software/sciapps/quantumespresso/","title":"Quantum ESPRESSO","text":""},{"location":"software/sciapps/quantumespresso/#quantum-espresso","title":"Quantum ESPRESSO","text":"<p>Quantum ESPRESSO is an integrated suite of Open-Source computer codes for electronic-structure calculations and materials modeling at the nanoscale. It is based on density-functional theory, plane waves, and pseudopotentials:</p> <ul> <li><code>pw.x</code>: Plane-Wave Self-Consistent Field (PWscf)</li> <li><code>pw.x</code> First Principles Molecular Dynamics (FPMD)</li> <li><code>cp.x</code> Car-Parrinello (CP)</li> </ul> <p>uenvs</p> <p>Quantum ESPRESSO is provided on ALPS via uenv. Please have a look at the uenv documentation for more information about uenvs and how to use them.</p>"},{"location":"software/sciapps/quantumespresso/#how-to-run","title":"How to run","text":"<p>The following sbatch script can be used as a template.</p> GH200Eiger <p><pre><code>#SBATCH -N 1\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=71\n#SBATCH --gpus-per-task=1\n#SBATCH -A &lt;account&gt;\n#SBATCH --uenv=quantumespresso/v7.4:v2\n#SBATCH --view=default\n\nexport OMP_NUM_THREADS=20\nexport MPICH_GPU_SUPPORT_ENABLED=1\nexport OMP_PLACES=cores\n\nsrun -u --cpu-bind=socket /user-environment/env/default/bin/pw.x &lt; pw.in\n</code></pre> Current observation is that best perfomance is achieved using one MPI rank per GPU. How to run multiple ranks per GPU is described here.</p> <pre><code>#SBATCH -N 1\n#SBATCH --ntasks-per-node=128\n#SBATCH -A &lt;account&gt;\n#SBATCH --uenv=quantumespresso/v7.3.1\n#SBATCH --view=default\n#SBATCH --hint=nomultithread\n\nexport OMP_NUM_THREADS=1\n\nsrun -u /user-environment/env/default/bin/pw.x &lt; pw.in\n</code></pre>"},{"location":"software/sciapps/quantumespresso/#building-qe-from-source","title":"Building QE from Source","text":""},{"location":"software/sciapps/quantumespresso/#using-modules","title":"Using modules","text":"GH200A100 <pre><code>uenv start --view=modules quantumespresso/v7.4:v2\nmodule load cmake \\\n    fftw \\\n    nvhpc \\\n    nvpl-lapack \\\n    nvpl-blas \\\n    cray-mpich \\\n    netlib-scalapack \\\n    libxc\n\nmkdir build &amp;&amp; cd build\nFC=mpif90 CXX=mpic++ CC=mpicc cmake .. \\\n    -DQE_ENABLE_MPI=ON \\\n    -DQE_ENABLE_OPENMP=ON \\\n    -DQE_ENABLE_SCALAPACK:BOOL=OFF \\\n    -DQE_ENABLE_LIBXC=ON \\\n    -DQE_ENABLE_CUDA=ON \\\n    -DQE_ENABLE_PROFILE_NVTX=ON \\\n    -DQE_CLOCK_SECONDS:BOOL=OFF \\\n    -DQE_ENABLE_MPI_GPU_AWARE:BOOL=OFF \\\n    -DQE_ENABLE_OPENACC=ON\nmake -j20\n</code></pre> <pre><code>uenv start --view=modules quantumespresso/v7.3.1:v2\nmodule load cmake \\\n    cray-mpich\n    cuda \\\n    fftw \\\n    gcc \\\n    libxc \\\n    nvhpc \\\n    openblas\nmkdir build &amp;&amp; cd build\nFC=mpif90 CXX=mpic++ CC=mpicc cmake .. \\\n    -DQE_ENABLE_MPI=ON \\\n    -DQE_ENABLE_OPENMP=ON \\\n    -DQE_ENABLE_SCALAPACK:BOOL=OFF \\\n    -DQE_ENABLE_LIBXC=ON \\\n    -DQE_ENABLE_CUDA=ON \\\n    -DQE_CLOCK_SECONDS:BOOL=OFF \\\n    -DQE_ENABLE_MPI_GPU_AWARE:BOOL=OFF \\\n    -DQE_ENABLE_OPENACC=ON\nmake -j20\n</code></pre>"},{"location":"software/sciapps/quantumespresso/#using-spack","title":"Using spack","text":"<ol> <li> <p>Clone spack using the same version that has been used to build the uenv. <pre><code>uenv start quantumespresso/v7.3.1\n# clone the same spack version as has been used to build the uenv\ngit clone -b $(jq -r .spack.commit /user-environment/meta/configure.json) $(jq -r .spack.repo /user-environment/meta/configure.json) $SCRATCH/spack\n</code></pre></p> </li> <li> <p>Activate spack with the uenv configured as upstream <pre><code># ensure spack is using the uenv as upstream repository (always required)\nexport SPACK_SYSTEM_CONFIG_PATH=/user-environment/config\n# active spack (always required)\n. $SCRATCH/spack/share/spack/setup-env.sh\n</code></pre></p> </li> <li> <p>Create an anonymous environment for QE <pre><code>spack env create -d $SCRATCH/qe-env\nspack -e $SCRATCH/qe-env add quantum-espresso%nvhpc +cuda\nspack -e $SCRATCH/qe-env config add packages:all:prefer:cuda_arch=90\nspack -e $SCRATCH/qe-env develop -p /path/to/your/QE-src quantum-espresso@=develop\nspack -e $SCRATCH/qe-env concretize -f\n</code></pre> Check the output of <code>spack concretize -f</code>. All dependencies should have been picked up from spack upstream, marked eiter by a green <code>[^]</code> or <code>[e]</code>. Next we create a local filesystem view, this instructs spack to create symlinks for binaries and libraries in a local directory <code>view</code>. <pre><code>spack -e $SCRATCH/qe-env env view enable view\nspack -e $SCRATCH/qe-env install\n</code></pre> To recompile QE after editing the source code re-run <code>spack -e $SCRATCH/qe-env install</code>.</p> </li> <li> <p>Run <code>pw.x</code> using the filesystem view generated in 3. <pre><code>uenv start quantumespresso/v7.3.1\nMPICH_GPU_SUPPORT_ENABLED=1 srun [...] $SCRATCH/qe-env/view/bin/pw.x &lt; pw.in\n</code></pre></p> </li> </ol> <p>Warning</p> <p>The <code>pw.x</code> is linked to the uenv, it won't work without activating the uenv, also it will only work with the exact same version of the uenv. </p> <p>Warning</p> <p>The physical installation path is in <code>$SCRATCH/spack</code>, deleting this directory will leave the anonymous spack environment created in 3. with dangling symlinks.</p>"},{"location":"software/sciapps/vasp/","title":"VASP","text":""},{"location":"software/sciapps/vasp/#vasp","title":"VASP","text":"<p>Todo</p> <p>port over docs</p>"},{"location":"storage/","title":"Storage","text":"<ul> <li> <p> File Systems</p> <p>Learn about the filesystems on Alps</p> <p> File Systems</p> </li> <li> <p> Data Transfer</p> <p>Moving data into and out of CSCS, and between CSCS systems.</p> <p> Data Transfer</p> </li> <li> <p> Long Term Storage</p> <p>The Long Term Storage (LTS) service enables CSCS users to preserve their scientific data and ensures that it can be publicly accessed through a persistent identifier.</p> <p> LTS</p> </li> <li> <p> Object Storage</p> <p>CSCS offers a public cloud object storage service, based on the Ceph Object Gateway.</p> <p> Object Storage</p> </li> </ul>"},{"location":"storage/filesystems/","title":"File Systems","text":""},{"location":"storage/filesystems/#file-systems","title":"File Systems","text":"<p>Todo</p> <p>these are already out of date and need significant refactoring to be coherently linked to cluster definitions.</p> <p>CSCS supports different file systems, whose specifications are summarized in the table below:</p> <p>Please build big software projects not fitting <code>$HOME</code> on <code>$PROJECT</code> instead. Since you should not run jobs from <code>$HOME</code> or <code>$PROJECT</code>, please copy the executables, libraries and data sets needed to run your simulations to <code>$SCRATCH</code> with the Slurm transfer queue.</p> <p>Users can also write temporary builds on <code>/dev/shm</code>, a filesystem using virtual memory rather than a persistent storage device: please note that files older than 24 hours will be deleted automatically.</p> <p></p>"},{"location":"storage/filesystems/#quota","title":"Quota","text":"<p>You can check your storage quotas with the command quota on the front-end system ela (ela.cscs.ch) and the login nodes of eiger, daint, santis, and clariden.</p> <pre><code>$ quota\nRetrieving data ...\n\nUser: testuser\nUsage data updated on: 2025-02-21 16:01:27\n+------------------------------------+--------+-----+-------+----+-------------+----------+-----+---------+------+-------------+\n|                                    |  User quota  | Proj quota |             |   User files   |   Proj files   |             |\n+------------------------------------+--------+-----+-------+----+-------------+----------+-----+---------+------+-------------+\n| Directory                          |   Used |   % |  Used |  % | Quota limit |     Used |   % |    Used |    % | Files limit |\n+------------------------------------+--------+-----+-------+----+-------------+----------+-----+---------+------+-------------+\n| /iopsstor/scratch/cscs/testuser    |   4.0K |   - |     - |  - |           - |        1 |   - |       - |    - |           - |\n| /capstor/scratch/cscs/testuser     |   8.0K | 0.0 |     - |  - |      150.0T |        2 | 0.0 |       - |    - |     1000000 |\n| /users/testuser                    |  32.0K | 0.0 |     - |  - |       50.0G |       42 | 0.0 |       - |    - |      500000 |\n+------------------------------------+--------+-----+-------+----+-------------+----------+-----+---------+------+-------------+\n</code></pre> <p>Quotas apply to the total size of stored data, and in some cases to the number of inodes, to the filesystems on Alps. The command reports both disk space and the number of files for each filesystem/directory.</p> what is an inode <p>inodes are data structures that describe Linux file system objects like files and directories - every file and directory has a corresponding inode.</p> <p>Large inode counts degrade file system performance in multiple ways. For example, Lustre filesystems have separate metadata and data management. Excessive inode usage can overwhelm the metadata services, causing degradation across the filesystem.</p> <p>Tip</p> <p>Consider archiving folders with the tar command in order to keep low the number of files owned by users and groups.</p> <p>Tip</p> <p>Consider compressing directories full of many small input files as squashfs images - which pack many files into a single file that can be mounted to access the contents efficiently.</p> <p>Note</p> <p>The size of the quota depends on the file system, platform and project.</p> <p></p>"},{"location":"storage/filesystems/#cleaning-policy-and-data-retention","title":"Cleaning Policy and Data Retention","text":""},{"location":"storage/filesystems/#scratch","title":"Scratch","text":"<p>The scratch file system is designed for performance rather than reliability, as a fast workspace for temporary storage. All CSCS systems provide a scratch personal folder for users that can be accessed through the environment variable <code>$SCRATCH</code>.</p> <p>Alps provides a Lustre scratch file system mounted on <code>/capstor/scratch/cscs</code>, while other clusters share the GPFS scratch file system under <code>/scratch/shared</code>.</p>"},{"location":"storage/filesystems/#soft-quota","title":"Soft Quota","text":"<p>No strict quotas are enforced on scratch, but the scratch file system on Alps (<code>/capstor/scratch/cscs</code>) has a soft quota on both disk occupancy and inodes (files and folders), with a grace period to allow data transfer. Note that when the grace time expires, the soft quotas will become hard limits if you are over quota, therefore you won\u2019t be able to write any longer on your personal scratch folder.</p> <p>Alps (Eiger) users need to check their disk space and inodes usage with the command quota, that is available on the front end Ela and on Eiger User Access Nodes (UANs) as well. Currently the soft quotas are 150 TB disk space and 1 million inodes on Alps scratch file system, with a grace time of two weeks.</p>"},{"location":"storage/filesystems/#cleaning-policy","title":"Cleaning Policy","text":"<p>Please note that a cleaning policy is in place on scratch: all files older than 30 days will be deleted by a script that runs daily, so please ensure that you do not target this filesystem as a long term storage. Furthermore, kindly note that in order to avoid performance and stability issues on the scratch filesystem, if the occupancy grows above the critical limit of 60% we will be forced to ask you immediate action to remove unnecessary data: if the occupancy continues to grow and we reach 80%, we will then need to free up disk space manually removing files and folders without further notice.</p> <p>As a matter of fact, when the occupancy goes above 80% the Lustre filesystem shows a performance degradation that affects all users. The same applies with large numbers of small files, since the Lustre filesystem is not behaving ideally when dealing with high volumes of small files.</p> <p>Keep also in mind that data on scratch are not backed up, therefore users are advised to move valuable data to the /project filesystem or alternative storage facilities as soon as batch jobs are completed.</p> <p>Note</p> <p>Do not use the <code>touch</code> command to prevent the cleaning policy from removing files, because this behaviour would deprive the community of a shared resource.</p>"},{"location":"storage/filesystems/#users","title":"Users","text":"<p>Users are not supposed to run jobs from this filesystem because of the low performance. In fact the emphasis on the <code>/users</code> filesystem is reliability over performance: all home directories are backed up with GPFS snapshots and no cleaning policy is applied.</p>"},{"location":"storage/filesystems/#quota_1","title":"Quota","text":"<p>The $HOME environment variable points to the personal folder /users/: please, keep in mind that you cannot exceed the 50 GB - 500 K files quota enforced on $HOME. Expiration <p>Warning</p> <p>All data will be deleted 3 months after the closure of the user account without further warning.</p>"},{"location":"storage/filesystems/#store-on-capstore","title":"Store on Capstore","text":"<p>The <code>/capstor/store</code> mount point of the Lustre file system <code>capstor</code> is intended for high-performance per-project storage on Alps. The mount point is accessible from the User Access Nodes (UANs) of Alps vClusters.</p> <p>Note</p> <p>Capstore store is not yet mounted on Eiger.</p> <p>Info</p> <p><code>/capstor/store</code> is equivalent to the  <code>/project</code> and <code>/store</code> GPFS mounts on the old Daint system.</p> <p>The mount point features subfolders named after tenants and customers: on <code>daint.alps</code>, the only tenant currently available is CSCS, therefore all customer folders are located under the folder <code>/capstor/store/cscs</code> </p> <ul> <li>UserLab customers can access their project folder on daint.alps  with the <code>$PROJECT</code> environment variable, that targets the personal folder <code>/capstor/store/cscs/userlab/group_id/$USER</code>.</li> <li>Please note that users need to create their own sub-folders under the project folder, as they are not created automatically.</li> <li>Contractual partners will find their project folder under the corresponding contractual name listed in <code>/capstor/store/cscs</code>. For instance, EMPA users will have their projects listed under the <code>/capstor/store/cscs/empa</code> folder, and so on </li> </ul> <p>Data on <code>/capstor/store</code> is backed up with no cleaning policy: it provides intermediate storage space for datasets, shared code or configuration scripts that need to be accessed from different vClusters. The performance of the Lustre file system (read and write) increases using larger files, therefore you should consider to archive small files with the tar utility. Quota</p>"},{"location":"storage/filesystems/#quota_2","title":"Quota","text":"<p>Access to <code>/capstor/store</code> is granted to all users with a production or large development project upon request at the time of proposal submission: please note that applicants should justify the requested storage as well as they do for compute resources. Each group folder has a quota space allocated that allows maximum 1 M files and 150 TB of disk space at present. Expiration </p> <p>Warning</p> <p>All data will be deleted 3 months after the end of the project without further warning!</p>"},{"location":"storage/filesystems/#project","title":"Project","text":"<p>This is a shared - parallel file system based on the IBM GPFS software. It is accessible from the login nodes of all CSCS platforms using the native GPFS client through Infiniband or ethernet, however it is mounted read-only on the compute nodes of the Cray computing systems.</p> <p>Warning</p> <p>Users are not allowed to run jobs from this file system because of the low performance. </p> <p>The <code>$PROJECT</code> environment variable targets the personal folder <code>/project/&lt;group_id&gt;/$USER</code> on the GPFS: please note that users need to create their own sub-folders under the project folder, as they are not created automatically. Data is backed up with GPFS snapshots and no cleaning policy is applied: it provides intermediate storage space for datasets, shared code or configuration scripts that need to be accessed from different platforms. Read and write performance increase using larger files, therefore you should consider to archive small files with the tar utility.</p>"},{"location":"storage/filesystems/#quota_3","title":"Quota","text":"<p>Access to <code>/project</code> is granted to all users with a production or large development project upon request at the time of proposal submission: please note that applicants should justify the requested storage as well as they do for compute resources. Each group folder has a quota space allocated that allows maximum 50 K files per TB of disk space. Expiration</p> <p>Warning</p> <p>All data will be deleted 3 months after the end of the project without further warning.</p>"},{"location":"storage/filesystems/#store","title":"Store","text":"<p>Users are NOT supposed to run jobs from this filesystem because of the low performance. This is a shared - parallel filesystem based on the IBM GPFS software. It is accessible from the login nodes of all CSCS platforms using the native GPFS client through Infiniband or ethernet, however it is mounted read-only on the compute nodes of the Cray computing systems.</p> <p>Data is backed up with GPFS snapshots and no cleaning policy is applied: it provides long term storage for large amount of datasets, code or scripts that need to be accessed from different platforms. It is also intended for large files: performance increases when using larger files, therefore you should consider archiving small files with the tar utility.</p>"},{"location":"storage/filesystems/#quota_4","title":"Quota","text":"<p>Access to /store can only be bought signing a contract with CSCS. Data and inode quotas are group based: the quota is enforced according to the signed contract, with maximum 50 K files per TB of disk space. Expiration</p> <p>Warning</p> <p>All data will be deleted 3 months after the end of the contract.</p>"},{"location":"storage/longterm/","title":"Long Term Storage (LTS)","text":"<p>The Long Term Storage (LTS) service enables CSCS users to preserve their scientific data and ensures that it can be publicly accessed through a persistent identifier. The current implementation of the LTS service addresses the first two principles of the FAIR quadrant: findable and accessible.</p> <ul> <li> Findable Data and supplementary materials have sufficiently rich metadata anda  unique and persistent identifier.</li> <li> Accessible Metadata and data are understandable to humans and machines. Data is deposited in a trusted repository.</li> <li> Interoperable Metadata use a formal, accessible, shared, and broadly applicable language for knowledge representation.</li> <li> Reusable Data and collections have a clear usage license and provide accurate information on provenance.</li> </ul> <p>These are the main features of the service:</p> <ul> <li>Storage repository with long term retention capabilities (10 years);</li> <li>Provide persistent identifiers;</li> <li>Ability to set public access to data when needed;</li> <li>Data stored in LTS easily accessible from a web browser (HTTP protocol);</li> <li>RESTful API to integrate with third party applications/portals;</li> <li>Scalable service that can cope with large volumes of data;</li> <li>Resiliency due to data protection measures against hardware/software failures;</li> <li>Clear licensing of the data.</li> </ul>"},{"location":"storage/longterm/#service-description","title":"Service Description","text":"<p>The main unit of the LTS workflow is the data collection. A data collection is a group of data files enriched with a set of metadata attributes and a persistent ID referencing the entire collection. In other contexts such an entity might be called dataset, data aggregate or data block.</p> <p>The data files are store in the CSCS Object Store thus the data collection and its associated PID handle (the specific type of persistent ID used by LTS) will contain a list of URLs. There is no need to have special clients to access the data URLs or the PID handle, standard HTTP client like a browser or the <code>curl</code> command are sufficient.</p> <p>The PID handles use for the LTS service are the one provided by the CSCS PID service. This enables the LTS service to guarantee the consistency between data collection, object store data and PID handle.</p>"},{"location":"storage/longterm/#pricing","title":"Pricing","text":"<p>As of 2021:</p> <ul> <li>Users from the free User Lab program are entitled to use 2 TB of LTS storage quota (for 10 years) free of charge per project</li> <li>Currently additional space can be purchased for CHF 600.- for each Terabyte (for 10 years)</li> </ul>"},{"location":"storage/longterm/#prerequisites","title":"Prerequisites","text":"<p>In order to create collections and upload files into the LTS service, a user needs the following prerequisites:</p> <ul> <li>CSCS project with a quota on the LTS (and/or LTS-TDS) facility</li> <li>HTTP client: for example <code>curl</code> or Python requests.</li> <li>Keycloak registered client (only to access the service via RESTful API, not needed when using the web portal)</li> <li> <p>Outgoing connectivity to the following services:</p> Service URL Description LTS Prod https://lts.cscs.ch Production LTS service LTS TDS https://lts-tds.cscs.ch Test LTS service Keycloak https://auth.cscs.ch Authentication service Object Store https://object.cscs.ch Object Store service PID https://hdl.handle.net PID service </li> </ul> <p>In order to download files from the LTS service, a user needs a web browser or any other HTTP client like <code>curl</code>.</p>"},{"location":"storage/longterm/#creating-collections-and-uploading-files","title":"Creating collections and uploading files","text":"<p>LTS can be accessed in two ways:</p> <ul> <li>the web portal available at lts.cscs.ch;</li> <li>the LTS RESTful API, whose endpoints are described by the online documentation at lts.cscs.ch/api</li> </ul>"},{"location":"storage/longterm/#authentication","title":"Authentication","text":"<p>The LTS authenticates users based on the CSCS authentication service, therefore a user needs a valid CSCS account in order to access the service. The LTS web portal will guide the user through the authentication process; in order to access LTS through the RESTful API, the user will need to create a Keycloak token first.</p>"},{"location":"storage/longterm/#authorization","title":"Authorization","text":"<p>LTS data collections belong to a CSCS project: data can be stored in the LTS service after the principal investigator of the project has granted a project member the permissions to store data on behalf of the project. This is done by enabling the LTS facility for the user on the CSCS Account and Resources Management Tool.</p>"},{"location":"storage/longterm/#data-collection-creation-workflow","title":"Data collection creation Workflow","text":"<p>The typical LTS workflow will involve several steps, as described below. During the process, the data collection will go through the following states:</p> <pre><code>graph LR\n  A[NEW] --&gt; B[COMMITTED]\n  B --&gt; C[VALIDATED]\n  C --&gt; D[HANDLE ASSIGNED]\n  D --&gt; E[COMPLETED]</code></pre>"},{"location":"storage/longterm/#data-collection-definition","title":"Data collection definition","text":"<p>The workflow begins with the creation of a data collection. A minimum set of attributes is necessary to create the data collection; the most important ones are the following:</p> <ul> <li>name of the collection</li> <li>brief description</li> <li>project owning the data</li> <li>list of metadata attributes</li> <li>list of data files</li> <li>data files license</li> </ul> <p>The initial state of a data collection is <code>NEW</code>: the list of attributes and data files can be specified both at creation time or also added afterwards. The list of data files must contain the filename and the corresponding md5 checksum.</p> <p>Currently all the LTS data collections are considered containing public data: in future, the user will be able to specify whether the data collection is going to be public or private.</p> <p>If the data files are covered by copyright the user have to select an appropriate license. The default LTS license is CC BY 4.0.</p>"},{"location":"storage/longterm/#data-collection-inspection","title":"Data collection inspection","text":"<p>The collection can be inspected immediately after its creation. This is useful to check the attributes/objects already included in its definition, the object checksums, the overall state and the state of the single objects and their temporary URLs.</p>"},{"location":"storage/longterm/#data-collection-update-and-data-upload","title":"Data collection update and data upload","text":"<p>After the collection has been created, it can be modified during a transient phase: meanwhile, the collection state is <code>NEW</code> and the user can do the following:</p> <ul> <li>add additional data files</li> <li>add additional metadata attributes</li> <li>update already defined data files/attributes</li> <li>delete already defined data files/attributes</li> <li>upload the data files</li> </ul> <p>The LTS service generates a set of object store temporary URLs, one for each data file, which will be used to upload them to the object store. LTS is not involved in the upload operation, the data path comes from the user machine to the CSCS Object Store servers.</p>"},{"location":"storage/longterm/#licensing","title":"Licensing","text":"<p>When you prepare a data collection you must specify the license under which you publish your data. The default is the Creative Commons CC BY 4.0 license, but you can choose other ones. This Licensing guide provides information about the data licenses available in LTS.</p> <p>Todo</p> <p>the link to the licensing guide on Confluence KB was broken.</p>"},{"location":"storage/longterm/#commit","title":"Commit","text":"<p>At the end of the creation/update/upload phase, the user has to declare that the definition of the collection is complete. This is done by the user, who sends a collection commit request: after the commit request, the collection state changes from <code>NEW</code> to <code>COMMITTED</code> and from that point on the data collection definition cannot be modified anymore.</p>"},{"location":"storage/longterm/#validation","title":"Validation","text":"<p>Once the LTS service has received the commit request, it starts performing the validation operations needed to assess whether the files uploaded are consistent with the the checksums defined in the collection. If a mismatch is found, then the collection gets a <code>FAILED</code> status and the issue is reported back to the user. If all files are found in the object store and they have the correct checksum, the collection state is set to <code>VALIDATED</code>. At the end of the validation process, LTS sets the Object Store container ACLs in order to make the container world readable.</p>"},{"location":"storage/longterm/#handle-assignment","title":"Handle assignment","text":"<p>When the collection has entered the <code>VALIDATED</code> state, it's time for the LTS service to talk with the PID service and ask for a handle. The handle is attached to the collection and at that point the creation workflow of the collection is complete and the collection enter in the state <code>HANDLE_ASSIGNED</code> and the state COMPLETED shortly after that.</p>"},{"location":"storage/longterm/#dealing-with-failures","title":"Dealing with Failures","text":"<p>After the collection is committed by the user, LTS performs a series of checks on the uploaded data and requests an handle to the ePIC handle service. If any error occurs during this phase the collection state will be <code>FAILED</code>. Possible reasons for this state are:</p> <ul> <li>a failure in one of the LTS microservices</li> <li>a failure in one of the underneath services (object store, handle server, database server etc ..)</li> <li>one or more data files were not uploaded</li> <li>one or more data file checksums are wrong</li> </ul> <p>When in state FAILED the collection is still editable. The user will have to review the content of the collection, fix the checksum or re-upload files if he/she spots anything wrong or missing. Then the collection needs to be saved and the validation retried. The validation process can be retried clicking on \"Retry Data Collection\" from the web portal or through the <code>commit</code> endpoint if using the RESTful API.</p>"},{"location":"storage/longterm/#downloading-data-from-lts","title":"Downloading data from LTS","text":"<p>The data is publicly accessible via the HTTP protocol once the status of the collection has entered the <code>COMPLETED</code> step. Its URL can be found under the \"Handle\" attribute of the data collection.</p>"},{"location":"storage/longterm/#further-documentation","title":"Further Documentation","text":"<p>Todo</p> <p>All of the links in this section (except the video) on the Confluence KB were broken.</p> <p>The Long Term Storage webinar, held in June 2021, provides an description of the Long Term Storage service use case, its architecture and workflow, and a demonstration of the Web Portal.</p> <p>The RESTful API HowTo provides some usage examples for the LTS API with curl and Python.</p> <p>The Web portal: create a data collection page provides usage examples for the creation of a data collection in the LTS web portal.</p> <p>The Web portal: define and upload objects page provides usage examples for definition and the upload of data collection objects in the LTS web portal.</p> <p>The Licensing guide provides information about the data licenses available in LTS.</p> <p>The Landing page page provides information about the LTS landing page.</p>"},{"location":"storage/object/","title":"Object Storage","text":"<p>Note</p> <p>This page is currently incomplete and it is being updated following recent developments.</p>"},{"location":"storage/object/#s3","title":"S3","text":"<p>CSCS offers a public cloud object storage service, based on the Ceph Object Gateway. The service can be accessed from S3-compatible clients.</p>"},{"location":"storage/object/#general-information","title":"General Information","text":"<ul> <li>Endpoint: https://rgw.cscs.ch</li> <li>URL: path-style in the format <code>https://rgw.cscs.ch/%(bucket)s/key-name</code></li> <li>Publicly accessible object links: <code>https://rgw.cscs.ch/&lt;tenant&gt;:&lt;bucket-name&gt;/key-name</code><ul> <li>after setting proper bucket policy</li> </ul> </li> </ul>"},{"location":"storage/object/#usage-examples","title":"Usage Examples","text":""},{"location":"storage/object/#aws-cli","title":"AWS CLI","text":""},{"location":"storage/object/#configuration","title":"Configuration","text":"<p>The first step is to configure the profile:</p> <pre><code>&gt; aws configure --profile naret-testuser\nAWS Access Key ID [None]: [REDACTED]\nAWS Secret Access Key [None]: [REDACTED]\nDefault region name [None]: cscs-zonegroup\nDefault output format [None]:\n</code></pre> <p>Then, settings such as the default endpoint and the path-style URLs can be placed in the configuration file:</p> <pre><code>[profile naret-testuser]\nendpoint_url = https://rgw.cscs.ch\nregion = cscs-zonegroup\ns3 =\n    addressing_style = path\n</code></pre>"},{"location":"storage/object/#creating-a-pre-signed-url","title":"Creating a pre-signed URL","text":"<pre><code>&gt; aws --profile=naret-testuser s3 presign s3://test-bucket/file.txt --expires-in 300\n\nhttps://rgw.cscs.ch/test-bucket/file.txt?X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Credential=IA6AOCNMKPDXQ0YNA3DP%2F20241209%2Fcscs-zonegroup%2Fs3%2Faws4_request&amp;X-Amz-Date=20241209T080748Z&amp;X-Amz-Expires=300&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Signature=f2e2adb457f6fd43401124e4ea2650fba528e614ab661f9c05e2fa2e77691b5d\n</code></pre> <p>Notice that the tenant part is missing from the URL: this is because S3 doesn't natively deal with multitenancy. The correct object is retrieved based on the access key. A more thorough explanation can be found in the RGW documentation.</p>"},{"location":"storage/object/#making-a-buckets-contents-anonymously-accessible-from-the-internet","title":"Making a bucket's contents anonymously accessible from the Internet","text":"<p>First, a bucket policy needs to be written:</p> <pre><code>&gt; cat test-public-bucket-anon-from-internet.json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": \"*\",\n      \"Action\": \"s3:GetObject\",\n      \"Resource\": [\n        \"arn:aws:s3:::test-public-bucket/*\",\n        \"arn:aws:s3:::test-public-bucket\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>Then, it can be applied to the bucket:</p> <pre><code>&gt; aws --profile=naret-testuser s3api put-bucket-policy \\\n      --bucket test-public-bucket --policy \\\n      file://test-public-bucket-anon-from-internet.json\n</code></pre> <p>At this point, the objects in test-public-bucket are accessible via direct links:</p> <pre><code>&gt; s3cmd --configure\n\nEnter new values or accept defaults in brackets with Enter.\nRefer to user manual for detailed description of all options.\n\nAccess key and Secret key are your identifiers for Amazon S3. Leave them empty for using the env variables.\nAccess Key: [REDACTED]\nSecret Key: [REDACTED]\nDefault Region [US]: cscs-zonegroup\n\nUse \"s3.amazonaws.com\" for S3 Endpoint and not modify it to the target Amazon S3.\nS3 Endpoint [s3.amazonaws.com]: rgw.cscs.ch\n\nUse \"%(bucket)s.s3.amazonaws.com\" to the target Amazon S3. \"%(bucket)s\" and \"%(location)s\" vars can be used\nif the target S3 system supports dns based buckets.\nDNS-style bucket+hostname:port template for accessing a bucket [%(bucket)s.s3.amazonaws.com]: rgw.cscs.ch/%(bucket)s\n\nEncryption password is used to protect your files from reading\nby unauthorized persons while in transfer to S3\nEncryption password:\nPath to GPG program:\n\nWhen using secure HTTPS protocol all communication with Amazon S3\nservers is protected from 3rd party eavesdropping. This method is\nslower than plain HTTP, and can only be proxied with Python 2.7 or newer\nUse HTTPS protocol [Yes]: Yes\n\nOn some networks all internet access must go through a HTTP proxy.\nTry setting it here if you can't connect to S3 directly\nHTTP Proxy server name:\n\nNew settings:\n  Access Key: [REDACTED]\n  Secret Key: [REDACTED]\n  Default Region: cscs-zonegroup\n  S3 Endpoint: rgw.cscs.ch\n  DNS-style bucket+hostname:port template for accessing a bucket: rgw.cscs.ch/%(bucket)s\n  Encryption password:\n  Path to GPG program: None\n  Use HTTPS protocol: True\n  HTTP Proxy server name:\n  HTTP Proxy server port: 0\n</code></pre> <p>And then confirm.</p> <p>IMPORTANT: The configuration is not complete yet.</p> <pre><code>&gt; s3cmd ls s3://test-bucket\nERROR: S3 error: 403 (SignatureDoesNotMatch)\n</code></pre> <p>To fix this, it is necessary to edit the <code>.s3cfg</code> file, normally located in the user's home directory, and change the <code>signature_v2</code> setting to true.</p> <pre><code>~ $ cat .s3cfg | grep signature_v2\nsignature_v2 = True\n\n$ s3cmd ls s3://test-bucket\n2024-12-09 08:05           15  s3://test-bucket/file.txt\n</code></pre>"},{"location":"storage/object/#cyberduck","title":"Cyberduck","text":""},{"location":"storage/object/#configuration_1","title":"Configuration","text":"<p>In order to be able to connect to the S3 endpoint using Cyberduck, a profile supporting path-style requests must be downloaded from here.</p> <p></p>"},{"location":"storage/transfer/","title":"Data Transfer","text":""},{"location":"storage/transfer/#data-transfer","title":"Data Transfer","text":""},{"location":"storage/transfer/#external-transfer","title":"External Transfer","text":"<p>CSCS currently offers the CSCS Globus online endpoint for uploading and downloading data from and to CSCS:</p> <p>The recommended way to transfer data externally occurs via the CSCS globus-online endpoint.</p> <ol> <li>Follow the official get started documentation to login<ul> <li>in case you don't have an organisation account, you can just use the option \"Sign in with Google\" </li> </ul> </li> <li>Use the file manager to search for an endpoint typing \"CSCS\"<ul> <li>Please make sure that the login page belongs to the cscs.ch domain (shown in the URL)</li> <li>The CSCS endpoint requires authentication, therefore use your CSCS credentials to log in </li> </ul> </li> <li>Once logged in, you can transfer data to and from CSCS.<ul> <li>if you want to transfer the data to another endpoint, just search for it and transfer the data</li> <li>if you want to download the data to your local system, you will need the Globus Connect Personal client: the client will turn your local system into an endpoint, so you will be able to select it and transfer the data.</li> </ul> </li> </ol> <p>For more information about Globus Connect Personal, please read the official Frequently Asked Questions.</p> <p>Currently Globus provide the following mount points at CSCS:</p> Mount Point Description <code>/scratch/snx3000</code> old Daint scratch area <code>/store</code> old Daint store area <code>/project</code> old Daint project area <code>/users</code> old Daint home directory <code>/scratch/shared</code> old Scratch-Shared area ( old meteoswiss clusters ) <code>/iopsstor/scratch/cscs</code> Mounted on Clariden <code>/capstor/scratch/cscs</code> New Alps Daint scratch area <code>/capstor/store/cscs</code> New Alps Daint store area <code>/capstor/users/cscs</code> Home directory for Bristen/Scopi/Errigal <code>/vast/users/cscs</code> New Alps vclusters home directory  ( Alps Daint and others ) <p></p>"},{"location":"storage/transfer/#internal-transfer","title":"Internal Transfer","text":"<p>The Slurm queue <code>xfer</code> is available on Alps clusters to address data transfers between internal CSCS file systems. The queue has been created to transfer files and folders from <code>/users</code>, <code>/capstor/store</code> or <code>/iopstor/store</code> to the <code>/capstor/scratch</code> and <code>/iopstor/scratch</code> file systems (stage-in) and vice versa (stage-out). Currently the following commands are available on the cluster supporting the queue xfer:</p> <pre><code>cp\nmv\nrm\nrsync\n</code></pre> <p>You can adjust the Slurm batch script below to transfer your input data on <code>$SCRATCH</code>, setting the variable command to the unix command that you intend to use, choosing from the list given above:</p> <pre><code>#!/bin/bash -l\n#\n#SBATCH --time=02:00:00\n#SBATCH --ntasks=1\n#SBATCH --partition=xfer\n\ncommand=\"rsync -av\"\necho -e \"$SLURM_JOB_NAME started on $(date):\\n $command $1 $2\\n\"\nsrun -n $SLURM_NTASKS $command $1 $2\necho -e \"$SLURM_JOB_NAME finished on $(date)\\n\"\n\nif [ -n \"$3\" ]; then\n  # unset memory constraint enabled on xfer partition\n  unset SLURM_MEM_PER_CPU\n  # submit job with dependency\n  sbatch --dependency=afterok:$SLURM_JOB_ID $3\nfi\n</code></pre> <p>The template Slurm batch script above requires at least two command line arguments, which are the source and the destination files (or folders) to be copied. The stage script may take as third command line argument the name of the production Slurm batch script to be submitted after the stage job: the Slurm dependency flag <code>--dependency=afterok:$SLURM_JOB_ID</code> ensures that the production job can begin execution only after the stage job has successfully executed (i.e. ran to completion with an exit code of zero).</p> <p>You can submit the stage job with a meaningful job name as below:</p> <pre><code># stage-in and production jobs\n$ sbatch --job-name=stage_in stage.sbatch \\\n         ${PROJECT}/&lt;source&gt; ${SCRATCH}/&lt;destination&gt; \\\n         production.sbatch\n</code></pre> <p>The Slurm flag <code>--job-name</code> will set the name of the stage job that will be printed in the Slurm output file: the latter is by default the file <code>slurm-${SLURM_JOB_ID}.out</code>, unless you set a specific name for output and error using the Slurm flags <code>-e/--error</code> and/or <code>-o/--output</code> (e.g.: <code>-o %j.out -e %j.err</code>, where the Slurm symbol <code>%j</code> will be replaced by <code>$SLURM_JOB_ID</code>). l The stage script will also submit the Slurm batch script production.sbatch given as third command line argument. The production script can submit in turn a stage job to transfer the results back. E.g.:</p> <pre><code># stage-out\nsbatch --dependency=afterok:${SLURM_JOB_ID} --job-name=stage_out \\\n       stage.sbatch ${SCRATCH}/&lt;source&gt; ${PROJECT}/&lt;destination&gt;\n</code></pre>"}]}